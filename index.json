[{"authors":["admin"],"categories":null,"content":"Giuseppe Manco is Director of Research at the Institute of High Performance Computing and Networks of the National Research Council of Italy. His research interests include User Profiling and Behavioral Modeling, Social Network Analysis, Information Propagation and Diffusion, Recommender Systems, Machine Learning for Cybersecurity.\nExpert on data science, data analytics and enabling technologies for data analytics. Interested in new frontiers of Computer Science and Technology aimed at analyzing Complex Big Data. Co-founder of Open Knowledge Technologies (OKT), a spin-off company of University of Calabria aimed at bringing innovation from academia to industry on the specific topics of Artificial Intelligence and Cybersecurity.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://gmanco.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Giuseppe Manco is Director of Research at the Institute of High Performance Computing and Networks of the National Research Council of Italy. His research interests include User Profiling and Behavioral Modeling, Social Network Analysis, Information Propagation and Diffusion, Recommender Systems, Machine Learning for Cybersecurity.\nExpert on data science, data analytics and enabling technologies for data analytics. Interested in new frontiers of Computer Science and Technology aimed at analyzing Complex Big Data.","tags":null,"title":"Giuseppe Manco","type":"authors"},{"authors":null,"categories":null,"content":" Importante Si avvisano gli studenti che il corso di Analisi di Immagini e Video – Corso di Laurea Magistrale in Ingegneria Informatica verrà erogato in modalità streaming, utilizzando l’applicativo TEAMS, a partire dal giorno 17/03/2019, alle ore 8:30.\nLa richiesta di iscrizione al corso può essere effettuata cliccando sul seguente link.\nGli studenti già pratici dell’utilizzo di TEAMS e che abbiano già scaricato l’APP, possono iscriversi direttamente al corso (senza approvazione del docente) utilizzando il codice: khm7h4s\nBreve descrizione del corso Il corso è finalizzato ad acquisire e sperimentare le tecniche di base per l’analisi di immagini e video. Verranno illustrati i concetti fondamentali per l’analisi delle immagini e verranno illustrate le principali tecniche di object detection, object tracking e action detection. Durante il corso saranno presentati modelli di reti neurali CNN e RNN, tecniche di transfer learning, Residual and Attention network ed una introduzione ai modelli generativi e alle Adversarial networks. Gli esempi applicativi faranno uso del linguaggio Python e dei framework di Deep Learning Pytorch/Tensorflow.\nCompetenze specifiche:\n comprensione dei concetti legati all’image e al video processing conoscenza degli aspetti caratterizzanti di machine learning e deep learning comprensione delle principali tecniche per l’analisi di immagini e video abilità di utilizzare algoritmi di image analysis per la risoluzione di problemi specifici  Docenti  Giuseppe Manco. Ricevimento: mercoledì’ 14:30-16:30. Francesco Pisani. Rivevimento lunedì 15-17.  Programma  Introduzione alla computer vision  Concetti fondamentali su Image processing e analysis: Image Basics, Python per Image Processing, Manipolazione di immagini. Trasformazioni: Normalizzazione, filtri, Edge detection, morfologia, thresholding e segmentazione.  Classificazione di immagini e video  Introduzione alla classificazione: applicazioni; approcci classici; scikit-Learn per la classificazione; limitazioni. Deep Learning: Review su Neural Networks per l\u0026rsquo;analisi di immagini e video. Convolutional Neural Networks. Reti ricorrenti. R-CNN. Gestione dell\u0026rsquo;overfitting. Concetti avanzati. Principali architetture di rete e loro caratteristiche: VGG, AlexNet, Inception and Residual Networks, Attention Networks. Transfer Learning.  Concetti avanzati  Object Detection: Sliding windows, boundary boxes e anchors. Region Proposal Networks. Yolo e Darknet. Applicazioni. Object tracking e action recognition. Optical flow; Single/multiple objects tracking; Action classification and localization. Image segmentation e synthesis. UNet. Neural style transfer. Modelli Generativi: Probabilistic Modeling, Autoencoders. Generative Adversarial Networks. Applicazioni: Colorization, Reconstruction, Super-Resolution, Synthesis, Text-to-image. Adversarial Machine Learning. Principali attacchi e contromisure. Adversarial-free deep networks.   Materiale didattico  Lucidi delle lezioni Notebooks e lucidi delle esercitazioni Libri:  E.R. Davies, Computer Vision: Principles, Algorithms, Applications, Learning. Fifth edition. Elsevier/Academic Press, 2018 [Davies18] Jan Erik Solem, Programming Computer Vision with Python. O\u0026rsquo;Reilly Media, 2012. [Solem12] Mohamed Elgendy, Deep Learning for Vision Systems. Manning, 2020. [Elg20] Rafael C. Gonzalez, Richard E. Woods, Digital image processing. 4th edition. Pearson, 2018. [Gon18]   Sillabo delle lezioni    Lezione Argomenti Materiale didattico Data     1 Introduzione al corso Slides 17/03/2020   2 Concetti fondamentali su Image processing e analysis Slides, notebook 19/03/2020    ","date":1582675200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1582675200,"objectID":"63ae979fd283aabb2a9d1b1c9e94f224","permalink":"https://gmanco.github.io/courses/computervision/","publishdate":"2020-02-26T00:00:00Z","relpermalink":"/courses/computervision/","section":"courses","summary":"Il corso mira a fornire solide basi in merito all’analisi di immagini e video e fornire una conoscenza delle principali tecniche di deep learning per il riconoscimento di oggetti e l’individuazione di sequenze rilevanti in un video.","tags":null,"title":"Panoramica","type":"docs"},{"authors":null,"categories":null,"content":" Introduzione al corso. Image Processing, Analysis e Computer Vision.\nSlides disponibili qui\nRiferimenti bibliografici  [Davies18], ch. 1. [Elg20], sect. 1.1-1.3. [Gon18], Ch. 1.  Link utili  Introduction to Computer Vision A gentle introduction to Computer Vision A Beginner\u0026rsquo;s guide to Computer Vision Everything you wanted to know about Computer Vision The Computer Vision Foundation  ","date":1582758000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582758000,"objectID":"3d7f16d70e17b38150e3809885d99456","permalink":"https://gmanco.github.io/courses/computervision/lecture1/","publishdate":"2020-02-27T00:00:00+01:00","relpermalink":"/courses/computervision/lecture1/","section":"courses","summary":" Introduzione al corso. Image Processing, Analysis e Computer Vision.\nSlides disponibili qui\nRiferimenti bibliografici  [Davies18], ch. 1. [Elg20], sect. 1.1-1.3. [Gon18], Ch. 1.  Link utili  Introduction to Computer Vision A gentle introduction to Computer Vision A Beginner\u0026rsquo;s guide to Computer Vision Everything you wanted to know about Computer Vision The Computer Vision Foundation  ","tags":null,"title":"Introduzione al corso","type":"docs"},{"authors":null,"categories":null,"content":" Concetti fondamentali su Image processing e analysis: Image Basics, Manipolazione di immagini. Introduzione alle librerie Python per Image Processing.\nSlides disponibili qui\nLa lezione è corredata di un notebook disponibile qui\nRiferimenti bibliografici  [Davies18], ch. 2. [Elg20], sect. 1.4-1.5. [Gon18], Ch. 2-3. [Solem12], Ch. 1.  Link utili  Stanford cs231n Python tutorial Stanford cs231n Jupyter tutorial Anaconda, la piattaforma di riferimento per installare Python e Jupyter Piattaforma azure di Microsoft per l\u0026rsquo;esecuzione di notebooks Google Colab The missing semester, un mini-corso del MIT sui tool di base che tutti i data scientist dovrebbero conoscere  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"370182fe2b01a1596cfa2150d45b1ec3","permalink":"https://gmanco.github.io/courses/computervision/lecture2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/computervision/lecture2/","section":"courses","summary":" Concetti fondamentali su Image processing e analysis: Image Basics, Manipolazione di immagini. Introduzione alle librerie Python per Image Processing.\nSlides disponibili qui\nLa lezione è corredata di un notebook disponibile qui\nRiferimenti bibliografici  [Davies18], ch. 2. [Elg20], sect. 1.4-1.5. [Gon18], Ch. 2-3. [Solem12], Ch. 1.  Link utili  Stanford cs231n Python tutorial Stanford cs231n Jupyter tutorial Anaconda, la piattaforma di riferimento per installare Python e Jupyter Piattaforma azure di Microsoft per l\u0026rsquo;esecuzione di notebooks Google Colab The missing semester, un mini-corso del MIT sui tool di base che tutti i data scientist dovrebbero conoscere  ","tags":null,"title":"Concetti fondamentali su Image processing e analysis","type":"docs"},{"authors":[],"categories":null,"content":"","date":1582023600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582023600,"objectID":"bc57f30b003b21fbab073e7a0136a896","permalink":"https://gmanco.github.io/talk/isi-feb20/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/isi-feb20/","section":"talk","summary":"I discuss some issues and solutions in devising generative models for marked temporal poin processes.","tags":["Point Processes","Generative models"],"title":"Adversarial Games for generative modeling of Temporally-Marked Event Sequences","type":"talk"},{"authors":null,"categories":null,"content":"In what follows I'll try to explain my basic understanding and interepretation of the semi-supervised framework based on Variational Autoencoders, as described in [1]. I shall assume a vector notation where bold symbols $\\mathbf{a}$ represent vectors, whose $j$-th component can be represented as $a_j$.\nThe starting point of the framework is to consider a dataset \\(D = S \\cup U\\), where:\n \\(S = \\{(\\mathbf{x}_1, \\mathbf{y}_1), \\ldots, (\\mathbf{x}_n, \\mathbf{y}_n)\\}\\),\n \\(U = \\{\\mathbf{x}_{n+1}, \\ldots, \\mathbf{x}_{n+m}\\}\\),  with \\(\\mathbf{x}_i \\in \\mathbb{R}^N\\) and \\(\\mathbf{y}_i \\in \\{0,1\\}^C\\) represents a one-hot encoding of a class in \\(\\{1, \\ldots, C\\}\\).\nThe basic assumption of variational autoencoders is that data is generated according to a density function \\(p_\\theta(\\mathbf{x}| \\mathbf{z})\\), where \\(\\mathbf{z}\\in \\mathbb{R}^K\\) is a latent variables governing the distribution of \\(\\mathbf{x}\\). \\(\\theta\\) represents a model parameter. The above density function can be modeled through a Neural network: thus \\(\\theta\\) represents all the network weights. An example PyTorch snippet, where the density is be modeled as a softmax over a simple linear layer, is illustrated below.\nimport torch import torch.nn as nn class Decoder(nn.Module): def __init__(self, latent_size,num_classes,out_size): super(Decoder, self).__init__() self.linear_layer = nn.Linear(latent_size + num_classes, out_size) nn.init.xavier_normal_(self.layer.weight) self.activation = nn.Sigmoid(dim=-1) def forward(self, z): return self.activation(self.linear_layer(z))  Here, we are assuming $\\mathbf{x}$ binary and consequently the reconstruction exploits a bernoullian distribution for each feature.\nLet's see how the generative framework can model the likelihood of the data and help us develop a semi-supervised classifier.\nUnsupervised examples First, let us consider \\(\\mathbf{x}\\in U\\). When \\(\\mathbf{y}\\) is unknown, we can consider it as a latent variable as well. Both \\(\\mathbf{y}\\) and \\(\\mathbf{z}\\) assume a prior distribution, given by\n\\[\\begin{split} \\mathbf{z} \\sim \u0026 \\mathcal{N}(\\mathbf{0},I_K)\\\\ \\mathbf{y} \\sim \u0026 \\mathit{Cat}(\\boldsymbol\\pi) \\end{split}\\]\nHere, \\(\\boldsymbol\\pi\\) is a prior multinomial distribution over \\(\\{1, \\ldots, C\\}\\).\nIn such a case, we can extend the generative setting where data samples $\\mathbf{x}$ are assumed to be generated by the conditional \\(p_\\theta(\\mathbf{x}| \\mathbf{z},\\mathbf{y})\\) through the relationship\n\\[ \\begin{equation}\\label{average}\\tag{1} p(\\mathbf{x}) = \\int p_\\theta(\\mathbf{x}| \\mathbf{z}, \\mathbf{y}) p(\\mathbf{z})p(\\mathbf{y}) \\mathrm{d} \\mathbf{z} \\mathrm{d} \\mathbf{y} \\end{equation} \\]\nIn principle, the \\(\\theta\\) parameter can be chosen to maximize the evidence on \\(D\\), i.e. by optimizing \\(\\log p(\\mathbf{x})\\). However, this approach is not feasible because it requires averaging over all possible \\(\\mathbf{z}\\) pairs. We can approximate the likelihood by sampling a subset of $\\mathbf{z}$ latent data points and then averaging over them. Again, this workaround exhibits the drawback that pure random sampling is exposed to high variance. The idea of Variational Autoencoders is to \u0026quot;guide\u0026quot; the sampling by exploiting the evidence: instead of freely choosing $\\mathbf{z},\\mathbf{y}$, we build a sampler \\(q_\\phi(\\mathbf{z},\\mathbf{y}|\\mathbf{x})\\) that tunes the probability of \\(\\mathbf{z},\\mathbf{y}\\) according to $\\mathbf{x}$. In practice, $q_\\phi$ encodes the information regarding \\(\\mathbf{x}\\) into the most probable \\(\\mathbf{z},\\mathbf{y}\\) latent variables.\nWe can factorize the decoder as \\(q_\\phi(\\mathbf{z},\\mathbf{y}| \\mathbf{x}) = q_\\varphi(\\mathbf{z}|\\mathbf{x},\\mathbf{y})q_\\vartheta(\\mathbf{y}|\\mathbf{x})\\), where\n\\[\\begin{split} q_\\varphi(\\mathbf{z}|\\mathbf{x},\\mathbf{y}) \\equiv \u0026 \\mathcal{N}(\\mathbf{z}| \\boldsymbol\\mu_\\varphi(\\mathbf{x},\\mathbf{y}), \\boldsymbol\\sigma_\\varphi(\\mathbf{x},\\mathbf{y})\\cdot I_K)\\\\ q_\\vartheta(\\mathbf{y}|\\mathbf{x}) \\equiv \u0026 \\mathit{Cat}(\\mathbf{y}|\\boldsymbol\\pi_\\vartheta(\\mathbf{x})), \\end{split}\\]\nHere, the parameters \\(\\boldsymbol\\mu_\\varphi(\\mathbf{x},\\mathbf{y}), \\boldsymbol\\sigma_\\varphi(\\mathbf{x},\\mathbf{y})\\) and \\(\\boldsymbol\\pi_\\vartheta(\\mathbf{x})\\) represent neural functions parameterized by $\\varphi$ and $\\vartheta$, respectively. Again, a PyTorch snippet is given below, where the two encoders are exemplified. Concerning \\(q_\\varphi(\\mathbf{z}|\\mathbf{x}, \\mathbf{y})\\), we have:\nclass Encoder_z(nn.Module): def __init__(self, input_size,latent_size): super(Decoder, self).__init__() self.latent_size = latent_size self.linear_layer = nn.Linear(input_size, 2*latent_size) nn.init.xavier_normal_(self.linear_layer.weight) def _sample_latent(self, mu_q, logvar_q): var_q = torch.exp(logvar_q) epsilon = torch.randn(var_q.size(),requires_grad=False).to(var_q.device) return mu_q + epsilon*var_q def forward(self, x, y): input = torch.cat([x,y],dim=-1) temp_out = self.linear_layer(input) mu_q = temp_out[:, :self.latent_size] logvar_q = temp_out[:, self.latent_size:] z = self._sample_latent(mu_q, logvar_q) return z, mu_q, logvar_q ​```  The computation within this class is a variable \\(\\mathbf{z}\\sim q_\\varphi(\\cdot |\\mathbf{x},\\mathbf{y})\\), as well as the parameters of the variational (gaussian) distribution \\(\\boldsymbol\\mu\\) and \\(\\boldsymbol\\sigma\\). Here, we are exploiting the reparameterization trick: given a variable \\(\\boldsymbol\\epsilon \\sim \\mathcal{N}(\\mathbf{0},I_K)\\), the transformation \\(z = \\mu + \\epsilon \\cdot \\sigma\\) guarantees that \\(\\mathbf{z}\\sim \\mathcal{N}(\\boldsymbol\\mu, \\boldsymbol\\sigma)\\) and at the same time it preserves the backpropagation of the gradient, since \\( \\frac{\\partial \\mathbf{z}}{\\partial w} = \\frac{\\partial \\boldsymbol\\mu}{\\partial w} + \\boldsymbol\\epsilon \\cdot \\frac{\\partial \\boldsymbol\\sigma}{\\partial w} \\) and both \\(\\boldsymbol\\mu\\) and \\(\\boldsymbol\\sigma\\) are deterministically computed as shown above.\nSimilarly, \\(q_\\vartheta(\\mathbf{y}|\\mathbf{x})\\) is exemplified by the following snippet:\nclass Classifier(nn.Module): def __init__(self, input_size,num_classes): super(Decoder, self).__init__() self.linear_layer = nn.Linear(input_size, 2*latent_size) nn.init.xavier_normal_(self.layer.weight) self.softmax = nn.Softmax(dim=-1) def forward(self, x): return self.softmax(self.linear_layer(x))  Notice that, differently from the previous case, within this class we directly output a probability distribution \\(q_\\vartheta(\\mathbf{y}|\\mathbf{x})\\), rather than a sample \\(\\mathbf{y} \\sim q_\\vartheta(\\cdot|\\mathbf{x})\\). The reason for this choice is that no reparameterization trick is possible for a discrete distribution that preserves backpropagation. However, this does not prevent us from averaging over all possible samples, as we shall see later.\nWhat is the relationship between the encoders and the decoder? We can observe that\n\\( \\begin{split} \\log p(\\mathbf{x}) \\geq \u0026 \\mathbb{E}_{q_\\phi(\\mathbf{z},\\mathbf{y}| \\mathbf{x})}\\left[\\log p_\\theta(\\mathbf{x}| \\mathbf{z}) + \\log p(\\mathbf{z}) + \\log p(\\mathbf{y}) - \\log q_\\phi(\\mathbf{z},\\mathbf{y}| \\mathbf{x})\\right] \\\\ = \u0026 \\sum_\\mathbf{y} \\mathbb{E}_{q_\\varphi(\\mathbf{z}| \\mathbf{x},\\mathbf{y})}\\Bigg[ q_\\vartheta(\\mathbf{y}| \\mathbf{x})\\bigg(\\log p_\\theta(\\mathbf{x}| \\mathbf{z}) + \\log p(\\mathbf{y}) - \\log q_\\vartheta(\\mathbf{y}| \\mathbf{x})\\bigg) \\\\ \u0026 \\qquad \\qquad + \\log p(\\mathbf{z}) - \\log q_\\varphi(\\mathbf{z}| \\mathbf{x},\\mathbf{y})\\Bigg] \\end{split} \\) We call the right-hand side of the equation the Evidence Lower Bound (ELBO). It turns out that, optimizing this equation with respect to \\(\\phi, \\theta\\) corresponds to optimizing \\(\\log p(\\mathbf{x})\\) as well. Thus, we can specify the loss \\(\\ell(\\mathit{x})\\) as the negative of the ELBO and exploit a gradient-based optimization strategy. The main difference with respect to directly optimizing eq. (\\(\\ref{average}\\)) is that the ELBO is tractable. In fact, we can rewrite it as\n\\(\\begin{split} \\ell(\\mathbf{x})= \u0026 - \\sum_\\mathbf{y} \\mathbb{E}_{\\boldsymbol\\epsilon\\sim \\mathcal{N}(\\mathbf{0},I_K)}\\Bigg[ q_\\vartheta(\\mathbf{y}| \\mathbf{x})\\bigg(\\log p_\\theta(\\mathbf{x}| \\mathbf{z}(\\boldsymbol\\epsilon, \\mathbf{x},\\mathbf{y})) + \\log p(\\mathbf{y}) - \\log q_\\vartheta(\\mathbf{y}| \\mathbf{x})\\bigg) \\\\ \u0026 \\qquad \\qquad + \\log p(\\mathbf{z}(\\boldsymbol\\epsilon, \\mathbf{x},\\mathbf{y})) - \\log q_\\varphi(\\mathbf{z}(\\boldsymbol\\epsilon, \\mathbf{x},\\mathbf{y})| \\mathbf{x},\\mathbf{y})\\Bigg] \\end{split}\\) where \\(\\mathbf{z}(\\boldsymbol\\epsilon, \\mathbf{x},\\mathbf{y}) = \\boldsymbol\\mu_\\varphi(\\mathbf{x},\\mathbf{y}) + \\boldsymbol\\epsilon \\cdot \\sigma_\\varphi(\\mathbf{x},\\mathbf{y})\\) represents the \\(\\mathbf{z}\\) component in the output of Decoder_z and \\(q_{\\vartheta}(\\mathbf{y}| \\mathbf{x})\\) represents the output of Classifier.\nBy analysing the above equation we can observe the following:\n Since \\(q_\\vartheta(\\mathbf{y}| \\mathbf{x}) = \\prod_{j=1}^C \\pi_{\\vartheta,j}(\\mathbf{x})^{y_j}\\) and \\(\\mathbf{y}\\) ranges over all possible classes, we can simplify the first part of the right-hand side of the equation with \\(\\sum_{j = 1}^C \\pi_{\\vartheta,j}(\\mathbf{x})\\bigg(p_\\theta(\\mathbf{x}| \\mathbf{z}(\\epsilon,\\mathbf{x},\\mathbf{y}), \\mathbf{e}_j) + \\log \\pi_j - \\log \\pi_{\\vartheta,j}(\\mathbf{x})\\bigg)\\), where \\(\\mathbf{e}_j\\) is the vector of all zeros except for position \\(j\\) and \\(\\pi_{\\vartheta,j}(\\mathbf{x})\\) is the \\(j\\)-th component of \\(\\pi_{\\vartheta}(\\mathbf{x})\\).\n Further, by exploiting the definitions, \\(\\mathbb{E}_{\\boldsymbol\\epsilon\\sim \\mathcal{N}(\\mathbf{0},I_K)}\\left[\\log p(\\mathbf{z}(\\boldsymbol\\epsilon, \\mathbf{x},\\mathbf{y})) - \\log q_\\varphi(\\mathbf{z}(\\boldsymbol\\epsilon, \\mathbf{x},\\mathbf{y})| \\mathbf{x},\\mathbf{y})\\right] = \\sum_{k} \\Bigg(\\log \\sigma_{\\varphi,k}(\\mathbf{x},\\mathbf{y}) + 1 - \\sigma_{\\varphi,k}(\\mathbf{x},\\mathbf{y}) - \\boldsymbol\\mu_{\\varphi,k}(\\mathbf{x},\\mathbf{y})^2\\Bigg)\\) and we see that the only source of nondeterminism is given by the component that computes the log-likelihood.\n  To summarize, the loss for an element \\(\\mathbf{x} \\in U\\) can be fully specified as follows:\n\\[\\begin{split} \\ell(\\mathbf{x})= \u0026 \\sum_{j = 1}^C \\pi_{\\vartheta,j}(\\mathbf{x})\\left(\\log \\pi_{\\vartheta,j}(\\mathbf{x}) - \\log \\pi_j \\right) \\\\ \u0026 - \\mathbb{E}_{\\boldsymbol\\epsilon\\sim \\mathcal{N(\\mathbf{0},I_K)}}\\left[\\sum_{j = 1}^C \\pi_{\\vartheta,j}(\\mathbf{x})p_\\theta(\\mathbf{x}| \\mathbf{z}(\\boldsymbol\\epsilon,\\mathbf{x},\\mathbf{e}_j), \\mathbf{e}_j)\\right]\\\\ \u0026 - \\sum_{j = 1}^C \\sum_{k} \\Bigg(\\log \\sigma_{\\varphi,k}(\\mathbf{x},\\mathbf{e}_j) + 1 - \\sigma_{\\varphi,k}(\\mathbf{x},\\mathbf{e}_j) - \\mu_{\\varphi,k}(\\mathbf{x},\\mathbf{e}_j)^2\\Bigg) \\end{split}\\]\nThe code snippet illustrating \\(\\ell(\\mathbf{x})\\) in PyTorch is the following.\ndef unsupervised_loss(x,encoder,decoder,classifier,num_classes,y_prior=1): y_q = classifier(x) kld_cat = torch.mean(torch.sum(y_q*(torch.exp(y_q) - torch.log(y_prior)),-1),-1) kld_norm = 0 e = torch.zeros(y_q.size()).to(x.device) prob_e = [] for j in range(num_classes): e[:,j] = 1. z, mu_q, logvar_q = encoder(x,e) kld_norm += torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1) prob_e.append(decoder(z)) e[:,j] = 0. kld_norm = torch.mean(kld_norm, -1) prob_e = torch.floatTensor(log_prob_e) prob_x = torch.matmul(llk_e,y_q).squeeze() loss = nn.BCELoss() llk = loss(prob_x,x) return llk + kld_cat + kld_norm  Here, since Decoder provides a probability distribution, the loss is the negative log likelihood. Again, we are assuming $\\mathbf{x}$ binary and the underlying probability is bernoullian on each feature.\nSupervised examples The case \\((\\mathbf{x},\\mathbf{y})\\in S\\) rensembles the unsupervised case, but with a major difference. For the labelled case, the joint probability \\(p(\\mathbf{x},\\mathbf{y},\\mathbf{z})\\) is decomposed as \\(p_\\theta(\\mathbf{x},\\mathbf{y},\\mathbf{z}) = q_{\\vartheta}(\\mathbf{y}|\\mathbf{x})p_\\theta(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})\\). In practice, we consider here a discriminative setting where the Classifier component as a part of the decoder. This is different from the unsupervised case, where \\(\\mathbf{y}\\) was considered a latent variable which encoded latent information from $\\mathbf{x}$ in a generative setting.\nAs a consequence, the joint likelihood can be approximated as\n\\[\\begin{split} \\log p(\\mathbf{x},\\mathbf{y}) \\geq \u0026 \\mathbb{E}_{q_\\varphi(\\mathbf{z}| \\mathbf{x},\\mathbf{y})}\\Bigg[\\log p_\\theta(\\mathbf{x}| \\mathbf{z}) + \\log q_\\vartheta(\\mathbf{y}|\\mathbf{x}) + \\log p(\\mathbf{z}) - \\log q_\\varphi(\\mathbf{z}| \\mathbf{x},\\mathbf{y})\\Bigg] \\end{split}\\]\nBy rearranging the formulas, we obtain the loss term for the supervised case:\n\\[\\begin{split}\\ell(\\mathbf{x},\\mathbf{y}) = \u0026 \\mathbb{E}_{\\boldsymbol\\epsilon\\sim \\mathcal{N}(0,1)}\\Bigg[\\log p_\\theta(\\mathbf{x}| \\mathbf{z}(\\boldsymbol\\epsilon,\\mathbf{x}, \\mathbf{y}))\\Bigg] \\\\ \u0026 + \\sum_{j=1}^C y_j \\log \\pi_{\\vartheta,j}(\\mathbf{x}) \\\\\u0026 + \\sum_{k} \\Bigg(\\log \\sigma_{\\varphi,k}(\\mathbf{x},\\mathbf{y}) + 1 - \\sigma_{\\varphi,k}(\\mathbf{x},\\mathbf{y}) - \\mu_{\\varphi,k}(\\mathbf{x},\\mathbf{y})^2\\Bigg)\\end{split}\\]\nThe corresponding example implementation:\ndef supervised_loss(x,y,encoder,decoder,classifier): z, mu_q, logvar_q = encoder(x,y) kld_norm = torch.mean(torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1),-1) prob_x = decoder(x) loss = nn.BCELoss() llk = loss(prob_x,x) y_q = classifier(x) loss = CrossEntropyLoss(dim=-1) llk_cat = loss(y_q,y) return llk + llk_cat + kld_norm  Other interpretations for the supervised case are possible: see, e.g. the treatment in Brian Keng's blog. However, I feel that separating the supervised and the unsupervised case and arranging the derivations accordingly is more intuitive.\nWrapping up We can finally combine all the above and devise a model for semi-supervised training:\nclass SSVAE(nn.Module): def __init__(self,input_size,num_classes,latent_size, y_prior = 1): self.input_size = input_size self.num_classes = num_classes self.latent_size = latent_size self.y_prior = y_prior self.encoder = Encoder(input_size,num_classes,latent_size) self.decoder = Decoder(latent_size,input_size) self.classifier = Classifier(input_size, num_classes) llk_loss = nn.BCELoss() cat_loss = nn.CrossEntropyLoss() def unsupervised_loss(self, x): y_q = self.classifier(x) kld_cat = torch.mean(torch.sum(y_q*(torch.log(y_q) - torch.log(self.y_prior)),-1),-1) kld_norm = 0 e = torch.zeros(y_q.size()).to(x.device) prob_e = [] for j in range(self.num_classes): e[:,j] = 1. z, mu_q, logvar_q = self.encoder(x,e) kld_norm += torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1) prob_e.append(self.decoder(z)) e[:,j] = 0. kld_norm = torch.mean(kld_norm, -1)) prob_e = torch.floatTensor(log_prob_e) prob_x = torch.matmul(llk_e,y_q).squeeze() llk = llk_loss(prob_x,x) return llk + kld_cat + kld_norm def supervised_loss(self,x,y): z, mu_q, logvar_q = self.encoder(x,y) kld_norm = torch.mean(torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1),-1) prob_x = self.decoder(x) llk = loss(prob_x,x) y_q = self.classifier(x) llk_cat = cat_loss(y_q,y) return llk + llk_cat + kld_norm def forward(self, x, y = None, train = True) if not train: return self.classifier(x) else: if y is not None: loss = self.supervised_loss(x,y) else loss = self.unsupervised_loss(x) return loss  We can observe that the model can be called in two modes: either in train mode or not. For the latter, it acts as a classifier and produces the class probabilities. In the training mode, on the other side, it computes either the supervised or the unsupervised, loss based on whether the data is labelled or not. The training procedure is quite straightforward as it just requires a data_loader capable of ranging over \\(D\\):\ndef train(data_loader,input_size, num_classes, latent_size, y_priors): model = SSVAE(input_size,num_classes, latent_size) optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001) for batch_idx,(x,y) in enumerate(data_loader): optimizer.zero_grad() loss = model(x,y) loss.backward() optimizer.step()  References [1] Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, Max Welling. Semi-supervised Learning with Deep Generative Models. Advances in Neural Information Processing Systems 27 (NIPS 2014), Montreal\n","date":1568131809,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568131809,"objectID":"aefb5630cf18e8eeb5c4a75d1cc30f31","permalink":"https://gmanco.github.io/post/on-semisupervised-vae/","publishdate":"2019-09-10T18:10:09+02:00","relpermalink":"/post/on-semisupervised-vae/","section":"post","summary":"In what follows I'll try to explain my basic understanding and interepretation of the semi-supervised framework based on Variational Autoencoders, as described in [1]. I shall assume a vector notation where bold symbols $\\mathbf{a}$ represent vectors, whose $j$-th component can be represented as $a_j$.\nThe starting point of the framework is to consider a dataset \\(D = S \\cup U\\), where:\n \\(S = \\{(\\mathbf{x}_1, \\mathbf{y}_1), \\ldots, (\\mathbf{x}_n, \\mathbf{y}_n)\\}\\),\n \\(U = \\{\\mathbf{x}_{n+1}, \\ldots, \\mathbf{x}_{n+m}\\}\\),  with \\(\\mathbf{x}_i \\in \\mathbb{R}^N\\) and \\(\\mathbf{y}_i \\in \\{0,1\\}^C\\) represents a one-hot encoding of a class in \\(\\{1, \\ldots, C\\}\\).","tags":["Variational Autoencoders","Generative Models","Semi-Supervised Learning"],"title":"Some notes on the Semi-Supervised Learning of Variational Autoencoders","type":"post"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://gmanco.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Giuseppe Manco","Ettore Ritacco","Nicola Barbieri"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"92fadd39e26ac4c18e4e877cb6769a15","permalink":"https://gmanco.github.io/publication/manco-2019/","publishdate":"2019-09-07T09:18:44.877085Z","relpermalink":"/publication/manco-2019/","section":"publication","summary":"In this paper we propose a survival factorization framework that models information cascades by tying together social influence patterns, topical structure and temporal dynamics. This is achieved through the introduction of a latent space which encodes: (a) the relevance of an information cascade on a topic; (b) the topical authoritativeness and the susceptibility of each individual involved in the information cascade, and (c) temporal topical patterns. By exploiting the cumulative properties of the survival function and of the likelihood of the model on a given adoption log, which records the observed activation times of users and side-information for each cascade, we show that the inference phase is linear in the number of users and in the number of adoptions. The evaluation on both synthetic and real-world data shows the effectiveness of the model in detecting the interplay between topics and social influence patterns, which ultimately provides high accuracy in predicting users activation times.","tags":["Social Influence","Information Diffusion","Social Network Analysis","Community Detection","Temporal Point Processes","Embedding","Generative Models"],"title":"A Factorization Approach for Survival Analysis on Diffusion Networks","type":"publication"},{"authors":["Noveen Sachdeva","Giuseppe Manco","Ettore Ritacco","Vikram Pudi"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"36136770e8928466cf6d4291ccfd4ad3","permalink":"https://gmanco.github.io/publication/sachdeva-2019/","publishdate":"2019-09-07T09:18:44.874439Z","relpermalink":"/publication/sachdeva-2019/","section":"publication","summary":"Variational autoencoders were proven successful in domains such as computer vision and speech processing. Their adoption for modeling user preferences is still unexplored, although recently it is starting to gain attention in the current literature. In this work, we propose a model which extends variational autoencoders by exploiting the rich information present in the past preference history. We introduce a recurrent version of the VAE, where instead of passing a subset of the whole history regardless of temporal dependencies, we rather pass the consumption sequence subset through a recurrent neural network. At each time-step of the RNN, the sequence is fed through a series of fully-connected layers, the output of which models the probability distribution of the most likely future preferences. We show that handling temporal information is crucial for improving the accuracy of the VAE: In fact, our model beats the current state-of-the-art by valuable margins because of its ability to capture temporal dependencies among the user-consumption sequence using the recurrent encoder still keeping the fundamentals of variational autoencoders intact.","tags":["Variational Autoencoders","Recurrent Networks","Sequence modeling","Collaborative Filtering","Deep Learning","Recommender Systems","Generative Models"],"title":"Sequential Variational Autoencoders for Collaborative Filtering","type":"publication"},{"authors":["Giuseppe Manco","Giuseppe Pirrò","Ettore Ritacco"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"9503dd9f9cd748dd797d456d374e6530","permalink":"https://gmanco.github.io/publication/manco-2018/","publishdate":"2019-09-07T09:18:44.891002Z","relpermalink":"/publication/manco-2018/","section":"publication","summary":"We tackle the problem of predict whether a target user (or group of users) will be active within an event stream before a time horizon. Our solution, called PATH, leverages recurrent neural networks to learn an embedding of the past events. The embedding allows to capture influence and susceptibility between users and places closer (the representation of) users that frequently get active in different event streams within a small time interval. We conduct an experimental evaluation on real world data and compare our approach with related work.","tags":["Social Influence","Information Diffusion","Deep Learning","Recurrent Networks","Temporal Point Processes"],"title":"Predicting Temporal Activation Patterns via Recurrent Neural Networks","type":"publication"},{"authors":["Giuseppe Manco","Ettore Ritacco","Pasquale Rullo","Lorenzo Gallucci","Will Astill","Dianne Kimber","Marco Antonelli"],"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509494400,"objectID":"24d06858f9d77e97fe9f38cf5247a9fc","permalink":"https://gmanco.github.io/publication/manco-2017/","publishdate":"2019-09-07T09:18:44.893116Z","relpermalink":"/publication/manco-2017/","section":"publication","summary":"Fault prediction is an important topic for the industry as, by providing effective methods for predictive maintenance, allows companies to perform important time and cost savings. In this paper we describe an application developed to predict and explain door failures on metro trains. To this end, the aim was twofold: first, devising prediction techniques capable of early detecting door failures from diagnostic data; second, describing failures in terms of properties distinguishing them from normal behavior. Data pre-processing was a complex task aimed at overcoming a number of issues with the dataset, like size, sparsity, bias, burst effect and trust. Since failure premonitory signals did not share common patterns, but were only characterized as non-normal device signals, fault prediction was performed by using outlier detection. Fault explanation was finally achieved by exhibiting device features showing abnormal values. An experimental evaluation was performed to assess the quality of the proposed approach. Results show that high-degree outliers are effective indicators of incipient failures. Also, explanation in terms of abnormal feature values (responsible for outlierness) seems to be quite expressive.There are some aspects in the proposed approach that deserve particular attention. We introduce a general framework for the failure detection problem based on an abstract model of diagnostic data, along with a formal problem statement. They both provide the basis for the definition of an effective data pre-processing technique where the behavior of a device, in a given time frame, is summarized through a number of suitable statistics. This approach strongly mitigates the issues related to data errors/noise, thus enabling to perform an effective outlier detection. All this, in our view, provides the grounds of a general methodology for advanced prognostic systems.","tags":["Fault Detection","Anomaly Detection","Outlier Explanation","Big Data","Sensor Data","Predictive Maintenance"],"title":"Fault detection and explanation through big data analysis on sensor streams","type":"publication"},{"authors":["Giuseppe Manco","Giuseppe Pirrò"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"f5981592235ff919d25c4517486177cc","permalink":"https://gmanco.github.io/publication/manco-2017-a/","publishdate":"2019-09-07T09:18:44.896224Z","relpermalink":"/publication/manco-2017-a/","section":"publication","summary":"The soaring amount of data coming from a variety of sources including social networks and mobile devices opens up new perspectives while at the same time posing new challenges. On one hand, AI-systems like Neural Networks paved the way toward new applications ranging from self-driving cars to text understanding. On the other hand, the management and analysis of data that fed these applications raises concerns about the privacy of data contributors. One robust (from the mathematical point of view) privacy definition is that of Differential Privacy (DP). The peculiarity of DP-based algorithms is that they do not work on anonymized versions of the data; they add a calibrated amount of noise before releasing the results, instead. The goals of this paper are: to give an overview on recent research results marrying DP and neural networks; to present a blueprint for differentially private neural networks; and, to discuss our findings and point out new research challenges.","tags":["Deep Learning","Differential Privacy"],"title":"Differential Privacy and Neural Networks: A Preliminary Analysis","type":"publication"},{"authors":["Nicola Barbieri","Giuseppe Manco","Ettore Ritacco"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"a6bab7156443905fc3c087016470c7d0","permalink":"https://gmanco.github.io/publication/barbieri-2017/","publishdate":"2019-09-07T09:18:44.905412Z","relpermalink":"/publication/barbieri-2017/","section":"publication","summary":"","tags":["Social Influence","Information Diffusion","Social Network Analysis","Community Detection","Temporal Point Processes","Embedding","Generative Models"],"title":"Survival Factorization on Diffusion Networks","type":"publication"},{"authors":["Nicola Barbieri","Francesco Bonchi","Giuseppe Manco"],"categories":null,"content":"","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"0f3611d5af8ec1d15892474902e569af","permalink":"https://gmanco.github.io/publication/barbieri-2016/","publishdate":"2019-09-07T09:18:44.894266Z","relpermalink":"/publication/barbieri-2016/","section":"publication","summary":"We study the problem of detecting social communities when the social graph is not available but instead we have access to a log of user activity, that is, a dataset of tuples (u, i, t) recording the fact that user u “adopted” item i at time t. We propose a stochastic framework that assumes that the adoption of items is governed by an underlying diffusion process over the unobserved social network and that such a diffusion model is based on community-level influence. That is, we aim at modeling communities through the lenses of social contagion. By fitting the model parameters to the user activity log, we learn the community membership and the level of influence of each user in each community. The general framework is instantiated with two different diffusion models, one with discrete time and one with continuous time, and we show that the computational complexity of both approaches is linear in the number of users and in the size of the propagation log. Experiments on synthetic data with planted community structure show that our methods outperform non-trivial baselines. The effectiveness of the proposed techniques is further validated on real-word data, on which our methods are able to detect high-quality communities.","tags":["Social Influence","Information Diffusion","Social Network Analysis","Community Detection","Temporal Point Processes","Generative Models"],"title":"Efficient Methods for Influence-Based Network-Oblivious Community Detection","type":"publication"},{"authors":["Giuseppe Manco","Pasquale Rullo","Lorenzo Gallucci","Mirko Paturzo"],"categories":null,"content":"","date":1475280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475280000,"objectID":"0d0949a67019a4e700b48b6519860c8d","permalink":"https://gmanco.github.io/publication/manco-2016/","publishdate":"2019-09-07T09:18:44.897728Z","relpermalink":"/publication/manco-2016/","section":"publication","summary":"A Knowledge Discovery (KD) process is a complex inter-disciplinary task, where different types of techniques coexist and cooperate for the purpose of extracting useful knowledge from large amounts of data. So, it is desirable having a unifying environment, built on a formal basis, where to design and perform the overall process. In this paper we propose a general framework which formalizes a KD process as an algebraic expression, that is, as a composition of operators representing elementary operations on two worlds: the data and the model worlds. Then, we describe a KD platform, named Rialto, based on such a framework. In particular, we provide the design principles of the underlying architecture, highlight the basic features, and provide a number of experimental results aimed at assessing the effectiveness of the design choices.","tags":["Knowledge Discovery Process","Data Mining","Business Analytics Platforms"],"title":"Rialto: A Knowledge Discovery suite for data analysis","type":"publication"},{"authors":["Shirley Coleman","Rainer Göb","Giuseppe Manco","Antonio Pievatolo","Xavier Tort-Martorell","Marco Seabra Reis"],"categories":null,"content":"","date":1462060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462060800,"objectID":"fe57696a5f66285140db50393e766a3e","permalink":"https://gmanco.github.io/publication/coleman-2016/","publishdate":"2019-09-07T09:18:44.899375Z","relpermalink":"/publication/coleman-2016/","section":"publication","summary":"Big data is big news, and large companies in all sectors are making significant advances in their customer relations, product selection and development and consequent profitability through using this valuable commodity. Small and medium enterprises (SMEs) have proved themselves to be slow adopters of the new technology of big data analytics and are in danger of being left behind. In Europe, SMEs are a vital part of the economy, and the challenges they encounter need to be addressed as a matter of urgency. This paper identifies barriers to SME uptake of big data analytics and recognises their complex challenge to all stakeholders, including national and international policy makers, IT, business management and data science communities.  The paper proposes a big data maturity model for SMEs as a first step towards an SME roadmap to data analytics. It considers the ‘state‐of‐the‐art’ of IT with respect to usability and usefulness for SMEs and discusses how SMEs can overcome the barriers preventing them from adopting existing solutions. The paper then considers management perspectives and the role of maturity models in enhancing and structuring the adoption of data analytics in an organisation. The history of total quality management is reviewed to inform the core aspects of implanting a new paradigm. The paper concludes with recommendations to help SMEs develop their big data capability and enable them to continue as the engines of European industrial and business success. ","tags":["Big Data"],"title":"How Can SMEs Benefit from Big Data? Challenges and a Path Forward","type":"publication"},{"authors":["Fabrizio Angiulli","Fabio Fassetti","Giuseppe Manco","Luigi Palopoli"],"categories":null,"content":"","date":1456790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456790400,"objectID":"012d7da8acfb1ddeec504c89e982c523","permalink":"https://gmanco.github.io/publication/angiulli-2016/","publishdate":"2019-09-07T09:18:44.892117Z","relpermalink":"/publication/angiulli-2016/","section":"publication","summary":"The outlying property detection problem (OPDP) is the problem of discovering the properties distinguishing a given object, known in advance to be an outlier in a database, from the other database objects. This problem has been recently analyzed focusing on categorical attributes only. However, numerical attributes are very relevant and widely used in databases. Therefore, in this paper, we analyze the OPDP within a context where also numerical attributes are taken into account, which represents a relevant case left open in the literature. As major contributions, we present an efficient parameter-free algorithm to compute the measure of object exceptionality we introduce, and propose a unified framework for mining exceptional properties in the presence of both categorical and numerical attributes.","tags":["Anomaly Detection","Explanation","Outlier Explanation","Probabilistic Modeling","Density Estimation"],"title":"Outlying property detection with numerical attributes","type":"publication"},{"authors":["Nicola Barbieri","Giuseppe Manco","Ettore Ritacco"],"categories":null,"content":"","date":1398902400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398902400,"objectID":"39b06ab51e7c95d4aa1d566867c653c6","permalink":"https://gmanco.github.io/publication/barbieri-2014/","publishdate":"2019-09-07T09:18:44.900808Z","relpermalink":"/publication/barbieri-2014/","section":"publication","summary":"The importance of accurate recommender systems has been widely recognized by academia and industry, and recommendation is rapidly becoming one of the most successful applications of data mining and machine learning. Understanding and predicting the choices and preferences of users is a challenging task: real-world scenarios involve users behaving in complex situations, where prior beliefs, specific tendencies, and reciprocal influences jointly contribute to determining the preferences of users toward huge amounts of information, services, and products. Probabilistic modeling represents a robust formal mathematical framework to model these assumptions and study their effects in the recommendation process.  This book starts with a brief summary of the recommendation problem and its challenges and a review of some widely used techniques Next, we introduce and discuss probabilistic approaches for modeling preference data. We focus our attention on methods based on latent factors, such as mixture models, probabilistic matrix factorization, and topic models, for explicit and implicit preference data. These methods represent a significant advance in the research and technology of recommendation. The resulting models allow us to identify complex patterns in preference data, which can be exploited to predict future purchases effectively.  The extreme sparsity of preference data poses serious challenges to the modeling of user preferences, especially in the cases where few observations are available. Bayesian inference techniques elegantly address the need for regularization, and their integration with latent factor modeling helps to boost the performances of the basic techniques.  We summarize the strengths and weakness of several approaches by considering two different but related evaluation perspectives, namely, rating prediction and recommendation accuracy. Furthermore, we describe how probabilistic methods based on latent factors enable the exploitation of preference patterns in novel applications beyond rating prediction or recommendation accuracy.  We finally discuss the application of probabilistic techniques in two additional scenarios, characterized by the availability of side information besides preference data. In summary, the book categorizes the myriad probabilistic approaches to recommendations and provides guidelines for their adoption in real-world situations.","tags":["Generative Models","Matrix Factorization","Topic Models","Recommender Systems","Collaborative Filtering","Social Network Analysis"],"title":"Probabilistic Approaches to Recommendations","type":"publication"},{"authors":["Gianni Costa","Giuseppe Manco","Riccardo Ortale"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"a4abe9e108a1fee568d35153a56168ab","permalink":"https://gmanco.github.io/publication/costa-2014/","publishdate":"2019-09-07T09:18:44.866796Z","relpermalink":"/publication/costa-2014/","section":"publication","summary":"A Bayesian generative model is presented for recommending interesting items and trustworthy users to the targeted users in social rating networks with asymmetric and directed trust relationships. The proposed model is the first unified approach to the combination of the two recommendation tasks. Within the devised model, each user is associated with two latent-factor vectors, i.e., her susceptibility and expertise. Items are also associated with corresponding latent-factor vector representations. The probabilistic factorization of the rating data and trust relationships is exploited to infer user susceptibility and expertise. Statistical social-network modeling is instead used to constrain the trust relationships from a user to another to be governed by their respective susceptibility and expertise. The inherently ambiguous meaning of unobserved trust relationships between users is suitably disambiguated. An intensive comparative experimentation on real-world social rating networks with trust relationships demonstrates the superior predictive performance of the presented model in terms of RMSE and AUC.","tags":["Collaborative Filtering","Matrix Factorization","Social Network Analysis","Recommender Systems","Embedding"],"title":"A Generative Bayesian Model for Item and User Recommendation in Social Rating Networks with Trust Relationships","type":"publication"},{"authors":["Nicola Barbieri","Francesco Bonchi","Giuseppe Manco"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"6bee2fb5d168ceaa8aa7d1de73b84052","permalink":"https://gmanco.github.io/publication/barbieri-2014-a/","publishdate":"2019-09-07T09:18:44.904056Z","relpermalink":"/publication/barbieri-2014-a/","section":"publication","summary":"User recommender systems are a key component in any on-line social networking platform: they help the users growing their network faster, thus driving engagement and loyalty. In this paper we study link prediction with explanations for user recommendation in social networks. For this problem we propose WTFW (\"Who to Follow and Why\"), a stochastic topic model for link prediction over directed and nodes-attributed graphs. Our model not only predicts links, but for each predicted link it decides whether it is a \"topical\" or a \"social\" link, and depending on this decision it produces a different type of explanation. A topical link is recommended between a user interested in a topic and a user authoritative in that topic: the explanation in this case is a set of binary features describing the topic responsible of the link creation. A social link is recommended between users which share a large social neighborhood: in this case the explanation is the set of neighbors which are more likely to be responsible for the link creation. Our experimental assessment on real-world data confirms the accuracy of WTFW in the link prediction and the quality of the associated explanations.","tags":["Social Influence","Information Diffusion","Social Network Analysis","Link Prediction","Generative Models","Topic Modeling"],"title":"Who to follow and why","type":"publication"},{"authors":["Nicola Barbieri","Francesco Bonchi","Giuseppe Manco"],"categories":null,"content":"","date":1385856000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1385856000,"objectID":"6ad73003c3dd06decb1ce4b9f0bf7a53","permalink":"https://gmanco.github.io/publication/barbieri-2013-a/","publishdate":"2019-09-07T09:18:44.871284Z","relpermalink":"/publication/barbieri-2013-a/","section":"publication","summary":"How can we detect communities when the social graphs is not available? We tackle this problem by modeling social contagion from a log of user activity, that is a dataset of tuples (u, i, t) recording the fact that user u \"adopted\" item i at time t. This is the only input to our problem. We propose a stochastic framework which assumes that item adoptions are governed by un underlying diffusion process over the unobserved social network, and that such diffusion model is based on community-level influence. By fitting the model parameters to the user activity log, we learn the community membership and the level of influence of each user in each community. This allows to identify for each community the \"key\" users, i.e., the leaders which are most likely to influence the rest of the community to adopt a certain item. The general framework can be instantiated with different diffusion models. In this paper we define two models: the extension to the community level of the classic (discrete time) Independent Cascade model, and a model that focuses on the time delay between adoptions. To the best of our knowledge, this is the first work studying community detection without the network.","tags":["Social Influence","Information Diffusion","Social Network Analysis","Community Detection","Temporal Point Processes","Generative Models"],"title":"Influence-Based Network-Oblivious Community Detection","type":"publication"},{"authors":["Gianni Costa","Giuseppe Manco","Elio Masciari"],"categories":null,"content":"","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"771c53d679bd0df7b61d16f7642f202f","permalink":"https://gmanco.github.io/publication/costa-2013-a/","publishdate":"2019-09-07T09:18:44.902128Z","relpermalink":"/publication/costa-2013-a/","section":"publication","summary":"Nowadays, almost all kind of electronic devices leave traces of their movements (e.g. smartphone, GPS devices and so on). Thus, the huge number of this “tiny” data sources leads to the generation of massive data streams of geo-referenced data. As a matter of fact, the effective analysis of such amounts of data is challenging, since the possibility to extract useful information from this peculiar kind of data is crucial in many application scenarios such as vehicle traffic management, hand-off in cellular networks, supply chain management. Moreover, spatial data streams management poses new challenges both for their proper definition and acquisition, thus making the overall process harder than for classical point data. In particular, we are interested in solving the problem of effective trajectory data streams clustering, that revealed really intriguing as we deal with sequential data that have to be properly managed due to their ordering. We propose a framework that allow data pre-elaboration in order to make the mining step more effective. As for every data mining tool, the experimental evaluation is crucial, thus we performed several tests on real world datasets that confirmed the efficiency and effectiveness of the proposed approach.","tags":["Spatial Data"," Math transforms","Clustering"],"title":"Dealing with trajectory streams by clustering and mathematical transforms","type":"publication"},{"authors":["Nicola Barbieri","Giuseppe Manco","Ettore Ritacco","Marco Carnuccio","Antonio Bevacqua"],"categories":null,"content":"","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"9abcdba862d257d45919745dd7479715","permalink":"https://gmanco.github.io/publication/barbieri-2013-b/","publishdate":"2019-09-07T09:18:44.88836Z","relpermalink":"/publication/barbieri-2013-b/","section":"publication","summary":"Probabilistic topic models are widely used in different contexts to uncover the hidden structure in large text corpora. One of the main (and perhaps strong) assumption of these models is that generative process follows a bag-of-words assumption, i.e. each token is independent from the previous one. We extend the popular Latent Dirichlet Allocation model by exploiting three different conditional Markovian assumptions: (i) the token generation depends on the current topic and on the previous token; (ii) the topic associated with each observation depends on topic associated with the previous one; (iii) the token generation depends on the current and previous topic. For each of these modeling assumptions we present a Gibbs Sampling procedure for parameter estimation. Experimental evaluation over real-word data shows the performance advantages, in terms of recall and precision, of the sequence-modeling approaches.","tags":["Recommender Systems","Collaborative Filtering","Topic Models","Generative Models"],"title":"Probabilistic topic models for sequence data","type":"publication"},{"authors":["Nicola Barbieri","Francesco Bonchi","Giuseppe Manco"],"categories":null,"content":"","date":1364774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1364774400,"objectID":"fb695ddcd75bb2c6f9427de70db098e9","permalink":"https://gmanco.github.io/publication/barbieri-2013-c/","publishdate":"2019-09-07T19:16:42.66436Z","relpermalink":"/publication/barbieri-2013-c/","section":"publication","summary":"","tags":["Social Influence","Topic Models","Probabilistic Modeling","Information Diffusion"],"title":"Topic-aware social influence propagation models","type":"publication"},{"authors":["Gianni Costa","Giuseppe Manco","Riccardo Ortale","Ettore Ritacco"],"categories":null,"content":"","date":1362096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1362096000,"objectID":"7414e297aff312f9ed9119f3560eb959","permalink":"https://gmanco.github.io/publication/costa-2013/","publishdate":"2019-09-07T09:18:44.883865Z","relpermalink":"/publication/costa-2013/","section":"publication","summary":"Clustering XML documents by structure is the task of grouping them by common structural components. Hitherto, this has been accomplished by looking at the occurrence of one preestablished type of structural components in the structures of the XML documents. However, the a-priori chosen structural components may not be the most appropriate for effective clustering. Moreover, it is likely that the resulting clusters exhibit a certain extent of inner structural inhomogeneity, because of uncaught differences in the structures of the XML documents, due to further neglected forms of structural components.  To overcome these limitations, a new hierarchical approach is proposed, that allows to consider (if necessary) multiple forms of structural components to isolate structurally-homogeneous clusters of XML documents. At each level of the resulting hierarchy, clusters are divided by considering some type of structural components (unaddressed at the preceding levels), that still differentiate the structures of the XML documents. Each cluster in the hierarchy is summarized through a novel technique, that provides a clear and differentiated understanding of its structural properties.","tags":["Clustering","XML/XSL/RDF","Text Mining"],"title":"Hierarchical clustering of XML documents focused on structural components","type":"publication"},{"authors":["Nicola Barbieri","Francesco Bonchi","Giuseppe Manco"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"48c4b02379dd3a6fb64543abbedafebb","permalink":"https://gmanco.github.io/publication/barbieri-2013/","publishdate":"2019-09-07T09:18:44.869871Z","relpermalink":"/publication/barbieri-2013/","section":"publication","summary":"Given a directed social graph and a set of past informa- tion cascades observed over the graph, we study the novel problem of detecting modules of the graph (communities of nodes), that also explain the cascades. Our key observation is that both information propagation and social ties forma- tion in a social network can be explained according to the same latent factor, which ultimately guide a user behavior within the network. Based on this observation, we propose the Community-Cascade Network (CCN) model, a stochas- tic mixture membership generative model that can fit, at the same time, the social graph and the observed set of cas- cades. Our model produces overlapping communities and for each node, its level of authority and passive interest in each community it belongs. For learning the parameters of the CCN model, we devise a Generalized Expectation Maximization procedure. We then apply our model to real-world social networks and in- formation cascades: the results witness the validity of the proposed CCN model, providing useful insights on its signif- icance for analyzing social behavior.","tags":["Social Influence","Topic Models","Community Detection","Generative Models","Social Network Analysis"],"title":"Cascade-based community detection","type":"publication"},{"authors":["Nicola Barbieri","Francesco Bonchi","Giuseppe Manco"],"categories":null,"content":"","date":1354320000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1354320000,"objectID":"352df88f3645218d0fc66ae45e522983","permalink":"https://gmanco.github.io/publication/barbieri-2012-a/","publishdate":"2019-09-07T09:18:44.875598Z","relpermalink":"/publication/barbieri-2012-a/","section":"publication","summary":"We study social influence from a topic modeling perspective. We introduce novel topic-aware influence-driven propagation models that experimentally result to be more accurate in describing real-world cascades than the standard propagation models studied in the literature. In particular, we first propose simple topic-aware extensions of the well-known Independent Cascade and Linear Threshold models. Next, we propose a different approach explicitly modeling authoritativeness, influence and relevance under a topic-aware perspective. We devise methods to learn the parameters of the models from a dataset of past propagations. Our experimentation confirms the high accuracy of the proposed models and learning schemes.","tags":["Social Influence","Topic Models","Probabilistic Modeling","Information Diffusion"],"title":"Topic-Aware Social Influence Propagation Models","type":"publication"},{"authors":["Nicola Barbieri","Giuseppe Manco","Riccardo Ortale","Ettore Ritacco"],"categories":null,"content":"","date":1333238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1333238400,"objectID":"02c86828bd713baac70caab874665ec8","permalink":"https://gmanco.github.io/publication/barbieri-2012/","publishdate":"2019-09-07T09:18:44.868885Z","relpermalink":"/publication/barbieri-2012/","section":"publication","summary":"Recent works in Recommender Systems (RS) have investigated the relationships between the prediction accuracy, i.e. the ability of a RS to minimize a cost function (for instance the RMSE measure) in estimating users' preferences, and the accuracy of the recommendation list provided to users. State-of-the-art recommendation algorithms, which focus on the minimization of RMSE, have shown to achieve weak results from the recommendation accuracy perspective, and vice versa. In this work we present a novel Bayesian probabilistic hierarchical approach for users' preference data, which is designed to overcome the limitation of current methodologies and thus to meet both prediction and recommendation accuracy. According to the generative semantics of this technique, each user is modeled as a random mixture over latent factors, which identify users community interests. Each individual user community is then modeled as a mixture of topics, which capture the preferences of the members on a set of items. We provide two different formalization of the basic hierarchical model: BH-Forced focuses on rating prediction, while BH-Free models both the popularity of items and the distribution over item ratings. The combined modeling of item popularity and rating provides a powerful framework for the generation of highly accurate recommendations. An extensive evaluation over two popular benchmark datasets reveals the effectiveness and the quality of the proposed algorithms, showing that BH-Free realizes the most satisfactory compromise between prediction and recommendation accuracy with respect to several state-of-the-art competitors.   Read More: https://epubs.siam.org/doi/10.1137/1.9781611972825.89","tags":["Recommender Systems","Collaborative Filtering","Probabilistic Modeling"],"title":"Balancing Prediction and Recommendation Accuracy: Hierarchical Latent Factors for Preference Data","type":"publication"},{"authors":["Nicola Barbieri","Antonio Bevacqua","Marco Carnuccio","Giuseppe Manco","Ettore Ritacco"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"23a1e0d189346e7b6b03ac464b38cc00","permalink":"https://gmanco.github.io/publication/barbieri-bcmr-12/","publishdate":"2019-09-07T09:18:44.873364Z","relpermalink":"/publication/barbieri-bcmr-12/","section":"publication","summary":"Probabilistic topic models are widely used in different contexts to uncover the hidden structure in large text corpora. One of the main features of these models is that generative process follows a bag-of-words assump- tion, i.e each token is independent from the previous one. We extend the popular Latent Dirichlet Allocation model by exploiting a conditional Markovian assumptions, where the token generation depends on the current topic and on the previous token. The resulting model is capable of accommodating temporal correlations among tokens, which better model user behavior. This is particularly significant in a collaborative filtering context, where the choice of a user can be exploited for recommendation purposes, and hence a more realistic and accurate modeling enables better recommendations. For the mentioned model we present a fast Gibbs Sampling procedure for the parameters estimation. A thorough experimental evaluation over real-word data shows the performance advantages, in terms of recall and precision, of the proposed sequence-modeling approach. ","tags":["Recommender Systems","Collaborative Filtering","Topic Models","Generative Models"],"title":"Probabilistic Sequence Modeling for Recommender Systems","type":"publication"},{"authors":["Gianni Costa","Giuseppe Manco","Riccardo Ortale","Ettore Ritacco"],"categories":null,"content":"","date":1322697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1322697600,"objectID":"63e9d905cf8b1c93061702907985db4e","permalink":"https://gmanco.github.io/publication/costa-2011/","publishdate":"2019-09-07T09:18:44.882922Z","relpermalink":"/publication/costa-2011/","section":"publication","summary":"We propose two models for improving the performance of rule-based classification under unbalanced and highly imprecise domains. Both models are probabilistic frameworks aimed to boost the performance of basic rule-based classifiers. The first model implements a global-to-local scheme, where the response of a global rule-based classifier is refined by performing a probabilistic analysis of the coverage of its rules. In particular, the coverage of the individual rules is used to learn local probabilistic models, which ultimately refine the predictions from the corresponding rules of the global classifier. The second model implements a dual local-to-global strategy, in which single classification rules are combined within an exponential probabilistic model in order to boost the overall performance as a side effect of mutual influence. Several variants of the basic ideas are studied, and their performances are thoroughly evaluated and compared with state-of-the-art algorithms on standard benchmark datasets.","tags":["Rule Learning","Pattern Discovery","Supervised Learning\"","Probabilistic Modeling"],"title":"From global to local and viceversa: uses of associative rule learning for classification in imprecise environments","type":"publication"},{"authors":["Nicola Barbieri","Giuseppe Manco","Ettore Ritacco"],"categories":null,"content":"","date":1301616000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1301616000,"objectID":"c27195e2c4c989ce357df8bf07fa4379","permalink":"https://gmanco.github.io/publication/barbieri-2011-a/","publishdate":"2019-09-07T09:18:44.867854Z","relpermalink":"/publication/barbieri-2011-a/","section":"publication","summary":"This paper presents a hierarchical probabilistic approach to collaborative filtering which allows the discovery and analysis of both global patterns (i.e., tendency of some products of being ‘universally appreciated’) and local patterns (tendency of users within a community to express a common preference on the same group of items). We reformulate the collaborative filtering approach as a clustering problem in a high-dimensional setting, and propose a probabilistic approach to model the data. The core of our approach is a co-clustering strategy, arranged in a hierarchical fashion: first, user communities are discovered, and then the information provided by each user community is used to discover topics, grouping items into categories. The resulting probabilistic framework can be used for detecting interesting relationships between users and items within user communities. The experimental evaluation shows that the proposed model achieves a competitive prediction accuracy with respect to the state-of-art collaborative filtering approaches.","tags":["Recommender Systems","Collaborative Filtering","Probabilistic Modeling"],"title":"A Probabilistic Hierarchical Approach for Pattern Discovery in Collaborative Filtering Data","type":"publication"},{"authors":["Nicola Barbieri","Giuseppe Manco"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"248733af9796133cd02548f4b7819a54","permalink":"https://gmanco.github.io/publication/barbieri-2011/","publishdate":"2019-09-07T09:18:44.865034Z","relpermalink":"/publication/barbieri-2011/","section":"publication","summary":"In this work we perform an analysis of probabilistic approaches to recommendation upon a different validation perspective, which focuses on accuracy metrics such as recall and precision of the recommendation list. Traditionally, state-of-art approches to recommendations consider the recommendation process from a “missing value prediction” perspective. This approach simplifies the model validation phase that is based on the minimization of standard error metrics such as RMSE. However, recent studies have pointed several limitations of this approach, showing that a lower RMSE does not necessarily imply improvements in terms of specific recommendations. We demonstrate that the underlying probabilistic framework offers several advantages over traditional methods, in terms of flexibility in the generation of the recommendation list and consequently in the accuracy of recommendation.","tags":["Recommender Systems","Collaborative Filtering","Probabilistic Modeling","Matrix Factorization","Topic Models"],"title":"An Analysis of Probabilistic Methods for Top-N Recommendation in Collaborative Filtering","type":"publication"},{"authors":["Nicola Barbieri","Gianni Costa","Giuseppe Manco","Riccardo Ortale"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"0d4d5c709cefd29dee1d63854eac7eff","permalink":"https://gmanco.github.io/publication/barbieri-2011-b/","publishdate":"2019-09-07T09:18:44.872528Z","relpermalink":"/publication/barbieri-2011-b/","section":"publication","summary":"We propose a bayesian probabilistic model for explicit preference data. The model introduces a generative process, which takes into account both item selection and rating emission to gather into communities those users who experience the same items and tend to adopt the same rating pattern. Each user is modeled as a random mixture of topics, where each topic is characterized by a distribution modeling the popularity of items within the respective user-community and by a distribution over preference values for those items. The proposed model can be associated with a novel item-relevance ranking criterion, which is based both on item popularity and user's preferences. We show that the proposed model, equipped with the new ranking criterion, outperforms state-of-art approaches in terms of accuracy of the recommendation list provided to users on standard benchmark datasets.","tags":["Recommender Systems","Collaborative Filtering","Probabilistic Modeling"],"title":"Modeling item selection and relevance for accurate recommendations","type":"publication"},{"authors":["Gianni Costa","Giuseppe Manco","Riccardo Ortale"],"categories":null,"content":"","date":1254355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1254355200,"objectID":"0fc74d0452b89a832b826adc2f1daa9e","permalink":"https://gmanco.github.io/publication/costa-2009/","publishdate":"2019-09-07T09:18:44.878072Z","relpermalink":"/publication/costa-2009/","section":"publication","summary":"We propose an incremental technique for discovering duplicates in large databases of textual sequences, i.e., syntactically different tuples, that refer to the same real-world entity. The problem is approached from a clustering perspective: given a set of tuples, the objective is to partition them into groups of duplicate tuples. Each newly arrived tuple is assigned to an appropriate cluster via nearest-neighbor classification. This is achieved by means of a suitable hash-based index, that maps any tuple to a set of indexing keys and assigns tuples with high syntactic similarity to the same buckets. Hence, the neighbors of a query tuple can be efficiently identified by simply retrieving those tuples that appear in the same buckets associated to the query tuple itself, without completely scanning the original database. Two alternative schemes for computing indexing keys are discussed and compared. An extensive experimental evaluation on both synthetic and real data shows the effectiveness of our approach.","tags":["Clustering","Indexing","Hashing","similarity measures","Entity resolution","Deduplication"],"title":"An incremental clustering scheme for data de-duplication","type":"publication"},{"authors":["E. Cesario","G. Manco","R. Ortale"],"categories":null,"content":"","date":1196467200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1196467200,"objectID":"166983565a1c198614c9239ad956de13","permalink":"https://gmanco.github.io/publication/cesario-2007-a/","publishdate":"2019-09-07T09:18:44.889808Z","relpermalink":"/publication/cesario-2007-a/","section":"publication","summary":"A parameter-free, fully-automatic approach to clustering high-dimensional categorical data is proposed. The technique is based on a two-phase iterative procedure, which attempts to improve the overall quality of the whole partition. In the first phase, cluster assignments are given, and a new cluster is added to the partition by identifying and splitting a low-quality cluster. In the second phase, the number of clusters is fixed, and an attempt to optimize cluster assignments is done. On the basis of such features, the algorithm attempts to improve the overall quality of the whole partition and finds clusters in the data, whose number is naturally established on the basis of the inherent features of the underlying data set rather than being previously specified. Furthermore, the approach is parametric to the notion of cluster quality: Here, a cluster is defined as a set of tuples exhibiting a sort of homogeneity. We show how a suitable notion of cluster homogeneity can be defined in the context of high-dimensional categorical data, from which an effective instance of the proposed clustering scheme immediately follows. Experiments on both synthetic and real data prove that the devised algorithm scales linearly and achieves nearly optimal results in terms of compactness and separation.","tags":["Clustering","Transactional Data","Information Retrieval"],"title":"Top-Down Parameter-Free Clustering of High-Dimensional Categorical Data","type":"publication"},{"authors":["Gianluigi Greco","Antonella Guzzo","Giuseppe Manco","Domenico Saccà"],"categories":null,"content":"","date":1183248000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1183248000,"objectID":"30b6d7bbc14667bf5496618a7d56b3db","permalink":"https://gmanco.github.io/publication/greco-2007/","publishdate":"2019-09-07T09:18:44.887112Z","relpermalink":"/publication/greco-2007/","section":"publication","summary":"General patterns of execution that have been frequently scheduled by a workflow management system provide the administrator with previously unknown, and potentially useful information, e.g., about the existence of unexpected causalities between subprocesses of a given workflow. This paper investigates the problem of mining unconnected patterns on the basis of some execution traces, i.e., of detecting sets of activities exhibiting no explicit dependency relationships that are frequently executed together. The problem is faced in the paper by proposing and analyzing two algorithms. One algorithm takes into account information about the structure of the control-flow graph only, while the other is a smart refinement where the knowledge of the frequencies of edges and activities in the traces at hand is also accounted for, by means of a sophisticated graphical analysis. Both algorithms have been implemented and integrated into a system prototype, which may profitably support the enactment phase of the workflow. The correctness of the two algorithms is formally proven, and several experiments are reported to evidence the ability of the graphical analysis to significantly improve the performances, by dramatically pruning the search space of candidate patterns.","tags":["Pattern discovery","Graph Mining","Workflow Mining"],"title":"Mining unconnected patterns in workflows","type":"publication"},{"authors":["Eugenio Cesario","Francesco Folino","Antonio Locane","Giuseppe Manco","Riccardo Ortale"],"categories":null,"content":"","date":1180656000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1180656000,"objectID":"9eb57127a252da29104ffb80a923daca","permalink":"https://gmanco.github.io/publication/cesario-2007/","publishdate":"2019-09-07T09:18:44.879152Z","relpermalink":"/publication/cesario-2007/","section":"publication","summary":"A novel approach for reconciling tuples stored as free text into an existing attribute schema is proposed. The basic idea is to subject the available text to progressive classification, i.e., a multi-stage classification scheme where, at each intermediate stage, a classifier is learnt that analyzes the textual fragments not reconciled at the end of the previous steps. Classification is accomplished by an ad hoc exploitation of traditional association mining algorithms, and is supported by a data transformation scheme which takes advantage of domain-specific dictionaries/ontologies. A key feature is the capability of progressively enriching the available ontology with the results of the previous stages of classification, thus significantly improving the overall classification accuracy. An extensive experimental evaluation shows the effectiveness of our approach.","tags":["Supervised Learning","Schema Reconciliation","Text Segmentation"],"title":"Boosting text segmentation via progressive classification","type":"publication"},{"authors":["Sergio Flesca","Giuseppe Manco","Elio Masciari","Luigi Pontieri","Andrea Pugliese"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"69865ec9aeae0da1fc331f35f6c9da57","permalink":"https://gmanco.github.io/publication/flesca-2007/","publishdate":"2019-09-07T09:18:44.880158Z","relpermalink":"/publication/flesca-2007/","section":"publication","summary":"In this paper, we propose a classification technique for Web pages, based on the detection of structural similarities among semistructured documents, and devise an architecture exploiting such technique for the purpose of information extraction. The proposal significantly differs from standard methods based on graph-matching algorithms, and is based on the idea of representing the structure of a document as a time series in which each occurrence of a tag corresponds to an impulse. The degree of similarity between documents is then stated by analyzing the frequencies of the corresponding Fourier transform. Experiments on real data show the effectiveness of the proposed technique.","tags":["Semistructured data","Wrapping","WWW tools"],"title":"Exploiting structural similarity for effective Web information extraction","type":"publication"},{"authors":["Giuseppe Manco","Elio Masciari","Andrea Tagarelli"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"127b0abc42e265e1a8a3b06ba250b1d7","permalink":"https://gmanco.github.io/publication/manco-2007/","publishdate":"2019-09-07T09:18:44.885957Z","relpermalink":"/publication/manco-2007/","section":"publication","summary":"The continuous exchange of information by means of the popular email service has raised the problem of managing the huge amounts of messages received from users in an effective and efficient way. We deal with the problem of email classification by conceiving suitable strategies for: (1) organizing messages into homogeneous groups, (2) redirecting further incoming messages according to an initial organization, and (3) building reliable descriptions of the message groups discovered. We propose a unified framework for handling and classifying email messages. In our framework, messages sharing similar features are clustered in a folder organization. Clustering and pattern discovery techniques for mining structured and unstructured information from email messages are the basis of an overall process of folder creation/maintenance and email redirection. Pattern discovery is also exploited for generating suitable cluster descriptions that play a leading role in cluster updating. Experimental evaluation performed on several personal mailboxes shows the effectiveness of our approach.","tags":["Email classification","Text Mining","Clustering","Pattern discovery"],"title":"Mining categories for emails via clustering and pattern discovery","type":"publication"},{"authors":["G. Greco","A. Guzzo","G. Manco","D. Sacca"],"categories":null,"content":"","date":1112313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1112313600,"objectID":"d41407ea9acba0e933cddaad58272d84","permalink":"https://gmanco.github.io/publication/greco-2005/","publishdate":"2019-09-07T09:18:44.884838Z","relpermalink":"/publication/greco-2005/","section":"publication","summary":"Today's workflow management systems represent a key technological infrastructure for advanced applications that is attracting a growing body of research, mainly focused in developing tools for workflow management, that allow users both to specify the \"static\" aspects, like preconditions, precedences among activities, and rules for exception handling, and to control its execution by scheduling the activities on the available resources. This paper deals with an aspect of workflows which has so far not received much attention even though it is crucial for the forthcoming scenarios of large scale applications on the Web: providing facilities for the human system administrator for identifying the choices performed more frequently in the past that had lead to a desired final configuration. In this context, we formalize the problem of discovering the most frequent patterns of executions, i.e., the workflow substructures that have been scheduled more frequently by the system. We attacked the problem by developing two data mining algorithms on the basis of an intuitive and original graph formalization of a workflow schema and its occurrences. The model is used both to prove some intractability results that strongly motivate the use of data mining techniques and to derive interesting structural properties for reducing the search space for frequent patterns. Indeed, the experiments we have carried out show that our algorithms outperform standard data mining algorithms adapted to discover frequent patterns of workflow executions.","tags":["Pattern discovery","Graph Mining","Workflow Mining"],"title":"Mining and reasoning on workflows","type":"publication"},{"authors":["S. Flesca","G. Manco","E. Masciari","L. Pontieri","A. Pugliese"],"categories":null,"content":"","date":1107216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1107216000,"objectID":"90e8a8f82be260b63a60ab570b13e4cd","permalink":"https://gmanco.github.io/publication/flesca-2005/","publishdate":"2019-09-07T09:18:44.881471Z","relpermalink":"/publication/flesca-2005/","section":"publication","summary":"Because of the widespread diffusion of semistructured data in XML format, much research effort is currently devoted to support the storage and retrieval of large collections of such documents. XML documents can be compared as to their structural similarity, in order to group them into clusters so that different storage, retrieval, and processing techniques can be effectively exploited. In this scenario, an efficient and effective similarity function is the key of a successful data management process. We present an approach for detecting structural similarity between XML documents which significantly differs from standard methods based on graph-matching algorithms, and allows a significant reduction of the required computation costs. Our proposal roughly consists of linearizing the structure of each XML document, by representing it as a numerical sequence and, then, comparing such sequences through the analysis of their frequencies. First, some basic strategies for encoding a document are proposed, which can focus on diverse structural facets. Moreover, the theory of discrete Fourier transform is exploited to effectively and efficiently compare the encoded documents (i.e., signals) in the domain of frequencies. Experimental results reveal the effectiveness of the approach, also in comparison with standard methods.","tags":["Web Mining","Data Mining","XML/XSL/RDF","Text Mining","Similarity Measures"],"title":"Fast detection of XML structural similarity","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f1d044c0738ab9f19347f15c290a71a1","permalink":"https://gmanco.github.io/research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/","section":"","summary":"","tags":null,"title":"","type":"page"}]
<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Panoramica | Giuseppe Manco</title>
    <link>https://gmanco.github.io/courses/computervision/</link>
      <atom:link href="https://gmanco.github.io/courses/computervision/index.xml" rel="self" type="application/rss+xml" />
    <description>Panoramica</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2021 Giuseppe Manco</copyright><lastBuildDate>Mon, 01 Mar 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://gmanco.github.io/img/manco.jpg</url>
      <title>Panoramica</title>
      <link>https://gmanco.github.io/courses/computervision/</link>
    </image>
    
    <item>
      <title>Analisi di Immagini e Video - A.A. 2019-2020</title>
      <link>https://gmanco.github.io/courses/computervision/2020/</link>
      <pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/2020/</guid>
      <description>

&lt;h2 id=&#34;annunci&#34;&gt;Annunci&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[31/07/2020]&lt;/strong&gt; Rilasciate le &lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/progetti/project3_spec/&#34; target=&#34;_blank&#34;&gt;specifiche&lt;/a&gt; progetto per esame del 03/09/2020. La consegna degli elaborati deve avvenire il 01/09/2020.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[01/07/2020]&lt;/strong&gt; Rilasciate le &lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/progetti/project2_spec/&#34; target=&#34;_blank&#34;&gt;specifiche&lt;/a&gt; progetto per esame del 31/07/2020. La consegna degli elaborati deve avvenire il 29/07/2020.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[09/06/2020]&lt;/strong&gt; Rilasciate le &lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/progetti/project1_spec/&#34; target=&#34;_blank&#34;&gt;specifiche&lt;/a&gt; progetto per esame del 01/07/2020. La consegna degli elaborati deve avvenire il 28/06/2020.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[08/06/2020]&lt;/strong&gt; E disponibile la valutazione del secondo esonero, nalla rispettiva &lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/esoneri/convolution/&#34; target=&#34;_blank&#34;&gt;pagina&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[07/05/2020]&lt;/strong&gt; E disponibile la valutazione del primo esonero, nalla rispettiva &lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/esoneri/listoffeaturedescriptors/&#34; target=&#34;_blank&#34;&gt;pagina&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[28/04/2020]&lt;/strong&gt; E disponibile la lista degli argomenti per il secondo esonero. La lista è disponibile &lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/esoneri/convolution/&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;. Gli studenti che selgono di effettuare l&amp;rsquo;esonero devono concordare l&amp;rsquo;opzione con il docente tramite email. La deadline per la consegna degli elaborati è il 10 maggio 2020.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[02/04/2020]&lt;/strong&gt; E disponibile la lista degli argomenti per il primo esonero. La lista è disponibile &lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/esoneri/listoffeaturedescriptors/&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;. Gli studenti che selgono di effettuare l&amp;rsquo;esonero devono comunicare le opzioni (3) al docente. Martedì 7 verrà comunicata l&amp;rsquo;assegnazione dei topic ai vari studenti. La deadline per la consegna degli elaborati è il 16 aprile 2020.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[03/03/2020]&lt;/strong&gt; Si avvisano gli studenti che il corso di Analisi di Immagini e Video – Corso di Laurea Magistrale in Ingegneria Informatica verrà erogato in modalità streaming, utilizzando l’applicativo TEAMS, a partire dal giorno 17/03/2019, alle ore 8:30. La richiesta di iscrizione al corso può essere effettuata cliccando sul seguente &lt;a href=&#34;https://teams.microsoft.com/l/channel/19%3ac60bff4a6e874db3b5e0d2034412ed18%40thread.tacv 2/Generale?groupId=b4509060-7b45-4eab-80ce-55182356fb88&amp;amp;tenantId=7519d0cd-2106-47d9- adcb-320023abff57&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt;. Gli studenti già pratici dell’utilizzo di TEAMS e che abbiano già scaricato l’APP, possono iscriversi direttamente al corso (senza approvazione del docente) utilizzando il codice: &lt;strong&gt;khm7h4s&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;breve-descrizione-del-corso&#34;&gt;Breve descrizione del corso&lt;/h2&gt;

&lt;p&gt;Il corso è finalizzato ad acquisire e sperimentare le tecniche di base per l’analisi di immagini e video. Verranno illustrati i concetti fondamentali per l’analisi delle immagini e verranno illustrate le principali tecniche di object detection, object tracking e action detection. Durante il corso saranno presentati modelli di reti neurali CNN e RNN, tecniche di transfer learning, Residual and Attention network ed una introduzione ai modelli generativi e alle Adversarial networks. Gli esempi applicativi faranno uso del linguaggio Python e dei framework di Deep Learning Pytorch/Tensorflow.&lt;/p&gt;

&lt;p&gt;Competenze specifiche:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;comprensione dei concetti legati all’image e al video processing&lt;/li&gt;
&lt;li&gt;conoscenza degli aspetti caratterizzanti di machine learning e deep learning&lt;/li&gt;
&lt;li&gt;comprensione delle principali tecniche per l’analisi di immagini e video&lt;/li&gt;
&lt;li&gt;abilità di utilizzare algoritmi di image analysis per la risoluzione di problemi specifici&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;docenti&#34;&gt;Docenti&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Giuseppe Manco. Ricevimento: mercoledì’ 14:30-16:30.&lt;/li&gt;
&lt;li&gt;Francesco Pisani. Rivevimento lunedì 15-17.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;programma&#34;&gt;Programma&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Introduzione alla computer vision&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Concetti fondamentali su Image processing e analysis: Image Basics, Python per Image Processing, Manipolazione di immagini.&lt;/li&gt;
&lt;li&gt;Trasformazioni: Normalizzazione, filtri, Edge detection, morfologia, thresholding e segmentazione.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Classificazione di immagini e video&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Introduzione alla classificazione: applicazioni; approcci classici; scikit-Learn per la classificazione; limitazioni.&lt;/li&gt;
&lt;li&gt;Deep Learning: Review su Neural Networks per l&amp;rsquo;analisi di immagini e video. Convolutional Neural Networks. Reti ricorrenti. R-CNN. Gestione dell&amp;rsquo;overfitting.&lt;/li&gt;
&lt;li&gt;Concetti avanzati. Principali architetture di rete e loro caratteristiche: VGG, AlexNet, Inception and Residual Networks, Attention Networks. Transfer Learning.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concetti avanzati&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Object Detection: Sliding windows, boundary boxes e anchors. Region Proposal Networks. Yolo e Darknet. Applicazioni.&lt;/li&gt;
&lt;li&gt;Object tracking e action recognition. Optical flow; Single/multiple objects tracking; Action classification and localization.&lt;/li&gt;
&lt;li&gt;Image segmentation e synthesis. UNet. Neural style transfer.&lt;/li&gt;
&lt;li&gt;Modelli Generativi: Probabilistic Modeling, Autoencoders. Generative Adversarial Networks. Applicazioni: Colorization, Reconstruction, Super-Resolution, Synthesis, Text-to-image.&lt;/li&gt;
&lt;li&gt;Adversarial Machine Learning. Principali attacchi e contromisure. Adversarial-free deep networks.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;materiale-didattico&#34;&gt;Materiale didattico&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Lucidi delle lezioni&lt;/li&gt;
&lt;li&gt;Notebooks e lucidi delle esercitazioni&lt;/li&gt;
&lt;li&gt;Libri di consultazione:

&lt;ul&gt;
&lt;li&gt;E.R. Davies, Computer Vision: Principles, Algorithms, Applications, Learning. Fifth edition. Elsevier/Academic Press, 2018 &lt;strong&gt;[Davies18]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Jan Erik Solem, Programming Computer Vision with Python. O&amp;rsquo;Reilly Media, 2012. &lt;strong&gt;[Solem12]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Mohamed Elgendy, &lt;a href=&#34;https://www.manning.com/books/deep-learning-for-vision-systems&#34; target=&#34;_blank&#34;&gt;Deep Learning for Vision Systems&lt;/a&gt;. Manning, 2020. &lt;strong&gt;[Elg20]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Rafael C. Gonzalez, Richard E. Woods, Digital image processing. 4th edition. Pearson, 2018. &lt;strong&gt;[Gon18]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Richard Szeliski, Computer Vision: Algorithms and Applications. Springer, 2011. &lt;strong&gt;[Sze11]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Jason M.Kinser, Image operators: image processing in Python. CRC Press, 2019. &lt;strong&gt;[Kin19]&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;sillabo-delle-lezioni&#34;&gt;Sillabo delle lezioni&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Lezione&lt;/th&gt;
&lt;th&gt;Argomenti&lt;/th&gt;
&lt;th&gt;Materiale didattico&lt;/th&gt;
&lt;th&gt;Data&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Introduzione al corso&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/lecture1/&#34; target=&#34;_blank&#34;&gt;Slides&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;17/03/2020&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;Concetti fondamentali su Image processing e analysis&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/lecture2/&#34; target=&#34;_blank&#34;&gt;Slides, notebook&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;19/03/2020&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Filtri&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/lecture3/&#34; target=&#34;_blank&#34;&gt;Slides, notebook&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;24/03/2020&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;Laboratorio 1: Image Processing in Python&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/lecture_lab1/&#34; target=&#34;_blank&#34;&gt;Slides, notebook&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;26/03/2020&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;Edge detection. Fourier Transform&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/lecture4/&#34; target=&#34;_blank&#34;&gt;Slides, notebook&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;31/03/2020&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;Classificazione. Feature detection &amp;amp; Description&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/lecture5/&#34; target=&#34;_blank&#34;&gt;Slides&lt;/a&gt;.    &lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/esoneri/listoffeaturedescriptors/&#34; target=&#34;_blank&#34;&gt;Esonero&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;02/04/2020&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;Laboratorio 2: sklearn. Pytorch&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/lecture_lab2/&#34; target=&#34;_blank&#34;&gt;Slides, notebook&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;07/04/2020&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;Modelli non lineari. CNN&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/lecture6/&#34; target=&#34;_blank&#34;&gt;Slides, notebook&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;16/04/2020, 21/04/2020&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;Laboratorio 3: Architetture CNN, data augmentation, transfer learning&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/lecture_lab3/&#34; target=&#34;_blank&#34;&gt;Slides, notebook&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;23/04/2020&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;Object Detection. Region Proposal Networks, Single-Shot Detection. Yolo.&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/lecture7/&#34; target=&#34;_blank&#34;&gt;Slides, notebooks&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;28/04/2020, 30/04/2020&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;Laboratorio 4: Object Detection&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/lecture_lab4/&#34; target=&#34;_blank&#34;&gt;Notebook&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;05/05/2020&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;Segmentazione&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/lecture8/&#34; target=&#34;_blank&#34;&gt;Slides, notebooks&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;07/05/2020, 12/05/2020, 14/05/2020&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;Action Recognition&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/lecture9/&#34; target=&#34;_blank&#34;&gt;Slides, notebooks&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;19/05/2020&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;Modelli generativi. Variational Autoencoders&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/lecture10/&#34; target=&#34;_blank&#34;&gt;Slides, notebooks&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;21/05/2020&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;Generative Adversarial Networks. Image translation&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/lecture11/&#34; target=&#34;_blank&#34;&gt;Slides, notebooks&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;26/05/2020, 28/05/2020&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;Laboratorio 5: Generative Adversarial Networks&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://gmanco.github.io/courses/computervision/2020/lecture_lab5/&#34; target=&#34;_blank&#34;&gt;Slides, notebook&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;04/06/2020&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Introduzione al corso</title>
      <link>https://gmanco.github.io/courses/computervision/lecture1/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture1/</guid>
      <description>

&lt;p&gt;Introduzione al corso. Image Processing, Analysis e Computer Vision.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/1.Intro.pdf&#34;&gt;qui&lt;/a&gt; (Con &lt;a href=&#34;https://web.microsoftstream.com/video/23e0ae1c-3fd8-44b0-adb5-1db861a8d0c9&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt; associato su Microsoft Stream)&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, sect. 1.1-1.3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, Ch. 1,2.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://algorithmia.com/blog/introduction-to-computer-vision&#34; target=&#34;_blank&#34;&gt;Introduction to Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/what-is-computer-vision/&#34; target=&#34;_blank&#34;&gt;A gentle introduction to Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/readers-writers-digest/beginners-guide-to-computer-vision-23606224b720&#34; target=&#34;_blank&#34;&gt;A Beginner&amp;rsquo;s guide to Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/everything-you-ever-wanted-to-know-about-computer-vision-heres-a-look-why-it-s-so-awesome-e8a58dfb641e&#34; target=&#34;_blank&#34;&gt;Everything you wanted to know about Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.thecvf.com/&#34; target=&#34;_blank&#34;&gt;The Computer Vision Foundation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.forbes.com/sites/forbestechcouncil/2020/04/03/three-ways-computer-vision-is-transforming-marketing/#1d6346d0214b&#34; target=&#34;_blank&#34;&gt;Three ways Computer Vision is transforming Marketing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0168169902001011)&#34; target=&#34;_blank&#34;&gt;https://www.sciencedirect.com/science/article/pii/S0168169902001011)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;approfondimento-ambiti-applicativi-con-il-contributo-degli-studenti&#34;&gt;Approfondimento: Ambiti applicativi (con il contributo degli studenti)&lt;/h2&gt;

&lt;h3 id=&#34;rilevamento-attenzione-del-conducente-fn1&#34;&gt;Rilevamento attenzione del conducente&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn1&#34;&gt;&lt;a href=&#34;#fn:fn1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;Gli algoritmi di apprendimento automatico e deep learning, a cui sono stati forniti migliaia di dati di volti attenti e disattenti, possono rilevare differenze tra occhi concentrati e stanchi, nonché indizi che rivelano una guida distratta. L&amp;rsquo;intelligenza artificiale in questo contesto è importante perché permette di proteggere sia il conducente che le altre entità limitrofe. Questo aspetto della computer vision inoltre può essere usato per rilevare eventuali conducenti che guidano in stato di ebbrezza.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0925231213006863&#34; target=&#34;_blank&#34;&gt;Riferimenti&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;conteggio-persone-e-distanza-covid-fn1&#34;&gt;Conteggio persone e distanza COVID&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn1&#34;&gt;&lt;a href=&#34;#fn:fn1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;Purtroppo il 2020 ha visto il diffondersi di un virus che ha investito la popolazione mondiale. Ogni persona è stata costretta a dover rispettare delle regole per cercare di contrastare il diffondersi della pandemia ed anche in questo spiacevole contesto, la computer vision ha assunto un ruolo peculiare. La tecnologia di interesse è stata sfruttata per esempio per rilevare eventuali assembramenti, per il conteggio delle persone, per l’individuazione di soggetti privi di mascherina. Uno dei progetti di ricerca anti-Covid a cui lavora Unimore ha l’obiettivo di realizzare un sistema che analizza i video, localizza le persone nello spazio 3D, riconosce le mascherine e calcola distanze. Usa l’intelligenza artificiale per il distanziamento sociale in spazi aperti e può essere di grande aiuto alle istituzioni per il calcolo in tempo reale degli assembramenti, così da ridurre il rischio di contagio nei luoghi pubblici.&lt;/p&gt;

&lt;p&gt;Riferimenti &lt;a href=&#34;https://www.farodiroma.it/arriva-il-contapersone-evolution-la-telecamera-che-conta- le-persone-e-aziona-un-semaforo/&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt; e &lt;a href=&#34;https://www.dire.it/30-06-2020/479790-arriva-un-software-predice-rischio-covid-in- luoghi-affollati/&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;drishti-intelligenza-artificiale-al-servizio-degli-ipovedenti-fn1&#34;&gt;Drishti, intelligenza artificiale al servizio degli ipovedenti&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn1&#34;&gt;&lt;a href=&#34;#fn:fn1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;Drishti, dal sanscrito “sguardo” è una tecnologia che attraverso un paio di occhiali, similissimi a comuni occhiali, e grazie alla tecnologia racchiusa in essi permette alle persone ipovedenti di guardare ciò che li circonda. Questa tecnologia permette di avere un senso in più: un senso tecnologico, fatto di bit e intelligenza artificiale, che attraverso una micro-camera nascosta nella montatura degli smart glasses e ad un software sviluppato dagli “Accenture Labs” consente anche a chi è ipovedente o cieco di gettare uno sguardo sul mondo che lo circonda.&lt;/p&gt;

&lt;p&gt;Riferimenti &lt;a href=&#34;http://byinnovation.eu/visione-artificiale-ipovedenti/&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt; e &lt;a href=&#34;https://startupitalia.eu/86342-20180227-drishti-intelligenza-artificiale-al-servizio-degli-ipovedenti&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;manifacturing-fn1&#34;&gt;Manifacturing&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn1&#34;&gt;&lt;a href=&#34;#fn:fn1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;Nell’ambito della produzione, la visione artificiale abbinata ai sensori può essere sfruttata per gestire e monitorare lo stato delle apparecchiature e degli strumenti. Oggi, la tecnologia viene utilizzata per controllare importanti impianti o attrezzature al loro interno. I guasti e i problemi dell&amp;rsquo;infrastruttura possono essere prevenuti con l&amp;rsquo;aiuto di una visione artificiale realizzata per stimare lo stato e l&amp;rsquo;efficienza delle macchine. Molte aziende stanno usando questa tipologia di manutenzione per mantenere i propri strumenti in buone condizioni. Ad esempio, il software ZDT realizzato da FANUC è un software di manutenzione preventiva progettato per raccogliere immagini dalla telecamera collegata ai robot. Quindi questi dati vengono elaborati per fornire la diagnosi dei problemi e rilevare eventuali problemi potenziali.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.fanuc.co.jp/en/product/robot/function/zdt.html&#34; target=&#34;_blank&#34;&gt;Riferimenti&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;video-sorveglianza-e-customer-profiling-fn1&#34;&gt;Video sorveglianza e Customer profiling&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn1&#34;&gt;&lt;a href=&#34;#fn:fn1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;La Computer Vision viene applicata alla videosorveglianza automatica per una serie di obiettivi che includono la gestione delle folle e la misurazione dell’efficacia del marketing.
 Grazie al Deep Learning possono essere sviluppate anche soluzioni per il conteggio delle persone e Stazioni ferroviarie, aeroporti, parcheggi, centri commerciali, stadi: in tutti questi luoghi, i sistemi di videosorveglianza possono diventare più veloci e affidabili se sfruttano le potenzialità della smart safety integrata con l’intelligenza artificiale.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;HTTPS://WWW.PIKKART.COM/CONTENUTO/CONTENUTI--ECM/1VIDEO- SORVEGLIANZA-AUTOMATICA-E-COSTUMER-PROFILING.ASHX&#34; target=&#34;_blank&#34;&gt;Riferimenti&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;vision-2-startup-binoocle-fn2&#34;&gt;VISION-2 (startup BInoocle)&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn2&#34;&gt;&lt;a href=&#34;#fn:fn2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;“VISION-2 è un sistema avanzato di &lt;strong&gt;Intelligenza Artificiale&lt;/strong&gt; immediatamente installabile, con nessuna necessità di assistenza, che opera attraverso qualsiasi webcam o telecamera in tempo reale (diretta) per svolgere diverse funzioni ed aiutare a mantenere comportamenti sicuri”. Il sistema è stato messo a punto per incentivare comportamenti sicuri durante l’emergenza COVID, in particolare, durante la “fase 2” dell’epidemia.&lt;/p&gt;

&lt;p&gt;Esistono tre applicazioni diverse di Vision-2 pensate per il monitoraggio di situazioni differenti:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pro Person: mantenere un numero sicuro di persone per evitare gli assembramenti. &lt;strong&gt;Pro Person&lt;/strong&gt; è un sistema di monitoraggio avanzato che rileva il numero e lo &lt;strong&gt;spostamento delle persone presenti in un luogo&lt;/strong&gt;&lt;strong&gt;,&lt;/strong&gt; inviando un segnale quando queste sono in numero superiore a quello deciso o consigliato.
Aiuta quindi a tenere sempre un numero ragionevole di persone in un ambiente ed evita gli &lt;strong&gt;assembramenti.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Face Mask: rilevare la presenza delle mascherine protettive. Face Mask è un sistema di monitoraggio avanzato che permette di rilevare la presenza delle mascherine protettive attraverso una webcam, &lt;strong&gt;fornendo in tempo reale segnali d’avvertimento nel caso di mancanza di protezione sui volti.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Concierge: il tuo portiere di fiducia che ti aiuta a gestire gli accessi e la ila in modo sicuro. Concierge permette di gestire la fila ed il flusso in entrata in modo ordinato e preciso, &lt;strong&gt;rilevando il momento opportuno in cui far entrare nuovi clienti&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt; È facilmente configurabile sul numero di persone massimo desiderate all’interno.
Il cliente saprà quindi quando deve aspettare o quando potrà accedere e verrà accolto da un segnale discreto e ben visibile.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;http://www.binoocle.com/vision-2/&#34; target=&#34;_blank&#34;&gt;Riferimenti&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;sportlogiq-fn2&#34;&gt;SportLogiq&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn2&#34;&gt;&lt;a href=&#34;#fn:fn2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;Sportlogiq è nna delle prime aziende che ha iniziato a scavare più a fondo nell&amp;rsquo;analisi abilitata dall&amp;rsquo;intelligenza artificiale. Sportlogiq aiuta le squadre di hockey, calcio e football a prendere decisioni più intelligenti utilizzando informazioni più approfondite.&lt;/p&gt;

&lt;p&gt;L&amp;rsquo;azienda utilizza telecamere abilitate alla visione artificiale che analizzano specifici eventi di gioco e sfumature come i movimenti dei giocatori, le traiettorie della palla, i tiri e i passaggi. Quindi questi dati vengono convertiti in rapporti significativi per allenatori, commentatori, scout, analisti e persino società di scommesse sportive delle squadre.&lt;/p&gt;

&lt;p&gt;SportLogic è nata con l’ obiettivo quello di portare un’intelligenza artificiale all’avanguardia nel mondo dello sport. Il team di analisti di dati di Sportlogiq elabora i dati grezzi ottenuti dai feed trasmessi e li modella in informazioni utilizzabili su misura per l&amp;rsquo;esperienza dello sport.&lt;/p&gt;

&lt;p&gt;·    Ingressi: gli esperti di Computer Vision forniscono a Sportlogiq la tecnologia per raccogliere i dati di tracciamento che configurano il rilevamento degli eventi. Il rilevamento degli eventi, parte del processo di apprendimento automatico, consente ai team di assemblare il più grande database nello sport&lt;/p&gt;

&lt;p&gt;·    Stima della posa: viene usata la stima della posa per comprendere la posizione e il movimento dei giocatori sul ghiaccio, sul campo o sul campo. Questa tecnologia consente un esame più forense delle prestazioni di un giocatore e una comprensione più profonda di ciò che sta accadendo nel gioco.&lt;/p&gt;

&lt;p&gt;·    Uscite: Attraverso l&amp;rsquo;apprendimento automatico, la visione artificiale e il lavoro dei nostri analisti di dati, Sportlogiq può produrre modelli che forniscono ai team metriche rivoluzionarie come: Obiettivi previsti (hockey), tempo automatizzato sul ghiaccio (Hockey), metriche fisiche (calcio), metriche contestuali (calcio), dati di monitoraggio (calcio)&lt;/p&gt;

&lt;p&gt;L&amp;rsquo;accesso a queste metriche basate sull&amp;rsquo;intelligenza artificiale consente ai team di scoprire tendenze all&amp;rsquo;interno della propria squadra e dei loro avversari che danno loro un vantaggio competitivo&lt;/p&gt;

&lt;p&gt;Riferimenti &lt;a href=&#34;https://sportlogiq.com/en/technology&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt; e &lt;a href=&#34;https://becominghuman.ai/5-game-changing-computer-vision-applications-in-sports-5f02ec35529b&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;google-lens-fn2&#34;&gt;Google Lens&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn2&#34;&gt;&lt;a href=&#34;#fn:fn2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;Google Lens è una applicazione mobile per il riconoscimento delle immagini sviluppata da Google. È stata progettata per portare informazioni pertinenti ad oggetti utilizzando l’analisi visiva. Inizialmente veniva fornita come app a parte, in seguito è stata integrata alla fotocamera Android.&lt;/p&gt;

&lt;p&gt;Google Lens riconosce gli oggetti e suggerisce le azioni appropriate. Le diverse funzionalità sono:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Scansione e traduzione del testo: per tradurre testo in tempo reale, cercare parole, aggiungere aventi al calendario.&lt;/li&gt;
&lt;li&gt;Acquisti: per cercare i negozi che vendono gli oggetti inquadrati&lt;/li&gt;
&lt;li&gt;Menu: per cercare del cibo da ordinare al ristorante direttamente dal menù&lt;/li&gt;
&lt;li&gt;Visita: esplorare luoghi visini, orari di apertura, fatti storici.&lt;/li&gt;
&lt;li&gt;Identifica: per scoprire il tipo di pianta o la razza del cane inquadrato&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://lens.google.com/&#34; target=&#34;_blank&#34;&gt;Riferimenti&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;covid-19-fn3&#34;&gt;COVID-19&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn3&#34;&gt;&lt;a href=&#34;#fn:fn3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;L’ultima minaccia per la salute globale è l’epidemia in corso di COVID-19. La Computer Vision ha avuto recentemente successo nella risoluzione di vari problemi riguardanti l’assistenza sanitaria e ha il potenziale per contribuire alla sfida contro il COVID-19. In particolare, le tecniche di visione artificiale possono essere adoperate in tre aree di ricerca differenti riassunte nello schema sotto riportato.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/image-20210309093707135.png&#34; alt=&#34;image-20210309093707135&#34; style=&#34;zoom:100%;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.researchgate.net/publication/340667109_Computer_Vision_for_COVID-19_Control_A_Survey&#34; target=&#34;_blank&#34;&gt;Riferimenti&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;controllo-della-qualità-nelle-industrie-fn3&#34;&gt;Controllo della qualità nelle industrie&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn3&#34;&gt;&lt;a href=&#34;#fn:fn3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;I sistemi di Computer Vision possono essere impiegati con successo nel campo dell’automazione industriale per il controllo qualità. Infatti, le tecnologie che adoperano la Computer Vision consentono di rilevare difetti, migliorare la qualità dei prodotti e ridurre i costi di produzione.&lt;/p&gt;

&lt;p&gt;Riferimenti &lt;a href=&#34;https://www.uqido.com/insights/controllo-qualita-computer-vision/&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://www.develer.com/soluzioni/visione-artificiale/&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;, &lt;a href=&#34;https://www.gntechonomy.com/soluzioni/gn-quality-assurance/&#34; target=&#34;_blank&#34;&gt;3&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;assistenza-per-persone-ipovedenti-fn3&#34;&gt;Assistenza per persone ipovedenti&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn3&#34;&gt;&lt;a href=&#34;#fn:fn3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;Per le persone ipovedenti o affette da cecità anche attività giornaliere molto semplici possono essere difficili da affrontare. Mediante l’utilizzo della visione artificiale tali problematiche possono essere affrontate grazie al Visual Question Answering. Una persona ipovedente può in questo modo scattare una foto, fare delle domande a questa inerenti e ricevere in modo rapido una risposta in linguaggio parlato che la possa guidare nelle azioni da svolgere.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://byinnovation.eu/visione-artificiale-ipovedenti/&#34; target=&#34;_blank&#34;&gt;Riferimenti&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;agricoltura-fn3&#34;&gt;Agricoltura&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn3&#34;&gt;&lt;a href=&#34;#fn:fn3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;Anche nel settore agricolo vengono ormai utilizzate tecnologie volte a migliorare le attività svolte dagli essere umani. Ne vengono adoperate di diverse al fine di aiutare gli agricoltori ad adottare metodi di crescita più efficienti , aumentare i raccolti e i profitti aiutando gli stessi agricoltori a prendere decisioni in merito ai possibili trattamenti. La maggior parte di queste, infatti, catturano le immagini dei campi per evidenziare eventualmente la presenza di infestazioni da parassiti o altri fattori che potrebbero intaccare il benessere delle colture e del bestiame.&lt;/p&gt;

&lt;p&gt;Riferimenti &lt;a href=&#34;https://emerj.com/ai-sector-overviews/computer-vision-applications-shopping-driving-and-more/&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://slantrange.com/&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;, &lt;a href=&#34;https://www.cainthus.com/&#34; target=&#34;_blank&#34;&gt;3&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;ambito-sanitario-fn3&#34;&gt;Ambito sanitario&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn3&#34;&gt;&lt;a href=&#34;#fn:fn3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;In questo ambito la Computer Vision aiuta gli operatori sanitari a classificare con precisione condizioni o malattie riducendo o eliminando diagnosi imprecise e trattamenti non corretti. Un ruolo che assume sempre più significato è quello relativo alla diagnosi precoce del cancro. Recenti studi hanno infatti portato alla generazione di tecnologie in grado di classificare, ad esempio, il cancro della pelle con un tasso di accuratezza pari a quello dei medici o ancora a diagnosi più precise ed efficienti in merito alla segmentazione dei tumori cerebrali o alla riduzione della complessità dell’esame istologico, necessario per studiare le manifestazioni della malattia.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://emerj.com/ai-sector-overviews/deep-learning-in-oncology/&#34; target=&#34;_blank&#34;&gt;Riferimenti&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;trasferimento-del-contenuto-di-un-immagine-in-un-altra-o-unione-di-immagini-fn4&#34;&gt;Trasferimento del contenuto di un’immagine in un’altra (O unione di immagini)&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn4&#34;&gt;&lt;a href=&#34;#fn:fn4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.groundai.com/project/automated-deep-photo-style-transfer/1&#34; target=&#34;_blank&#34;&gt;Riferimenti&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;riconoscimento-automatico-delle-espressioni-facciali-fn4&#34;&gt;Riconoscimento automatico delle espressioni facciali&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn4&#34;&gt;&lt;a href=&#34;#fn:fn4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S1319157818303379&#34; target=&#34;_blank&#34;&gt;Riferimenti&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;real-time-lane-detection-fn4&#34;&gt;Real-time Lane Detection&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn4&#34;&gt;&lt;a href=&#34;#fn:fn4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2020/05/tutorial-real-time-lane-detection-opencv/?utm_source=blog&amp;amp;utm_medium=18_open-Source_computer_vision_projects&#34; target=&#34;_blank&#34;&gt;Riferimenti&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;riconoscimento-automatico-di-colori-fn4&#34;&gt;Riconoscimento automatico di colori&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn4&#34;&gt;&lt;a href=&#34;#fn:fn4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://pysource.com/2019/02/15/detecting-colors-hsv-color-space-opencv-with-python/&#34; target=&#34;_blank&#34;&gt;Riferimenti&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;contatore-di-persone-fn4&#34;&gt;Contatore di persone&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn4&#34;&gt;&lt;a href=&#34;#fn:fn4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.pyimagesearch.com/2018/08/13/opencv-people-counter/&#34; target=&#34;_blank&#34;&gt;Riferimenti&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:fn1&#34;&gt;Giulia Natale
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:fn1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:fn2&#34;&gt;Silvia Plutino
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:fn2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:fn3&#34;&gt;Gaia Musacchio
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:fn3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:fn4&#34;&gt;Tommaso Ruga
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:fn4&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Python and image processing</title>
      <link>https://gmanco.github.io/courses/computervision/lecture_lab1/</link>
      <pubDate>Fri, 05 Mar 2021 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture_lab1/</guid>
      <description>

&lt;p&gt;Python &amp;amp; image processing.&lt;/p&gt;

&lt;p&gt;Principali argomenti trattati:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Color models&lt;/li&gt;
&lt;li&gt;Introduzione ai Jupyter notebooks&lt;/li&gt;
&lt;li&gt;Caricamento di immagini con PIL&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/a400017b616115ba0be8667028c74589c8583e1f/labs_lecture/lab01/color_models.pdf&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt; e &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/a400017b616115ba0be8667028c74589c8583e1f/labs_lecture/lab01/esercitazione01.pdf&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;La lezione è corredata di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/tree/master/labs_lecture/lab01&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 5.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 4.2.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch. 13.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://pillow.readthedocs.io/en/stable/handbook/tutorial.html&#34; target=&#34;_blank&#34;&gt;PIL tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/scikit-image/skimage-tutorials&#34; target=&#34;_blank&#34;&gt;Scikit Image Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cs.brown.edu/courses/cs092/VA10/HTML/ColorModels.html&#34; target=&#34;_blank&#34;&gt;Color models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jupyterlab.readthedocs.io/en/stable/&#34; target=&#34;_blank&#34;&gt;jupyterlab.readthedocs.io&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Concetti fondamentali su Image processing e analysis</title>
      <link>https://gmanco.github.io/courses/computervision/lecture2/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture2/</guid>
      <description>

&lt;p&gt;Concetti fondamentali su Image processing e analysis: Image Basics, Manipolazione di immagini. Introduzione alle librerie Python per Image Processing.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/2.Image_fundamentals.pdf&#34;&gt;qui&lt;/a&gt; (con &lt;a href=&#34;https://web.microsoftstream.com/video/d6f46094-e54b-49e8-b86b-996caf39058a?list=studio&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt; della lezione e &lt;a href=&#34;../pdf/l2_appunti.pdf&#34;&gt;appunti&lt;/a&gt; allegati).&lt;/p&gt;

&lt;p&gt;La lezione è corredata di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/1.Image_fundamentals.ipynb&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 2.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, sect. 1.4-1.5.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 2-3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 3.1, 3.6.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch.1-6.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://cs231n.github.io/python-numpy-tutorial/&#34; target=&#34;_blank&#34;&gt;Stanford cs231n Python tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cs231n.github.io/ipython-tutorial/&#34; target=&#34;_blank&#34;&gt;Stanford cs231n Jupyter tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.anaconda.com/&#34; target=&#34;_blank&#34;&gt;Anaconda, la piattaforma di riferimento per installare Python e Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://notebooks.azure.com/&#34; target=&#34;_blank&#34;&gt;Piattaforma azure di Microsoft per l&amp;rsquo;esecuzione di notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/notebooks/intro.ipynb&#34; target=&#34;_blank&#34;&gt;Google Colab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://missing.csail.mit.edu/?fbclid=IwAR0N3s-DbRLrSWW3tB1L5iu_thdiEtFuL7cGUwxyOL-yc7skytSDGdT9ZAo&#34; target=&#34;_blank&#34;&gt;The missing semester, un mini-corso del MIT sui tool di base che tutti i data scientist dovrebbero conoscere&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Filtri</title>
      <link>https://gmanco.github.io/courses/computervision/lecture3/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture3/</guid>
      <description>

&lt;p&gt;Manipolazione spaziale. Filtri. Convoluzione.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/3.Filters.pdf&#34;&gt;qui&lt;/a&gt; con &lt;a href=&#34;https://web.microsoftstream.com/video/93ece5f0-126e-4cca-922d-551ba5e74998&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt; di riferimento e &lt;a href=&#34;../pdf/l3_appunti.pdf&#34;&gt;appunti&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;La lezione è corredata di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/2.Filters.ipynb&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, Ch. 3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 3.2, 3.3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch. 2, 10-12.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/7075360&#34; target=&#34;_blank&#34;&gt;Adaptive filters for color image processing: A survey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/image-filters-in-python-26ee938e57d2&#34; target=&#34;_blank&#34;&gt;Image filters in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scipy-lectures.org/advanced/image_processing/&#34; target=&#34;_blank&#34;&gt;Image manipulation using Numpy and SciPy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@enzoftware/how-to-build-amazing-images-filters-with-python-median-filter-sobel-filter-%EF%B8%8F-%EF%B8%8F-22aeb8e2f540&#34; target=&#34;_blank&#34;&gt;How to build an image filter in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=otLGDpBglEA&amp;amp;feature=player_embedded&#34; target=&#34;_blank&#34;&gt;Instagram filters in 15 lines of code&lt;/a&gt;. Il codice è disponibile &lt;a href=&#34;https://github.com/lukexyz/CV-Instagram-Filters/blob/master/gotham.py&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Python and image processing</title>
      <link>https://gmanco.github.io/courses/computervision/lecture_lab2/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture_lab2/</guid>
      <description>

&lt;p&gt;Python &amp;amp; image processing.&lt;/p&gt;

&lt;p&gt;Principali argomenti trattati:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Operazioni di base

&lt;ul&gt;
&lt;li&gt;Traslazioni&lt;/li&gt;
&lt;li&gt;Rotazioni&lt;/li&gt;
&lt;li&gt;Resizing&lt;/li&gt;
&lt;li&gt;Scaling&lt;/li&gt;
&lt;li&gt;Cropping&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Trasformazioni

&lt;ul&gt;
&lt;li&gt;Slicing&lt;/li&gt;
&lt;li&gt;Masking&lt;/li&gt;
&lt;li&gt;Blurring&lt;/li&gt;
&lt;li&gt;Sharpening&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;OpenCV

&lt;ul&gt;
&lt;li&gt;Operazioni di base&lt;/li&gt;
&lt;li&gt;Linee e punti&lt;/li&gt;
&lt;li&gt;ROI&lt;/li&gt;
&lt;li&gt;Trasformazioni geometriche&lt;/li&gt;
&lt;li&gt;Prospettiva&lt;/li&gt;
&lt;li&gt;Operazioni aritmetiche e booleane&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;SKimage

&lt;ul&gt;
&lt;li&gt;Operazioni di base&lt;/li&gt;
&lt;li&gt;Trasformazioni geometriche&lt;/li&gt;
&lt;li&gt;Edge detection&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La lezione è corredata di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/labs_lecture/lab02&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 5.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 4.2.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch. 13.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/master/index.html&#34; target=&#34;_blank&#34;&gt;OpenCV&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-image.org/docs/stable/api/api.html&#34; target=&#34;_blank&#34;&gt;Scikit Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pillow.readthedocs.io/en/stable/handbook/tutorial.html&#34; target=&#34;_blank&#34;&gt;PIL tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/scikit-image/skimage-tutorials&#34; target=&#34;_blank&#34;&gt;Scikit Image Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Derivate. Edge Detection</title>
      <link>https://gmanco.github.io/courses/computervision/lecture4/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture4/</guid>
      <description>

&lt;p&gt;Derivate di immagini. Gradient-based filtering. trasformata di Fourier e Filtraggio.&lt;/p&gt;

&lt;p&gt;Slides disponibili:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../pdf/4a.Edge_Detection.pdf&#34;&gt;Edge Detection&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../pdf/4b.FFT.pdf&#34;&gt;Frequency Filtering&lt;/a&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La lezione è corredata di due notebook:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/3.Edge_detection.ipynb&#34; target=&#34;_blank&#34;&gt;Edge detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/4.Fourier_transform.ipynb&#34; target=&#34;_blank&#34;&gt;Fourier Transform&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Il video di riferimento della lezione è disponibile &lt;a href=&#34;https://web.microsoftstream.com/video/07df3bb6-5d74-47c2-8070-33d6cdacd2b4&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 5.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 3, 4.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 4.2.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch. 10, 11, 13.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Andrew Ng on Edge Detection: youtube lectures &lt;a href=&#34;https://www.youtube.com/watch?v=XuD4C8vJzEQ&#34; target=&#34;_blank&#34;&gt;(1)&lt;/a&gt; e &lt;a href=&#34;https://www.youtube.com/watch?v=am36dePheDc&#34; target=&#34;_blank&#34;&gt;(2)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://datacarpentry.org/image-processing/08-edge-detection/&#34; target=&#34;_blank&#34;&gt;Introduction to Edge Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tutorialspoint.com/dip/concept_of_edge_detection.htm&#34; target=&#34;_blank&#34;&gt;Concept of Image detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=aDY4aBLFOIg&#34; target=&#34;_blank&#34;&gt;OpenCV Python Tutorial: Image Gradients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@hicraigchen/digital-image-processing-using-fourier-transform-in-python-bcb49424fd82&#34; target=&#34;_blank&#34;&gt;Filtraggio con la trasformata&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Edges, Lines, Shapes</title>
      <link>https://gmanco.github.io/courses/computervision/lecture5/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture5/</guid>
      <description>

&lt;p&gt;Edge Detection. Canny Edge Detection. Hough Transform.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/5.Edges_lines_shapes.pdf&#34;&gt;qui&lt;/a&gt;. La lezione è corredata di un &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/5.Edges_lines_circles.ipynb&#34; target=&#34;_blank&#34;&gt;notebook&lt;/a&gt;. Il video della lezione è disponibile &lt;a href=&#34;https://web.microsoftstream.com/video/0d774350-bb0b-48fd-a11e-ec33464421fc&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 5, 10, 11.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Sect. 10.2.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 4.2.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch. 13,14.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/canny-edge-detection-step-by-step-in-python-computer-vision-b49c3a2d8123&#34; target=&#34;_blank&#34;&gt;Canny Edge Detection step by step&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/master/da/d22/tutorial_py_canny.html&#34; target=&#34;_blank&#34;&gt;OpenCV Python Tutorial: Canny Edge Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/lines-detection-with-hough-transform-84020b3b1549&#34; target=&#34;_blank&#34;&gt;Introduction to Hough Transform&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.sci.utah.edu/~gerig/CS7960-S2010/handouts/Ballard-Generalized-HoughT.pdf&#34; target=&#34;_blank&#34;&gt;Generalizing the Hough Transform to detect arbitrary shapes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1502.02160.pdf&#34; target=&#34;_blank&#34;&gt;A survey on Hough Transform&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Python and image processing</title>
      <link>https://gmanco.github.io/courses/computervision/lecture_lab3/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture_lab3/</guid>
      <description>

&lt;p&gt;Python &amp;amp; image processing.&lt;/p&gt;

&lt;p&gt;Principali argomenti trattati:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Trasformata di Hough&lt;/li&gt;
&lt;li&gt;Trasformata di FFT&lt;/li&gt;
&lt;li&gt;Presentazione modalità esonero&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La lezione è corredata di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/labs_lecture/lab03&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 4.3.2&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/3.4/d9/db0/tutorial_hough_lines.html&#34; target=&#34;_blank&#34;&gt;Hough with OpenCV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-image.org/docs/dev/auto_examples/edges/plot_line_hough_transform.html&#34; target=&#34;_blank&#34;&gt;Straight line Hough transform with skimage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://homepages.inf.ed.ac.uk/rbf/HIPR2/hough.htm&#34; target=&#34;_blank&#34;&gt;Hough introduction and excercise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/lines-detection-with-hough-transform-84020b3b1549&#34; target=&#34;_blank&#34;&gt;Line detection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Feature descriptors. Classificazione</title>
      <link>https://gmanco.github.io/courses/computervision/lecture6/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture6/</guid>
      <description>

&lt;p&gt;Introduzione al supervised learning. Regressione logistica. Feature descriptors. Harris Corner Detection. SIFT.&lt;/p&gt;

&lt;p&gt;Slides disponibili:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../pdf/6a.Classificazione.pdf&#34;&gt;Introduzione alla classificatione&lt;/a&gt;. &lt;a href=&#34;https://web.microsoftstream.com/video/3709906f-41a8-4557-9e53-118a2b453b6a&#34; target=&#34;_blank&#34;&gt;Video&lt;/a&gt; della lezione, con appunti &lt;a href=&#34;../pdf/l6_appunti.pdf&#34;&gt;allegati&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../pdf/6b.Features.pdf&#34;&gt;Keypoint detection and Descritpors&lt;/a&gt;. &lt;a href=&#34;https://web.microsoftstream.com/video/544b3842-6736-4a41-b7e2-5bd033fd9a80&#34; target=&#34;_blank&#34;&gt;Video&lt;/a&gt; della lezione, con appunti &lt;a href=&#34;../pdf/l7_appunti.pdf&#34;&gt;allegati&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notebooks di studio:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/6a.Features_Harris.ipynb&#34; target=&#34;_blank&#34;&gt;Harris corner detector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/labs_lecture/lab04/6b_Features_SIFT.ipynb&#34; target=&#34;_blank&#34;&gt;SIFT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, Ch. 6, 13.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 2, 8.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, Ch. 4.1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch. 17.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9&#34; target=&#34;_blank&#34;&gt;Introduction to Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/logistic-regression-tutorial-for-machine-learning/&#34; target=&#34;_blank&#34;&gt;Tutorial on Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mathematical-tours.github.io/book-sources/optim-ml/OptimML.pdf&#34; target=&#34;_blank&#34;&gt;Course Notes on Optimization for Machine Learning&lt;/a&gt; and &lt;a href=&#34;https://mathematical-tours.github.io/book-sources/FundationsDataScience.pdf&#34; target=&#34;_blank&#34;&gt;Mathematical Foundations of Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/tutorial/basic/tutorial.html&#34; target=&#34;_blank&#34;&gt;An Introduction to Machine Learning with scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/analytics-vidhya/introduction-to-feature-detection-and-matching-65e27179885d&#34; target=&#34;_blank&#34;&gt;Introduction to Feature Detection and Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/data-breach/introduction-to-harris-corner-detector-32a88850b3f6&#34; target=&#34;_blank&#34;&gt;Harris Corner Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aishack.in/tutorials/sift-scale-invariant-feature-transform-features/&#34; target=&#34;_blank&#34;&gt;SIFT: Theory and Practice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_table_of_contents_feature2d/py_table_of_contents_feature2d.html&#34; target=&#34;_blank&#34;&gt;OpenCV Tutorials on Feature Descriptors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://people.eecs.berkeley.edu/~malik/cs294/lowe-ijcv04.pdf&#34; target=&#34;_blank&#34;&gt;Distinctive image features from scale invariant Keypoints&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://persci.mit.edu/pub_pdfs/pyramid83.pdf&#34; target=&#34;_blank&#34;&gt;The laplacian pyramid as a compact image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/sift-scale-invariant-feature-transform-c7233dc60f37&#34; target=&#34;_blank&#34;&gt;SIFT: a detailed description&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Neural Networks</title>
      <link>https://gmanco.github.io/courses/computervision/lecture7/</link>
      <pubDate>Sat, 18 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture7/</guid>
      <description>

&lt;p&gt;Grafi di computazione e modelli non lineari. Reti Convoluzionali. LeNet-5.&lt;/p&gt;

&lt;p&gt;Materiale didattico:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Modelli non lineari (&lt;a href=&#34;../pdf/7.Reti neurali_parte1.pdf&#34;&gt;slides&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Reti convoluzionali (&lt;a href=&#34;../pdf/7.Reti neurali_parte2.pdf&#34;&gt;slides&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/7.Neural_networks.ipynb&#34; target=&#34;_blank&#34;&gt;Notebook&lt;/a&gt; di accompagnamento&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, Ch. 15.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 13.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, Ch. 2,3&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9&#34; target=&#34;_blank&#34;&gt;Introduction to Neural Networks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://d2l.ai/index.html&#34; target=&#34;_blank&#34;&gt;Dive into Deep Learning&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://mathematical-tours.github.io/book-sources/optim-ml/OptimML.pdf&#34; target=&#34;_blank&#34;&gt;Course Notes on Optimization for Machine Learning&lt;/a&gt; and &lt;a href=&#34;https://mathematical-tours.github.io/book-sources/FundationsDataScience.pdf&#34; target=&#34;_blank&#34;&gt;Mathematical Foundations of Data Science&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/&#34; target=&#34;_blank&#34;&gt;Understanding graphs and automatic differentiation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95&#34; target=&#34;_blank&#34;&gt;Pytorch autograd: Understanding the heart of Pytorch&amp;rsquo;s magic&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b&#34; target=&#34;_blank&#34;&gt;Yes you should understand backpropagation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf&#34; target=&#34;_blank&#34;&gt;Visualizing and understanding convolutional Networks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Vincent Dumoulin, Francesco Visin - &lt;a href=&#34;https://arxiv.org/abs/1603.07285&#34; target=&#34;_blank&#34;&gt;A guide to convolution arithmetic for deep learning&lt;/a&gt;]&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@_init_/an-illustrated-explanation-of-performing-2d-convolutions-using-matrix-multiplications-1e8de8cd2544&#34; target=&#34;_blank&#34;&gt;An illustrated explanation of performing 2D convolution using matrix multiplication&lt;/a&gt;. Un esempio illustrativo anche su &lt;a href=&#34;https://www.slideshare.net/EdwinEfranJimnezLepe/convolution-as-matrix-multiplication?from_action=save&#34; target=&#34;_blank&#34;&gt;slideshare&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/datadriveninvestor/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4&#34; target=&#34;_blank&#34;&gt;Visualizing neural networks using saliency maps&lt;/a&gt;. Reference paper su &lt;a href=&#34;https://arxiv.org/abs/1312.6034&#34; target=&#34;_blank&#34;&gt;arxiv&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Saliency maps in &lt;a href=&#34;[https://mc.ai/feature-visualisation-in-pytorch%E2%80%8A-%E2%80%8Asaliency-maps/](https://mc.ai/feature-visualisation-in-pytorch - saliency-maps/)&#34; target=&#34;_blank&#34;&gt;Flashtorch&lt;/a&gt;. Medium &lt;a href=&#34;https://towardsdatascience.com/feature-visualisation-in-pytorch-saliency-maps-a3f99d08f78a&#34; target=&#34;_blank&#34;&gt;description&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A collection of &lt;a href=&#34;https://github.com/utkuozbulak/pytorch-cnn-visualizations&#34; target=&#34;_blank&#34;&gt;Deep network visualization techniques&lt;/a&gt; in Pytorch&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Image Classification</title>
      <link>https://gmanco.github.io/courses/computervision/lecture_lab5/</link>
      <pubDate>Thu, 23 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture_lab5/</guid>
      <description>

&lt;p&gt;Principali argomenti trattati:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AlexNet&lt;/li&gt;
&lt;li&gt;VGG&lt;/li&gt;
&lt;li&gt;Residual Network&lt;/li&gt;
&lt;li&gt;Inception Network&lt;/li&gt;
&lt;li&gt;Data Augmentation&lt;/li&gt;
&lt;li&gt;Transfer Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Il materiale didattico (lucidi e notebooks) è disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/labs_lecture/lab05&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 15.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, ch. 4.6, 5, 6.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/torchvision/models.html&#34; target=&#34;_blank&#34;&gt;Torch pretrained models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/hub/&#34; target=&#34;_blank&#34;&gt;TorchHUB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.vision.caltech.edu/Image_Datasets/Caltech101/&#34; target=&#34;_blank&#34;&gt;Caltech101 dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/article/10.1186/s40537-019-0197-0&#34; target=&#34;_blank&#34;&gt;Survey on data augmentation technique&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Introduzione a Pytorch</title>
      <link>https://gmanco.github.io/courses/computervision/lecture_lab4/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture_lab4/</guid>
      <description>

&lt;p&gt;Esercitazione 4&lt;/p&gt;

&lt;p&gt;Principali argomenti trattati:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pytorch basic operations&lt;/li&gt;
&lt;li&gt;Convoluzioni e DNN&lt;/li&gt;
&lt;li&gt;Applicazione pratica di SIFT&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Il materiale didattico è disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/labs_lecture/lab04&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, sect. 15&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/beginner/basics/intro.html&#34; target=&#34;_blank&#34;&gt;Pytorch Basic Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/computational-graphs-in-pytorch-and-tensorflow-c25cc40bdcd1&#34; target=&#34;_blank&#34;&gt;Gradiente e Computational Graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53&#34; target=&#34;_blank&#34;&gt;Convolutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vdumoulin/conv_arithmetic&#34; target=&#34;_blank&#34;&gt;Convolutions: padding, stride, dilation animation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&#34; target=&#34;_blank&#34;&gt;SIFT Paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Object Detection</title>
      <link>https://gmanco.github.io/courses/computervision/lecture8/</link>
      <pubDate>Tue, 28 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture8/</guid>
      <description>

&lt;p&gt;Object detection.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Region Proposal Networks: R-CNN, Fast R-CNN, Faster R-CNN.&lt;/li&gt;
&lt;li&gt;Single Shot Detectors: SSD, RetinaNet, YOLO.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Materiale didattico:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../pdf/8.Object_detection.pdf&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/8a.Object_Detection.ipynb&#34; target=&#34;_blank&#34;&gt;Notebook&lt;/a&gt; di accompagnamento&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/8b.YOLO_demo.ipynb&#34; target=&#34;_blank&#34;&gt;Demo YOLO&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, Ch. 7&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@fractaldle/brief-overview-on-object-detection-algorithms-ec516929be93&#34; target=&#34;_blank&#34;&gt;An overview of Deep-Learning based Object Detection algorithms&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1807.05511.pdf&#34; target=&#34;_blank&#34;&gt;Object detection with Deep Learning: A review&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1311.2524.pdf&#34; target=&#34;_blank&#34;&gt;Rich Feature-Based hierarchies for accurate object detection and semantic segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1504.08083.pdf&#34; target=&#34;_blank&#34;&gt;Fast R-CNN&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1506.01497&#34; target=&#34;_blank&#34;&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Faster R-CNN tutorials: &lt;a href=&#34;https://medium.com/@fractaldle/guide-to-build-faster-rcnn-in-pytorch-95b10c273439&#34; target=&#34;_blank&#34;&gt;A guide to building Faster R-CNN in Pytorch&lt;/a&gt; e &lt;a href=&#34;https://towardsdatascience.com/fasterrcnn-explained-part-1-with-code-599c16568cff&#34; target=&#34;_blank&#34;&gt;Detecting objects in (almost) Real time&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1512.02325&#34; target=&#34;_blank&#34;&gt;SSD: Single Shot MultiBox Detector&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection&#34; target=&#34;_blank&#34;&gt;SSD Tutorial&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/detectron&#34; target=&#34;_blank&#34;&gt;Detectron&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/detectron2&#34; target=&#34;_blank&#34;&gt;Detectron2&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;You Only Look Once: &lt;a href=&#34;https://arxiv.org/pdf/1506.02640.pdf&#34; target=&#34;_blank&#34;&gt;YOLOv1&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1612.08242.pdf&#34; target=&#34;_blank&#34;&gt;YOLOv2&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1804.02767.pdf&#34; target=&#34;_blank&#34;&gt;YOLOv3&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://christopher5106.github.io/object/detectors/2017/08/10/bounding-box-object-detectors-understanding-yolo.html&#34; target=&#34;_blank&#34;&gt;Understanding YOLO&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b&#34; target=&#34;_blank&#34;&gt;What&amp;rsquo;s new in YOLOv3?&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@jonathan_hui/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359&#34; target=&#34;_blank&#34;&gt;Speed and accuracy comparison in object detection&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.10934&#34; target=&#34;_blank&#34;&gt;YOLOv4&lt;/a&gt; paper (with &lt;a href=&#34;https://github.com/AlexeyAB/darknet&#34; target=&#34;_blank&#34;&gt;code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@riteshkanjee/yolov4-superior-faster-more-accurate-object-detection-7e8194bf1872&#34; target=&#34;_blank&#34;&gt;YOLOv4 -  Superior, faster and more accurate&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@michaelchan_2146/faster-real-time-object-detection-yolov4-in-pytorch-6eef8436ba75&#34; target=&#34;_blank&#34;&gt;YOLOv4 in Pytorch&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/axinc-ai/yolov5-the-latest-model-for-object-detection-b13320ec516b&#34; target=&#34;_blank&#34;&gt;YOLOv5: The Latest Model for Object Detection&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1708.02002&#34; target=&#34;_blank&#34;&gt;Focal Loss for Dense Object Detection&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4&#34; target=&#34;_blank&#34;&gt;Review - RetinaNet&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://blog.zenggyu.com/en/post/2018-12-05/retinanet-explained-and-demystified/#fn3&#34; target=&#34;_blank&#34;&gt;RetinaNet explained and demystified&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@lekorotkov/5-tools-to-create-a-custom-object-detection-dataset-27ca37f91e05&#34; target=&#34;_blank&#34;&gt;5 tools to create a custom object detection dataset&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;NEW&lt;/strong&gt; &lt;a href=&#34;https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/&#34; target=&#34;_blank&#34;&gt;DETR: End-to-end object detection with Transformers&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Esonero 1</title>
      <link>https://gmanco.github.io/courses/computervision/esoneri/esonero1/</link>
      <pubDate>Tue, 28 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/esoneri/esonero1/</guid>
      <description>

&lt;h1 id=&#34;risultati-esonero-su-image-processing&#34;&gt;Risultati esonero su image processing&lt;/h1&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Matricola&lt;/th&gt;
&lt;th&gt;Nome&lt;/th&gt;
&lt;th&gt;Cognome&lt;/th&gt;
&lt;th&gt;VOTO&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;207057&lt;/td&gt;
&lt;td&gt;GIUSEPPE&lt;/td&gt;
&lt;td&gt;SURACE&lt;/td&gt;
&lt;td&gt;3,8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;214244&lt;/td&gt;
&lt;td&gt;PIERPAOLO&lt;/td&gt;
&lt;td&gt;BELLUSCI&lt;/td&gt;
&lt;td&gt;7,5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;214254&lt;/td&gt;
&lt;td&gt;VITO&lt;/td&gt;
&lt;td&gt;BARBARA&lt;/td&gt;
&lt;td&gt;6,5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;214455&lt;/td&gt;
&lt;td&gt;SERAFINO&lt;/td&gt;
&lt;td&gt;SALATINO&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;214503&lt;/td&gt;
&lt;td&gt;FRANCESCO&lt;/td&gt;
&lt;td&gt;RIGANELLO&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;214510&lt;/td&gt;
&lt;td&gt;CLAUDIA&lt;/td&gt;
&lt;td&gt;GRAZIOSO&lt;/td&gt;
&lt;td&gt;3,2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;214533&lt;/td&gt;
&lt;td&gt;GIULIANO&lt;/td&gt;
&lt;td&gt;STIRPARO&lt;/td&gt;
&lt;td&gt;1,5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;214555&lt;/td&gt;
&lt;td&gt;ADELE PIA&lt;/td&gt;
&lt;td&gt;ROMANO&lt;/td&gt;
&lt;td&gt;8,9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;214566&lt;/td&gt;
&lt;td&gt;ROSARIA&lt;/td&gt;
&lt;td&gt;SPADA&lt;/td&gt;
&lt;td&gt;5,7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;214587&lt;/td&gt;
&lt;td&gt;GIOVANNI&lt;/td&gt;
&lt;td&gt;MIRANTE&lt;/td&gt;
&lt;td&gt;5,3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;214636&lt;/td&gt;
&lt;td&gt;FRANCESCA&lt;/td&gt;
&lt;td&gt;SENATORE&lt;/td&gt;
&lt;td&gt;0,9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;214808&lt;/td&gt;
&lt;td&gt;SILVIA&lt;/td&gt;
&lt;td&gt;PLUTINO&lt;/td&gt;
&lt;td&gt;1,4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;216635&lt;/td&gt;
&lt;td&gt;IVAN&lt;/td&gt;
&lt;td&gt;SCUDERI&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;216640&lt;/td&gt;
&lt;td&gt;ANDREA&lt;/td&gt;
&lt;td&gt;ZOFREA&lt;/td&gt;
&lt;td&gt;4,8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;216648&lt;/td&gt;
&lt;td&gt;FRANCESCO MARIA&lt;/td&gt;
&lt;td&gt;GRANATA&lt;/td&gt;
&lt;td&gt;6,8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;216649&lt;/td&gt;
&lt;td&gt;MATTIA&lt;/td&gt;
&lt;td&gt;GATTO&lt;/td&gt;
&lt;td&gt;3,3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;216652&lt;/td&gt;
&lt;td&gt;DOMENICO&lt;/td&gt;
&lt;td&gt;FRONTERA&lt;/td&gt;
&lt;td&gt;4,6&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;216662&lt;/td&gt;
&lt;td&gt;ARIANNA&lt;/td&gt;
&lt;td&gt;RUSSO&lt;/td&gt;
&lt;td&gt;1,9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;216694&lt;/td&gt;
&lt;td&gt;LUCIA&lt;/td&gt;
&lt;td&gt;GENTILE&lt;/td&gt;
&lt;td&gt;3,9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;216701&lt;/td&gt;
&lt;td&gt;GAETANO&lt;/td&gt;
&lt;td&gt;PALMIERI&lt;/td&gt;
&lt;td&gt;3,5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;216759&lt;/td&gt;
&lt;td&gt;LOPEZ&lt;/td&gt;
&lt;td&gt;SIMONPAOLO&lt;/td&gt;
&lt;td&gt;4,7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;224431&lt;/td&gt;
&lt;td&gt;SALVATORE&lt;/td&gt;
&lt;td&gt;PETROLO&lt;/td&gt;
&lt;td&gt;7,5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Le bozze di soluzione della prova d&amp;rsquo;esonero sono disponibili su &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/tree/master/test/Esonero1/soluzioni_esonero1&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

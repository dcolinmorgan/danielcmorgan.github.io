<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Giuseppe Manco</title>
    <link>https://gmanco.github.io/</link>
      <atom:link href="https://gmanco.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Giuseppe Manco</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Giuseppe Manco</copyright><lastBuildDate>Mon, 25 May 2020 00:00:00 +0100</lastBuildDate>
    <image>
      <url>https://gmanco.github.io/img/manco.jpg</url>
      <title>Giuseppe Manco</title>
      <link>https://gmanco.github.io/</link>
    </image>
    
    <item>
      <title>Introduzione al corso</title>
      <link>https://gmanco.github.io/courses/computervision/lecture1/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture1/</guid>
      <description>

&lt;p&gt;Introduzione al corso. Image Processing, Analysis e Computer Vision.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/1.Intro.pdf&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, sect. 1.1-1.3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, Ch. 1,2.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://algorithmia.com/blog/introduction-to-computer-vision&#34; target=&#34;_blank&#34;&gt;Introduction to Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/what-is-computer-vision/&#34; target=&#34;_blank&#34;&gt;A gentle introduction to Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/readers-writers-digest/beginners-guide-to-computer-vision-23606224b720&#34; target=&#34;_blank&#34;&gt;A Beginner&amp;rsquo;s guide to Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/everything-you-ever-wanted-to-know-about-computer-vision-heres-a-look-why-it-s-so-awesome-e8a58dfb641e&#34; target=&#34;_blank&#34;&gt;Everything you wanted to know about Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.thecvf.com/&#34; target=&#34;_blank&#34;&gt;The Computer Vision Foundation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.forbes.com/sites/forbestechcouncil/2020/04/03/three-ways-computer-vision-is-transforming-marketing/#1d6346d0214b&#34; target=&#34;_blank&#34;&gt;Three ways Computer Vision is transforming Marketing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;approfondimenti-contributi-dagli-studenti-alcune-applicazioni-per-la-computer-vision&#34;&gt;Approfondimenti (contributi dagli studenti): Alcune applicazioni per la computer vision&lt;/h2&gt;

&lt;h3 id=&#34;amazon-go&#34;&gt;Amazon GO&lt;/h3&gt;

&lt;p&gt;Il progetto Amazon GO consiste in un negozio in cui non è presente alcuna cassa in quanto il check-out avviene automaticamente direttamente sul proprio account Amazon. I clienti attivano l’app sul telefono e scansionano il proprio qr-code prima di entrare nel negozio, dopodiché un sistema di videocamere e sensori tracceranno gli spostamenti dei clienti, tenendo traccia dei prodotti da loro acquistati. All’uscita dal negozio si scansiona nuovamente il qr-code ed i prodotti acquistati verranno addebitati direttamente sul proprio account amazon.
Riferimenti:
- &lt;a href=&#34;https://youtu.be/uoKsY9HDk6o&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;
- &lt;a href=&#34;https://emerj.com/ai-sector-overviews/computer-vision-applications-shopping-driving-and-more/&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;blood-loss-monitoring&#34;&gt;Blood loss monitoring&lt;/h3&gt;

&lt;p&gt;Questo sistema utilizza le immagini delle spugne chirurgiche e delle taniche di aspirazione per andare a stimare la perdita di sangue tramite il processamento di tali immagini tramite algoritmi di machine learning e di computer vision. Questa applicazione viene utilizzata in alcuni ospedali durante i parti cesarei. Tali misurazioni risultano essere più accurate delle valutazioni fatte ad occhio nudo e risultano essere fondamentali per stabilire se siano necessarie trasfusioni o meno.
Riferimenti:
- &lt;a href=&#34;https://youtu.be/cBzJ43zU4FY&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;
- &lt;a href=&#34;https://emerj.com/ai-sector-overviews/computer-vision-applications-shopping-driving-and-more/&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;real-time-sports-tracking&#34;&gt;Real-time sports tracking&lt;/h3&gt;

&lt;p&gt;La computer vision viene utilizzata per aiutare quella che è l’analisi di gioco e delle strategie ed anche per andare a misurare le performance dei giocatori, questa però non è la sola applicazione utilizzata infatti quest’ultima viene utilizzata anche per studiare quella che è la visibilità dei brand presenti ad esempio sulle divise dei giocatori. Questo è quello che viene effettuato dall’azienda Gumgum.
Riferimenti:
- &lt;a href=&#34;https://gumgum.com/sports/&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;
- &lt;a href=&#34;https://www.forbes.com/sites/bernardmarr/2019/04/08/7-amazing-examples-of-computer-and-machine-vision-in-practice/#2c1921cc1018&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;from-image-to-3d-models&#34;&gt;From image to 3D models&lt;/h3&gt;

&lt;p&gt;La computer vision ha consentito la costruzione di sistemi in grado di ricostruire modelli 3d a partire dalle immagini. I dettagli di tale lavoro sono descritti nel seguente paper. Quest’ultimo mostra anche una serie di campi in cui viene impiegata tale applicazione.
Link utili:
- &lt;a href=&#34;https://www.researchgate.net/profile/Marc_Pollefeys/publication/220423006_From_images_to_3D_models/links/53f5cf0e0cf22be01c3fb089/From-images-to-3D-models.pdf&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;food-quality-control&#34;&gt;Food quality control&lt;/h3&gt;

&lt;p&gt;La computer vision viene utilizzata anche per effettuare la valutazione della qualità del cibo, tale applicazione viene facilmente utilizzata nel campo industriale in quanto veloce e maggiormente oggettiva rispetto all’analisi umana. Il paper riportato riporta una carrellata di tecniche utilizzate e dell’accuratezza da esse riportate per diverse tipologie di cibo che vanno dalla verdura alla frutta a cibi più complessi come ad esempio la pizza ed i dolci.
Link utili:
- &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0168169902001011&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Concetti fondamentali su Image processing e analysis</title>
      <link>https://gmanco.github.io/courses/computervision/lecture2/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture2/</guid>
      <description>

&lt;p&gt;Concetti fondamentali su Image processing e analysis: Image Basics, Manipolazione di immagini. Introduzione alle librerie Python per Image Processing.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/2.Image_fundamentals.pdf&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;La lezione è corredata di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/1.Image_fundamentals.ipynb&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 2.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, sect. 1.4-1.5.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 2-3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 3.1, 3.6.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch.1-6.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://cs231n.github.io/python-numpy-tutorial/&#34; target=&#34;_blank&#34;&gt;Stanford cs231n Python tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cs231n.github.io/ipython-tutorial/&#34; target=&#34;_blank&#34;&gt;Stanford cs231n Jupyter tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.anaconda.com/&#34; target=&#34;_blank&#34;&gt;Anaconda, la piattaforma di riferimento per installare Python e Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://notebooks.azure.com/&#34; target=&#34;_blank&#34;&gt;Piattaforma azure di Microsoft per l&amp;rsquo;esecuzione di notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/notebooks/intro.ipynb&#34; target=&#34;_blank&#34;&gt;Google Colab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://missing.csail.mit.edu/?fbclid=IwAR0N3s-DbRLrSWW3tB1L5iu_thdiEtFuL7cGUwxyOL-yc7skytSDGdT9ZAo&#34; target=&#34;_blank&#34;&gt;The missing semester, un mini-corso del MIT sui tool di base che tutti i data scientist dovrebbero conoscere&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Filtri</title>
      <link>https://gmanco.github.io/courses/computervision/lecture3/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture3/</guid>
      <description>

&lt;p&gt;Manipolazione spaziale. Filtri. Convoluzione.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/3.Filters.pdf&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;La lezione è corredata di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/2.Filters.ipynb&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, Ch. 3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 3.2, 3.3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch. 2, 10-12.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/7075360&#34; target=&#34;_blank&#34;&gt;Adaptive filters for color image processing: A survey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/image-filters-in-python-26ee938e57d2&#34; target=&#34;_blank&#34;&gt;Image filters in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scipy-lectures.org/advanced/image_processing/&#34; target=&#34;_blank&#34;&gt;Image manipulation using Numpy and SciPy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@enzoftware/how-to-build-amazing-images-filters-with-python-median-filter-sobel-filter-%EF%B8%8F-%EF%B8%8F-22aeb8e2f540&#34; target=&#34;_blank&#34;&gt;How to build an image filter in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=otLGDpBglEA&amp;amp;feature=player_embedded&#34; target=&#34;_blank&#34;&gt;Instagram filters in 15 lines of code&lt;/a&gt;. Il codice è disponibile &lt;a href=&#34;https://github.com/lukexyz/CV-Instagram-Filters/blob/master/gotham.py&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Python &amp; image processing</title>
      <link>https://gmanco.github.io/courses/computervision/lecture_lab1/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture_lab1/</guid>
      <description>

&lt;p&gt;Principali argomenti trattati:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Introduzione ai Jupyter notebooks&lt;/li&gt;
&lt;li&gt;Caricamento di immagini con PIL&lt;/li&gt;
&lt;li&gt;Panoramica delle librerie e delle principali funzioni per effettuare trasformazioni geometriche, operazioni aritmetiche, edge detection.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/es01_imageloading.pdf&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;La lezione è corredata di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/labs_lecture/lab01&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 5.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 4.2.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch. 13.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/master/index.html&#34; target=&#34;_blank&#34;&gt;OpenCV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-image.org/docs/stable/api/api.html&#34; target=&#34;_blank&#34;&gt;Scikit Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pillow.readthedocs.io/en/stable/handbook/tutorial.html&#34; target=&#34;_blank&#34;&gt;PIL tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/scikit-image/skimage-tutorials&#34; target=&#34;_blank&#34;&gt;Scikit Image Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Derivate. Edge Detection</title>
      <link>https://gmanco.github.io/courses/computervision/lecture4/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture4/</guid>
      <description>

&lt;p&gt;Derivate di immagini. Gradient-based filtering. trasformata di Fourier e Filtraggio.&lt;/p&gt;

&lt;p&gt;Slides disponibili:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../pdf/4a.Edge Detection.pdf&#34;&gt;edge detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../pdf/4b.Edge_Detection.pdf&#34;&gt;Fourier Transform&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La lezione è corredata di due notebook:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/3.Edge_detection.ipynb&#34; target=&#34;_blank&#34;&gt;Edge detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/4.Fourier_transform.ipynb&#34; target=&#34;_blank&#34;&gt;Fourier Transform&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 5.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 3, 4.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 4.2.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch. 10, 11, 13.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Andrew Ng on Edge Detection: youtube lectures &lt;a href=&#34;https://www.youtube.com/watch?v=XuD4C8vJzEQ&#34; target=&#34;_blank&#34;&gt;(1)&lt;/a&gt; e &lt;a href=&#34;https://www.youtube.com/watch?v=am36dePheDc&#34; target=&#34;_blank&#34;&gt;(2)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://datacarpentry.org/image-processing/08-edge-detection/&#34; target=&#34;_blank&#34;&gt;Introduction to Edge Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tutorialspoint.com/dip/concept_of_edge_detection.htm&#34; target=&#34;_blank&#34;&gt;Concept of Image detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=aDY4aBLFOIg&#34; target=&#34;_blank&#34;&gt;OpenCV Python Tutorial: Image Gradients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@hicraigchen/digital-image-processing-using-fourier-transform-in-python-bcb49424fd82&#34; target=&#34;_blank&#34;&gt;Filtraggio con la trasformata&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Feature descriptors. Classificazione</title>
      <link>https://gmanco.github.io/courses/computervision/lecture5/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture5/</guid>
      <description>

&lt;p&gt;Derivate di immagini. Gradient-based filtering. trasformata di Fourier e Filtraggio.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/5.Classificazione.pdf&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Importante&lt;/strong&gt;
La parte relativa allo studio dei feature descriptors riguarderà anche il primo esonero. I dettagli dell&amp;rsquo;esonero sono pubblicati &lt;a href=&#34;https://gmanco.github.io/courses/computervision/esoneri/listoffeaturedescriptors/&#34; target=&#34;_blank&#34;&gt;nell&amp;rsquo;apposita pagina&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, Ch. 6, 13.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 2, 8.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, Ch. 4.1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch. 17.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9&#34; target=&#34;_blank&#34;&gt;Introduction to Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/logistic-regression-tutorial-for-machine-learning/&#34; target=&#34;_blank&#34;&gt;Tutorial on Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mathematical-tours.github.io/book-sources/optim-ml/OptimML.pdf&#34; target=&#34;_blank&#34;&gt;Course Notes on Optimization for Machine Learning&lt;/a&gt; and &lt;a href=&#34;https://mathematical-tours.github.io/book-sources/FundationsDataScience.pdf&#34; target=&#34;_blank&#34;&gt;Mathematical Foundations of Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/tutorial/basic/tutorial.html&#34; target=&#34;_blank&#34;&gt;An Introduction to Machine Learning with scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/analytics-vidhya/introduction-to-feature-detection-and-matching-65e27179885d&#34; target=&#34;_blank&#34;&gt;Introduction to Feature Detection and Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aishack.in/tutorials/sift-scale-invariant-feature-transform-features/&#34; target=&#34;_blank&#34;&gt;SIFT: Theory and Practice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_table_of_contents_feature2d/py_table_of_contents_feature2d.html&#34; target=&#34;_blank&#34;&gt;OpenCV Tutorials on Feature Descriptors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Pytorch, SKlearn and Classification</title>
      <link>https://gmanco.github.io/courses/computervision/lecture_lab2/</link>
      <pubDate>Tue, 07 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture_lab2/</guid>
      <description>

&lt;p&gt;Pytorch &amp;amp; SKlearn&lt;/p&gt;

&lt;p&gt;Principali argomenti trattati:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pre-process di un dataset e costruzione dei train/validation/test set&lt;/li&gt;
&lt;li&gt;SKLearn, regressione lineare e logistica&lt;/li&gt;
&lt;li&gt;Introduzione a Pytorch&lt;/li&gt;
&lt;li&gt;Costruzione di un modello di NN&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La lezione è corredata di due notebook disponibili &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/labs_lecture/lab02&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 13.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, ch. 2.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/&#34; target=&#34;_blank&#34;&gt;SKLearn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/index.html&#34; target=&#34;_blank&#34;&gt;Pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.openml.org/d/554/&#34; target=&#34;_blank&#34;&gt;MINIST dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Neural Networks</title>
      <link>https://gmanco.github.io/courses/computervision/lecture6/</link>
      <pubDate>Sat, 18 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture6/</guid>
      <description>

&lt;p&gt;Grafi di computazione e modelli non lineari. Reti Convoluzionali. LeNet-5.&lt;/p&gt;

&lt;p&gt;Materiale didattico:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Modelli non lineari (&lt;a href=&#34;../pdf/6a.Reti neurali_parte1.pdf&#34;&gt;slides&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Reti convoluzionali (&lt;a href=&#34;../pdf/6b.Reti neurali_parte2.pdf&#34;&gt;slides&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/6.Neural_networks.ipynb&#34; target=&#34;_blank&#34;&gt;Notebook&lt;/a&gt; di accompagnamento&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, Ch. 15.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 13.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, Ch. 2,3&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9&#34; target=&#34;_blank&#34;&gt;Introduction to Neural Networks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://d2l.ai/index.html&#34; target=&#34;_blank&#34;&gt;Dive into Deep Learning&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://mathematical-tours.github.io/book-sources/optim-ml/OptimML.pdf&#34; target=&#34;_blank&#34;&gt;Course Notes on Optimization for Machine Learning&lt;/a&gt; and &lt;a href=&#34;https://mathematical-tours.github.io/book-sources/FundationsDataScience.pdf&#34; target=&#34;_blank&#34;&gt;Mathematical Foundations of Data Science&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/&#34; target=&#34;_blank&#34;&gt;Understanding graphs and automatic differentiation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b&#34; target=&#34;_blank&#34;&gt;Yes you should understand backpropagation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf&#34; target=&#34;_blank&#34;&gt;Visualizing and understanding convolutional Networks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Vincent Dumoulin, Francesco Visin - &lt;a href=&#34;https://arxiv.org/abs/1603.07285&#34; target=&#34;_blank&#34;&gt;A guide to convolution arithmetic for deep learning&lt;/a&gt;]&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@_init_/an-illustrated-explanation-of-performing-2d-convolutions-using-matrix-multiplications-1e8de8cd2544&#34; target=&#34;_blank&#34;&gt;An illustrated explanation of performing 2D convolution using matrix multiplication&lt;/a&gt;. Un esempio illustrativo anche su &lt;a href=&#34;https://www.slideshare.net/EdwinEfranJimnezLepe/convolution-as-matrix-multiplication?from_action=save&#34; target=&#34;_blank&#34;&gt;slideshare&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/datadriveninvestor/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4&#34; target=&#34;_blank&#34;&gt;Visualizing neural networks using saliency maps&lt;/a&gt;. Reference paper su &lt;a href=&#34;https://arxiv.org/abs/1312.6034&#34; target=&#34;_blank&#34;&gt;arxiv&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Saliency maps in &lt;a href=&#34;[https://mc.ai/feature-visualisation-in-pytorch%E2%80%8A-%E2%80%8Asaliency-maps/](https://mc.ai/feature-visualisation-in-pytorch - saliency-maps/)&#34; target=&#34;_blank&#34;&gt;Flashtorch&lt;/a&gt;. Medium &lt;a href=&#34;https://towardsdatascience.com/feature-visualisation-in-pytorch-saliency-maps-a3f99d08f78a&#34; target=&#34;_blank&#34;&gt;description&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A collection of &lt;a href=&#34;https://github.com/utkuozbulak/pytorch-cnn-visualizations&#34; target=&#34;_blank&#34;&gt;Deep network visualization techniques&lt;/a&gt; in Pytorch&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Object Detection</title>
      <link>https://gmanco.github.io/courses/computervision/lecture7/</link>
      <pubDate>Tue, 28 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture7/</guid>
      <description>

&lt;p&gt;Object detection.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Region Proposal Networks: R-CNN, Fast R-CNN, Faster R-CNN.&lt;/li&gt;
&lt;li&gt;Single Shot Detectors: SSD, RetinaNet, YOLO.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Materiale didattico:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../pdf/8.Object_detection.pdf&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/8a.Object_Detection.ipynb&#34; target=&#34;_blank&#34;&gt;Notebook&lt;/a&gt; di accompagnamento&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/8b.YOLO_demo.ipynb&#34; target=&#34;_blank&#34;&gt;Demo YOLO&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, Ch. 7&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@fractaldle/brief-overview-on-object-detection-algorithms-ec516929be93&#34; target=&#34;_blank&#34;&gt;An overview of Deep-Learning based Object Detection algorithms&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1311.2524.pdf&#34; target=&#34;_blank&#34;&gt;Rich Feature-Based hierarchies for accurate object detection and semantic segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1504.08083.pdf&#34; target=&#34;_blank&#34;&gt;Fast R-CNN&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1506.01497&#34; target=&#34;_blank&#34;&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Faster R-CNN tutorials: &lt;a href=&#34;https://medium.com/@fractaldle/guide-to-build-faster-rcnn-in-pytorch-95b10c273439&#34; target=&#34;_blank&#34;&gt;A guide to building Faster R-CNN in Pytorch&lt;/a&gt; e &lt;a href=&#34;https://towardsdatascience.com/fasterrcnn-explained-part-1-with-code-599c16568cff&#34; target=&#34;_blank&#34;&gt;Detecting objects in (almost) Real time&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1512.02325&#34; target=&#34;_blank&#34;&gt;SSD: Single Shot MultiBox Detector&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection&#34; target=&#34;_blank&#34;&gt;SSD Tutorial&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/detectron&#34; target=&#34;_blank&#34;&gt;Detectron&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/detectron2&#34; target=&#34;_blank&#34;&gt;Detectron2&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;You Only Look Once: &lt;a href=&#34;https://arxiv.org/pdf/1506.02640.pdf&#34; target=&#34;_blank&#34;&gt;YOLOv1&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1612.08242.pdf&#34; target=&#34;_blank&#34;&gt;YOLOv2&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1804.02767.pdf&#34; target=&#34;_blank&#34;&gt;YOLOv3&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://christopher5106.github.io/object/detectors/2017/08/10/bounding-box-object-detectors-understanding-yolo.html&#34; target=&#34;_blank&#34;&gt;Understanding YOLO&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b&#34; target=&#34;_blank&#34;&gt;What&amp;rsquo;s new in YOLOv3?&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@jonathan_hui/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359&#34; target=&#34;_blank&#34;&gt;Speed and accuracy comparison in object detection&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.10934&#34; target=&#34;_blank&#34;&gt;YOLOv4&lt;/a&gt; paper (with &lt;a href=&#34;https://github.com/AlexeyAB/darknet&#34; target=&#34;_blank&#34;&gt;code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@riteshkanjee/yolov4-superior-faster-more-accurate-object-detection-7e8194bf1872&#34; target=&#34;_blank&#34;&gt;YOLOv4 -  Superior, faster and more accurate&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@michaelchan_2146/faster-real-time-object-detection-yolov4-in-pytorch-6eef8436ba75&#34; target=&#34;_blank&#34;&gt;YOLOv4 in Pytorch&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1708.02002&#34; target=&#34;_blank&#34;&gt;Focal Loss for Dense Object Detection&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4&#34; target=&#34;_blank&#34;&gt;Review - RetinaNet&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://blog.zenggyu.com/en/post/2018-12-05/retinanet-explained-and-demystified/#fn3&#34; target=&#34;_blank&#34;&gt;RetinaNet explained and demystified&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@lekorotkov/5-tools-to-create-a-custom-object-detection-dataset-27ca37f91e05&#34; target=&#34;_blank&#34;&gt;5 tools to create a custom object detection dataset&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;NEW&lt;/strong&gt; &lt;a href=&#34;https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/&#34; target=&#34;_blank&#34;&gt;DETR: End-to-end object detection with Transformers&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>CNN Architectures</title>
      <link>https://gmanco.github.io/courses/computervision/lecture_lab3/</link>
      <pubDate>Fri, 24 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture_lab3/</guid>
      <description>

&lt;p&gt;Principali argomenti trattati:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AlexNet&lt;/li&gt;
&lt;li&gt;VGG&lt;/li&gt;
&lt;li&gt;Residual Network&lt;/li&gt;
&lt;li&gt;Inception Network&lt;/li&gt;
&lt;li&gt;Data Augmentation&lt;/li&gt;
&lt;li&gt;Transfer Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La lezione è corredata di &lt;a href=&#34;../pdf/7.Reti neurali_parte3.pdf&#34;&gt;slides&lt;/a&gt; e di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/labs_lecture/lab03&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Importante&lt;/strong&gt;
Il secondo esonero riguarderà approfondimenti sulle reti convoluzionali. I dettagli dell&amp;rsquo;esonero sono pubblicati &lt;a href=&#34;https://gmanco.github.io/courses/computervision/esoneri/convolution/&#34; target=&#34;_blank&#34;&gt;nell&amp;rsquo;apposita pagina&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 15.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, ch. 4.6, 5, 6.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Yann LeCun&amp;rsquo;s page on &lt;a href=&#34;http://yann.lecun.com/exdb/lenet/&#34; target=&#34;_blank&#34;&gt;LeNet&lt;/a&gt; (with papers and description).&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&#34; target=&#34;_blank&#34;&gt;ImageNet Classification with Deep Convolutional Neural Networks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/difference-between-local-response-normalization-and-batch-normalization-272308c034ac&#34; target=&#34;_blank&#34;&gt;Local Response Norm vs. Bach Norm&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1409.1556v6.pdf&#34; target=&#34;_blank&#34;&gt;Very Deep Convolutional Networks for Large Scale Image Recognition&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43022.pdf&#34; target=&#34;_blank&#34;&gt;Going Deeper with Convolutions&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202&#34; target=&#34;_blank&#34;&gt;A simple guide to the versions of Inception Networks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1502.03167&#34; target=&#34;_blank&#34;&gt;Batch normalization: accelerating deep network training by reducing internal covariate shift&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1512.03385.pdf&#34; target=&#34;_blank&#34;&gt;Residual Learning for image Recognition&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1411.1792.pdf&#34; target=&#34;_blank&#34;&gt;How transferable are features in deep neural networks?&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/torchvision/models.html&#34; target=&#34;_blank&#34;&gt;Torch pretrained models&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/hub/&#34; target=&#34;_blank&#34;&gt;TorchHUB&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.vision.caltech.edu/Image_Datasets/Caltech101/&#34; target=&#34;_blank&#34;&gt;Caltech101 dataset&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://link.springer.com/article/10.1186/s40537-019-0197-0&#34; target=&#34;_blank&#34;&gt;Survey on data augmentation technique&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Object Detection</title>
      <link>https://gmanco.github.io/courses/computervision/lecture_lab4/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture_lab4/</guid>
      <description>

&lt;p&gt;Principali argomenti trattati:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Anchor box&lt;/li&gt;
&lt;li&gt;SSD&lt;/li&gt;
&lt;li&gt;YOLOv3&lt;/li&gt;
&lt;li&gt;RetinaNet&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La lezione è corredata di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/labs_lecture/lab04&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, ch. 7.&lt;/li&gt;
&lt;li&gt;Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y.,  &amp;amp; Berg, A. C. (2016). Ssd: single shot multibox detector. &lt;em&gt;European conference on computer vision&lt;/em&gt; (pp. 21–37).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection&#34; target=&#34;_blank&#34;&gt;SSD tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/andreaazzini/retinanet.pytorch&#34; target=&#34;_blank&#34;&gt;RetinaNet in pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1708.02002&#34; target=&#34;_blank&#34;&gt;Retinanet Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.10934&#34; target=&#34;_blank&#34;&gt;YOLOv4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/retinanet-how-focal-loss-fixes-single-shot-detection-cb320e3bb0de&#34; target=&#34;_blank&#34;&gt;FocalLoss tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/&#34; target=&#34;_blank&#34;&gt;YOLOv3 example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Segmentation</title>
      <link>https://gmanco.github.io/courses/computervision/lecture8/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture8/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Segmentation.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Approcci classici. Conditional Random Fields. &lt;a href=&#34;../pdf/9a.Segmentation_part1.pdf&#34;&gt;slides&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Approcci Neurali. Semantic Segmentation. Instance Segmentation. &lt;a href=&#34;../pdf/9a.Segmentation_part2.pdf&#34;&gt;slides&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/9.Segmentation.ipynb&#34; target=&#34;_blank&#34;&gt;Notebook&lt;/a&gt; di accompagnamento.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, Ch. 5&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, Ch.11&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch.10&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf&#34; target=&#34;_blank&#34;&gt;An Introduction to Conditional Random Fields&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.nowozin.net/sebastian/papers/nowozin2011structured-tutorial.pdf&#34; target=&#34;_blank&#34;&gt;Structured Learning and Prediction in Computer Vision&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.csd.uwo.ca/~yboykov/Papers/pami04.pdf&#34; target=&#34;_blank&#34;&gt;An experimental Comparison of Min-cut/Max Flow algorithms for Energy Minimization in Vision&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1210.5644.pdf&#34; target=&#34;_blank&#34;&gt;Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials&lt;/a&gt; (with &lt;a href=&#34;http://vladlen.info/papers/densecrf-supplementary.pdf&#34; target=&#34;_blank&#34;&gt;Supplemental Material&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/conditional-random-fields-explained-e5b8256da776&#34; target=&#34;_blank&#34;&gt;Conditional Random Fields explained&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/&#34; target=&#34;_blank&#34;&gt;Introduction to Conditional Random Fields&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.topbots.com/semantic-segmentation-guide/&#34; target=&#34;_blank&#34;&gt;A simple guide to semantic Segmentation&lt;/a&gt; and &lt;a href=&#34;https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc&#34; target=&#34;_blank&#34;&gt;A 2019 guide to semantic segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1605.06211&#34; target=&#34;_blank&#34;&gt;Fully Convolutional Networks for Semantic Segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1&#34; target=&#34;_blank&#34;&gt;Review - FCN&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/activating-robotic-minds/up-sampling-with-transposed-convolution-9ae4f2df52d0&#34; target=&#34;_blank&#34;&gt;Upsampling with Transposed Convolution&lt;/a&gt;; &lt;a href=&#34;https://towardsdatascience.com/transposed-convolution-demystified-84ca81b4baba&#34; target=&#34;_blank&#34;&gt;Transposed Convolution demystified&lt;/a&gt;; &lt;a href=&#34;https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8&#34; target=&#34;_blank&#34;&gt;Transposed convolutions explained with Excel&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://distill.pub/2016/deconv-checkerboard/&#34; target=&#34;_blank&#34;&gt;Deconvolution and checkerboard artifacts&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://cvlab.postech.ac.kr/research/deconvnet/&#34; target=&#34;_blank&#34;&gt;Learning Deconvolution Network for Semantic Segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/1511.00561&#34; target=&#34;_blank&#34;&gt;SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation&lt;/a&gt;]&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1505.04597.pdf&#34; target=&#34;_blank&#34;&gt;U-Net: Convolutional Networks for Biomedical Image Segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.09326&#34; target=&#34;_blank&#34;&gt;The One Hundred Layers Tiramisu&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1612.01105.pdf&#34; target=&#34;_blank&#34;&gt;Pyramid scene parsing Networks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1606.00915.pdf&#34; target=&#34;_blank&#34;&gt;DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1511.07122&#34; target=&#34;_blank&#34;&gt;Multiscale Context aggregation by dilated convolutions&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.05587.pdf&#34; target=&#34;_blank&#34;&gt;Rethinking Atrous Convolution for Semantic Image Segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Review: &lt;a href=&#34;https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d&#34; target=&#34;_blank&#34;&gt;DeepLabv2&lt;/a&gt; e &lt;a href=&#34;https://towardsdatascience.com/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74&#34; target=&#34;_blank&#34;&gt;DeepLabv3&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://openreview.net/pdf?id=S1uHiFyyg&#34; target=&#34;_blank&#34;&gt;Speeding up Semantic segmentation for autonomous driving&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1606.02147.pdf&#34; target=&#34;_blank&#34;&gt;ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;ICNet for Real-Time Semantic Segmentation on High-Resolution Images&#34; target=&#34;_blank&#34;&gt;ICNet for Real-Time Semantic Segmentation on High-Resolution Images&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1902.04502&#34; target=&#34;_blank&#34;&gt;Fast-SCNN: Fast Semantic Segmentation Network&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://openaccess.thecvf.com/content_CVPR_2019/html/Li_DFANet_Deep_Feature_Aggregation_for_Real-Time_Semantic_Segmentation_CVPR_2019_paper.html&#34; target=&#34;_blank&#34;&gt;DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1703.06870&#34; target=&#34;_blank&#34;&gt;Mask R-CNN&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@jonathan_hui/image-segmentation-with-mask-r-cnn-ebe6d793272&#34; target=&#34;_blank&#34;&gt;Image Segmentation with Mask R-CNN&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1611.07709.pdf&#34; target=&#34;_blank&#34;&gt;Fully Convolutional Instance-aware Semantic Segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1904.02689.pdf&#34; target=&#34;_blank&#34;&gt;YOLACT&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1912.06218&#34; target=&#34;_blank&#34;&gt;YOLOACT++&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1801.00868.pdf&#34; target=&#34;_blank&#34;&gt;Pantropic Segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Action Recognition</title>
      <link>https://gmanco.github.io/courses/computervision/lecture9/</link>
      <pubDate>Tue, 19 May 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture9/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Action Recognition.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;3D Convolution. Recurrent Networks. Optical Flow. &lt;a href=&#34;../pdf/10.Action_recognition.pdf&#34;&gt;slides&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/10.action_recognition.ipynb&#34; target=&#34;_blank&#34;&gt;Notebook&lt;/a&gt; di accompagnamento.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, Ch. 8&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, Ch.20&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;•https://www.pyimagesearch.com/2019/11/25/human-activity-recognition-with-opencv-and-deep-learning/&#34; target=&#34;_blank&#34;&gt;Deep Learning for Action Recognition: A Review&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/pytorch-step-by-step-implementation-3d-convolution-neural-network-8bf38c70e8b3&#34; target=&#34;_blank&#34;&gt;3D Convolution in Pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1412.0767.pdf&#34; target=&#34;_blank&#34;&gt;Learning Spatiotemporal Features with 3D Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1705.07750&#34; target=&#34;_blank&#34;&gt;Quo vadis, action recognition? a new model and the kinetics dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dbs.ifi.lmu.de/~yu_k/icml2010_3dcnn.pdf&#34; target=&#34;_blank&#34;&gt;3d convolutional neural networks for human action recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://static.googleusercontent.com/media/research.google.com/it//pubs/archive/42455.pdf&#34; target=&#34;_blank&#34;&gt;Large-Scale Video Classification with Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1711.10305&#34; target=&#34;_blank&#34;&gt;Learning spatio-temporal representation with pseudo-3d residual networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1712.04851.pdf&#34; target=&#34;_blank&#34;&gt;Rethinking Spatiotemporal Feature Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://papers.nips.cc/paper/6433-spatiotemporal-residual-networks-for-video-action-recognition.pdf&#34; target=&#34;_blank&#34;&gt;Spatiotemporal residual networks for video action recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1812.05038&#34; target=&#34;_blank&#34;&gt;Long-Term Feature Banks for Detailed Video Understanding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1705.06950&#34; target=&#34;_blank&#34;&gt;The Kinetics Human Action Recognition Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1411.4389&#34; target=&#34;_blank&#34;&gt;Long-term Recurrent Convolutional Networks for Visual Recognition and Description&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1511.04119.pdf&#34; target=&#34;_blank&#34;&gt;Action Recognition Using Visual Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;Recurrent Tubelet Proposal and Recognition Networks for Action Detection&#34; target=&#34;_blank&#34;&gt;Recurrent Tubelet Proposal and Recognition Networks for Action Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1607.01794.pdf&#34; target=&#34;_blank&#34;&gt;VideoLSTM Convolves, Attends and Flows for Action Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1503.08909.pdf&#34; target=&#34;_blank&#34;&gt;Beyond Short Snippets: Deep Networks for Video Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pdfs.semanticscholar.org/51fe/a461cf3724123c888cb9184474e176c12e61.pdf&#34; target=&#34;_blank&#34;&gt;An Iterative Image Registration Technique with an Application to Stero Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf&#34; target=&#34;_blank&#34;&gt;Two-Frame Motion Estimation with Polynomial Expansion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nanonets.com/blog/optical-flow/&#34; target=&#34;_blank&#34;&gt;Introduction to Motion Estimation with Optical Flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.toronto.edu/~fleet/research/Papers/flowChapter05.pdf&#34; target=&#34;_blank&#34;&gt;Optical Flow Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1504.06852&#34; target=&#34;_blank&#34;&gt;FlowNet: Learning Optical Flow with Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1406.2199.pdf&#34; target=&#34;_blank&#34;&gt;Two-Stream Convolutional Networks for Action Recognition in Videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;•https://medium.com/swlh/what-is-optical-flow-and-why-does-it-matter-in-deep-learning-b3278bb205b5&#34; target=&#34;_blank&#34;&gt;What is optical flow and why does it matter in deep learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=tIwpDuqJqcE&#34; target=&#34;_blank&#34;&gt;Michael Black&amp;rsquo;s lecture on Optical Flow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Modelli Generativi</title>
      <link>https://gmanco.github.io/courses/computervision/lecture10/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture10/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;Modelli Generativi: Variational Autoencoders.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../pdf/11.Generative_models_VAE.pdf&#34;&gt;Slides&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/11.Generative_models.ipynb&#34; target=&#34;_blank&#34;&gt;Notebook&lt;/a&gt; di accompagnamento.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1312.6114&#34; target=&#34;_blank&#34;&gt;Autoencoding Variational Bayes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1401.4082&#34; target=&#34;_blank&#34;&gt;Stochastic Backpropagation and Approximate Inference in Deep Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jaan.io/what-is-variational-autoencoder-vae-tutorial/&#34; target=&#34;_blank&#34;&gt;What is a Variational Autoencoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://papers.nips.cc/paper/6528-variational-autoencoder-for-deep-learning-of-images-labels-and-captions.pdf&#34; target=&#34;_blank&#34;&gt;Variational Autoencoder for Deep Learning of Images, Labels and Captions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1911.07389.pdf&#34; target=&#34;_blank&#34;&gt;Towards Visually Explaining Variational Autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73&#34; target=&#34;_blank&#34;&gt;Understanding VAEs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1606.04934&#34; target=&#34;_blank&#34;&gt;Improved Variational Inference with Inverse Autoregressive Flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gmanco.github.io/post/on-semisupervised-vae/&#34; target=&#34;_blank&#34;&gt;Semi-supervised Variational Autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://deepgenerativemodels.github.io/&#34; target=&#34;_blank&#34;&gt;Deep Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openai.com/blog/generative-models/&#34; target=&#34;_blank&#34;&gt;Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openreview.net/pdf?id=Sy2fzU9gl&#34; target=&#34;_blank&#34;&gt;$\beta$-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Generative Adversarial Networks</title>
      <link>https://gmanco.github.io/courses/computervision/lecture11/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture11/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;Modelli Generativi: Adversarial Learning.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../pdf/12.Generative_models_GAN.pdf&#34;&gt;Slides&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/12.GAN.ipynb&#34; target=&#34;_blank&#34;&gt;Notebook&lt;/a&gt; di accompagnamento.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, Ch. 8, 9.3.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://deepgenerativemodels.github.io/&#34; target=&#34;_blank&#34;&gt;Deep Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1406.2661&#34; target=&#34;_blank&#34;&gt;Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://danieltakeshi.github.io/2017/03/05/understanding-generative-adversarial-networks/&#34; target=&#34;_blank&#34;&gt;Understanding Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1606.03498.pdf&#34; target=&#34;_blank&#34;&gt;Improved Techniques for training GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1511.05101&#34; target=&#34;_blank&#34;&gt;How (not) to train your Generative Model: Scheduled Sampling, Likelihood, Adversary?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1706.08500&#34; target=&#34;_blank&#34;&gt;GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.08224v2.pdf&#34; target=&#34;_blank&#34;&gt;Do GAN actually Learn the Distribution?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.10196.pdf&#34; target=&#34;_blank&#34;&gt;Progressive Growing of GANs for Improved Quality, Stability and Variation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1711.10337&#34; target=&#34;_blank&#34;&gt;Are GANs Created Equal? A Large-Scale Study&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1701.07875.pdf&#34; target=&#34;_blank&#34;&gt;Wasserstein GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wasserstein GANs: Blog posts &lt;a href=&#34;https://medium.com/@jonathan_hui/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://www.depthfirstlearning.com/2019/WassersteinGAN&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;, &lt;a href=&#34;https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html&#34; target=&#34;_blank&#34;&gt;3&lt;/a&gt;, &lt;a href=&#34;http://lernapparat.de/improved-wasserstein-gan/&#34; target=&#34;_blank&#34;&gt;4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans&#34; target=&#34;_blank&#34;&gt;Improved training fo Wasserstein GANs&lt;/a&gt; (with some &lt;a href=&#34;https://github.com/t-vi/pytorch-tvmisc/blob/master/wasserstein-distance/Semi-Improved_Training_of_Wasserstein_GAN.ipynb&#34; target=&#34;_blank&#34;&gt;tricks&lt;/a&gt; on how to implement it in Pytorch)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/znxlwm/pytorch-generative-model-collections&#34; target=&#34;_blank&#34;&gt;A collection of Generative Models in Pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/soumith/ganhacks&#34; target=&#34;_blank&#34;&gt;How to train a GAN?&lt;/a&gt; (&lt;a href=&#34;https://www.youtube.com/watch?v=Qc1F3-Rblbw&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1411.1784&#34; target=&#34;_blank&#34;&gt;Conditional Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.07004&#34; target=&#34;_blank&#34;&gt;Image to image translation with Conditional Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/a-gentle-introduction-to-cycle-consistent-adversarial-networks-6731c8424a87&#34; target=&#34;_blank&#34;&gt;A gentle introduction to CycleGAN for Image Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1703.10593&#34; target=&#34;_blank&#34;&gt;Unpaired Image-to-image Translation using Cycle-Consistent Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1807.00374.pdf&#34; target=&#34;_blank&#34;&gt;Augmented Cyclic Adversarial Learning for Low Resource Domain Adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hardikbansal.github.io/CycleGANBlog/&#34; target=&#34;_blank&#34;&gt;Understanding and Implementing CycleGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1610.09585.pdf&#34; target=&#34;_blank&#34;&gt;Conditional Image Synthesis with Auxiliary Classifier GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1803.04469.pdf&#34; target=&#34;_blank&#34;&gt;An Introduction to Image Synthesis with Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1809.11096.pdf&#34; target=&#34;_blank&#34;&gt;Large-Scale GAN Training for High-Fidelity Natural Image Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1809.06222.pdf&#34; target=&#34;_blank&#34;&gt;GANs for Medical Image Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hindupuravinash/the-gan-zoo&#34; target=&#34;_blank&#34;&gt;The GAN Zoo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/c/deepfake-detection-challenge/discussion/121313&#34; target=&#34;_blank&#34;&gt;GAN and Deepfakes resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@jonathan_hui/how-deep-learning-fakes-videos-deepfakes-and-how-to-detect-it-c0b50fbf7cb9&#34; target=&#34;_blank&#34;&gt;How deep learning fakes videos and how to detect it&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/deepfakes-the-ugly-and-the-good-49115643d8dd&#34; target=&#34;_blank&#34;&gt;Deepfakes: The ugly, and the good&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://neurohive.io/en/news/deepfake-videos-gan-sythesizes-a-video-from-a-single-photo/&#34; target=&#34;_blank&#34;&gt;Deepfake Videos: GAN synthesizes a Video from a Single Photo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Neural Style Transfer &lt;a href=&#34;[https://towardsdatascience.com/neural-style-transfer-tutorial-part-1-f5cd3315fa7f&#34; target=&#34;_blank&#34;&gt;Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1508.06576.pdf&#34; target=&#34;_blank&#34;&gt;A Neural algorithm of Artistic Style&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3390462&#34; target=&#34;_blank&#34;&gt;A Deep Journey into Superresolution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>CycleGAN and DeepFake</title>
      <link>https://gmanco.github.io/courses/computervision/lecture_lab5/</link>
      <pubDate>Thu, 23 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture_lab5/</guid>
      <description>

&lt;p&gt;Principali argomenti trattati:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CycleGAN&lt;/li&gt;
&lt;li&gt;DeepFake&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La lezione è corredata di slide disponibili &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/labs_lecture/lab07&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&#34; target=&#34;_blank&#34;&gt;CycleGAN Pytorch implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/image-to-image-translation-using-cyclegan-model-d58cfff04755&#34; target=&#34;_blank&#34;&gt;CycleGAN intro&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jinfagang/faceswap_pytorch&#34; target=&#34;_blank&#34;&gt;DeepFake - face swap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/OValery16/swap-face&#34; target=&#34;_blank&#34;&gt;DeepFake&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Esonero 1</title>
      <link>https://gmanco.github.io/courses/computervision/esoneri/listoffeaturedescriptors/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/esoneri/listoffeaturedescriptors/</guid>
      <description>

&lt;h1 id=&#34;feature-descriptors&#34;&gt;Feature descriptors&lt;/h1&gt;

&lt;p&gt;In questo esonero è chiesto agli studenti di sperimentare con un feature descriptr tra quelli elencati in seguito. In particolare si chiede di:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Scegliere un feature descriptor&lt;/li&gt;
&lt;li&gt;Preparare delle slides dettagliate (max 15) che lo descrivono&lt;/li&gt;
&lt;li&gt;Preparare un notebook in cui:

&lt;ul&gt;
&lt;li&gt;si implementa l&amp;rsquo;algoritmo che sta alla slide 62 (istogramma basato sui feature descriptors)&lt;/li&gt;
&lt;li&gt;si applica la regressione logistica (da scikit-learn) sul set di immagini (MNIST)&lt;/li&gt;
&lt;li&gt;si valutano i risultati della classificazione confrontandoli con la regressione logistica applicata alla flattenizzazione raw dell&amp;rsquo;immagine (opportunamente preprocessata).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Gli studenti interessati ad effettuare l&amp;rsquo;esonero dovranno mandare una mail al docente indicando, in ordine di priorità, tre scelte dalla lista di cui sotto.
Le assegnazioni verranno comunicate a lezione. La deadline per la consegna degli elaborati è il &lt;strong&gt;16 aprile 2020&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;opencv&#34;&gt;OpenCV&lt;/h2&gt;

&lt;p&gt;Tutorials disponibili &lt;a href=&#34;https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_table_of_contents_feature2d/py_table_of_contents_feature2d.html&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/4.2.0/d5/d3c/classcv_1_1xfeatures2d_1_1SIFT.html&#34; target=&#34;_blank&#34;&gt;SIFT&lt;/a&gt;, Class for extracting keypoints and computing descriptors using the Scale Invariant Feature Transform (&lt;a href=&#34;https://docs.opencv.org/4.2.0/d5/d3c/classcv_1_1xfeatures2d_1_1SIFT.html&#34; target=&#34;_blank&#34;&gt;SIFT&lt;/a&gt;) algorithm by D. Lowe. Riferimenti: David G Lowe. &lt;a href=&#34;https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&#34; target=&#34;_blank&#34;&gt;Distinctive image features from scale-invariant keypoints&lt;/a&gt;. &lt;em&gt;International journal of computer vision&lt;/em&gt;, 60(2):91–110, 2004.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/4.2.0/d5/df7/classcv_1_1xfeatures2d_1_1SURF.html&#34; target=&#34;_blank&#34;&gt;SURF&lt;/a&gt;, Class for extracting Speeded Up Robust Features from an image. Riferimenti: Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. &lt;a href=&#34;http://scholar.google.it/scholar_url?url=https://lirias.kuleuven.be/retrieve/78517&amp;amp;hl=it&amp;amp;sa=X&amp;amp;scisig=AAGBfm2HNoDAKFbI70IQ22ndV5cBLu7pBw&amp;amp;nossl=1&amp;amp;oi=scholarr&#34; target=&#34;_blank&#34;&gt;Surf: Speeded up robust features&lt;/a&gt;. &lt;em&gt;Computer Vision–ECCV 2006&lt;/em&gt;, pages 404–417, 2006.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/4.2.0/d8/d30/classcv_1_1AKAZE.html&#34; target=&#34;_blank&#34;&gt;AKAZE&lt;/a&gt;, Class implementing the &lt;a href=&#34;https://docs.opencv.org/4.2.0/d8/d30/classcv_1_1AKAZE.html&#34; target=&#34;_blank&#34;&gt;AKAZE&lt;/a&gt; keypoint detector and descriptor extractor. Riferimenti: Pablo F Alcantarilla, Jesús Nuevo, and Adrien Bartoli. &lt;a href=&#34;http://www.bmva.org/bmvc/2013/Papers/paper0013/paper0013.pdf&#34; target=&#34;_blank&#34;&gt;Fast explicit diffusion for accelerated features in nonlinear scale spaces&lt;/a&gt;. &lt;em&gt;Trans. Pattern Anal. Machine Intell&lt;/em&gt;, 34(7):1281–1298, 2011.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/4.2.0/de/dbf/classcv_1_1BRISK.html&#34; target=&#34;_blank&#34;&gt;Brisk&lt;/a&gt;, Class implementing the &lt;a href=&#34;https://docs.opencv.org/4.2.0/de/dbf/classcv_1_1BRISK.html&#34; target=&#34;_blank&#34;&gt;BRISK&lt;/a&gt; keypoint detector and descriptor extractor. [Stefan Leutenegger, Margarita Chli, and Roland Yves Siegwart. &lt;a href=&#34;https://www.researchgate.net/publication/221110715_BRISK_Binary_Robust_invariant_scalable_keypoints&#34; target=&#34;_blank&#34;&gt;Brisk: Binary robust invariant scalable keypoints&lt;/a&gt;. In &lt;em&gt;Computer Vision (ICCV), 2011 IEEE International Conference on&lt;/em&gt;, pages 2548–2555. IEEE, 2011.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/4.2.0/d3/d28/classcv_1_1MSER.html&#34; target=&#34;_blank&#34;&gt;Mser&lt;/a&gt;, Maximally stable extremal region extractor, for grey scale and color image. Riferimenti: David Nistér and Henrik Stewénius. &lt;a href=&#34;https://www.researchgate.net/publication/221304597_Linear_Time_Maximally_Stable_Extremal_Regions&#34; target=&#34;_blank&#34;&gt;Linear time maximally stable extremal regions&lt;/a&gt;. In &lt;em&gt;Computer Vision–ECCV 2008&lt;/em&gt;, pages 183–196. Springer, 2008.; Per-Erik Forssén. Maximally stable colour regions for recognition and matching. In &lt;em&gt;Computer Vision and Pattern Recognition, 2007. CVPR&amp;rsquo;07. IEEE Conference on&lt;/em&gt;, pages 1–8. IEEE, 2007.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/4.2.0/db/d95/classcv_1_1ORB.html&#34; target=&#34;_blank&#34;&gt;ORB&lt;/a&gt;, The algorithm uses FAST in pyramids to detect stable keypoints, selects  the strongest features using FAST or Harris response, finds their  orientation using first-order moments and computes the descriptors using BRIEF (where the coordinates of random point pairs (or k-tuples) are  rotated according to the measured orientation). Riferimenti: Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski. &lt;a href=&#34;http://www.willowgarage.com/sites/default/files/orb_final.pdf&#34; target=&#34;_blank&#34;&gt;Orb: an efficient alternative to sift or surf&lt;/a&gt;. In &lt;em&gt;Computer Vision (ICCV), 2011 IEEE International Conference on&lt;/em&gt;, pages 2564–2571. IEEE, 2011.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;sk-image&#34;&gt;SK-Image&lt;/h2&gt;

&lt;p&gt;Documentation at the home page of the &lt;a href=&#34;https://scikit-image.org/docs/0.16.x/api/skimage.feature.html&#34; target=&#34;_blank&#34;&gt;scikit-image feature description package&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;skimage.feature.daisy&lt;/code&gt;, Extract DAISY feature descriptors densely for the given image. DAISY is a feature descriptor similar to SIFT formulated in a way that allows for fast dense extraction. Typically, this is practical for bag-of-features image representations. Riferimenti: Tola et al. &lt;a href=&#34;https://infoscience.epfl.ch/record/138785/files/tola_daisy_pami_1.pdf&#34; target=&#34;_blank&#34;&gt;Daisy: An efficient dense descriptor applied to wide- baseline stereo&lt;/a&gt;. Pattern Analysis and Machine Intelligence, IEEE Transactions on 32.5 (2010): 815-830.]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;skimage.feature.hog&lt;/code&gt;, Extract Histogram of Oriented Gradients (HOG) for a given image. Riferimenti: Dalal, N and Triggs, B, &lt;a href=&#34;https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf&#34; target=&#34;_blank&#34;&gt;Histograms of Oriented Gradients for Human Detection&lt;/a&gt;, IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2005 San Diego, CA, USA, DOI:10.1109/CVPR.2005.177]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;skimage.feature.local_binary_pattern&lt;/code&gt;, Gray scale and rotation invariant LBP (Local Binary Patterns). Riferimenti: Timo Ojala, Matti Pietikainen, Topi Maenpaa. &lt;a href=&#34;http://www.ee.oulu.fi/research/mvmp/mvg/files/pdf/pdf_94.pdf&#34; target=&#34;_blank&#34;&gt;Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns&lt;/a&gt;. 2002]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;skimage.feature.haar_like_feature&lt;/code&gt;, Compute the Haar-like features for a region of interest (ROI) of an integral image. Riferimenti: Messom, Christopher H. and Andre L. C. Barczak. &lt;a href=&#34;https://www.semanticscholar.org/paper/Stream-processing-for-fast-and-efficient-rotated-Messom-Barczak/b55e215acd9bc0496f1f611d6193fea0b10e4212&#34; target=&#34;_blank&#34;&gt;Stream processing for fast and efficient rotated Haar-like features using rotated integral images&lt;/a&gt;. &lt;em&gt;IJISTA&lt;/em&gt; 7 (2006): 40-57.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;skimage.feature.BRIEF&lt;/code&gt;, BRIEF binary descriptor extractor. BRIEF (Binary Robust Independent Elementary Features) is an efficient feature point descriptor. It is highly discriminative even when using relatively few bits and is computed using simple intensity difference tests. Riferimenti: Calonder, Michael &amp;amp; Lepetit, Vincent &amp;amp; Strecha, Christoph &amp;amp; Fua, Pascal. (2010). &lt;a href=&#34;https://www.cs.ubc.ca/~lowe/525/papers/calonder_eccv10.pdf&#34; target=&#34;_blank&#34;&gt;BRIEF: Binary Robust Independent Elementary Features&lt;/a&gt;. Eur. Conf. Comput. Vis.. 6314. 778-792. 10.&lt;sup&gt;1007&lt;/sup&gt;&amp;frasl;&lt;sub&gt;978&lt;/sub&gt;-3-642-15561-1_56.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;skimage.feature.ORB&lt;/code&gt;, Oriented FAST and rotated BRIEF feature detector and binary descriptor extractor. Riferimenti: Ethan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski. &lt;a href=&#34;http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf&#34; target=&#34;_blank&#34;&gt;ORB: An efficient alternative to SIFT and SURF&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;assegnazioni&#34;&gt;Assegnazioni&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Giulia Katia Galimberti  &lt;strong&gt;SIFT&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Maria Francesca Alati  &lt;strong&gt;skimage.feature.hog&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Lorenzo Defina  &lt;strong&gt;Mser&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Emilio Casella  &lt;strong&gt;Surf&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Simona Nisticò matricola &lt;strong&gt;ORB&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Domenico Montesanto &lt;strong&gt;AKAZE&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Caterina Maugeri &lt;strong&gt;skimage.feature.local_binary_pattern&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Giuseppe Surace  &lt;strong&gt;skimage.feature.haar_like_feature&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Antonello Crea &lt;strong&gt;Daisy&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Anile Anna  &lt;strong&gt;skimage.feature.BRIEF&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Vincenzo Parrilla  &lt;strong&gt;SURF with Harris Corner Detection&lt;/strong&gt;. (OpenCV)&lt;/li&gt;
&lt;li&gt;Davide Medaglia &lt;strong&gt;SIFT with Harris Corner Detection&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Antonio Commisso &lt;strong&gt;Brisk&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Antonio Gagliostro: &lt;strong&gt;skimage.feature.ORB&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;risultati&#34;&gt;Risultati&lt;/h1&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Studente&lt;/th&gt;
&lt;th&gt;Voto&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Alati Maria  Francesca&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Anile Anna&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Casella Emilio&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Commisso Antonio&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Crea Antonello&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Defina Lorenzo&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Gagliostro Antonio&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Galimberti Giulia&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Maugeri Caterina&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Medaglia Davide&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Montesanto Domenico&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Nisticò Simona&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Parrilla Vincenzo&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Surace Giuseppe&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Esonero 2</title>
      <link>https://gmanco.github.io/courses/computervision/esoneri/convolution/</link>
      <pubDate>Tue, 28 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/esoneri/convolution/</guid>
      <description>

&lt;h1 id=&#34;convolutional-neural-networks&#34;&gt;Convolutional Neural Networks&lt;/h1&gt;

&lt;p&gt;In questo esonero è chiesto agli studenti di studiare le reti convoluzionali.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Scegliere un topic&lt;/li&gt;
&lt;li&gt;Preparare delle slides dettagliate (max 15) che lo descrivono&lt;/li&gt;
&lt;li&gt;Preparare un notebook in cui:

&lt;ul&gt;
&lt;li&gt;si implementa quello che è richiesto&lt;/li&gt;
&lt;li&gt;si illustrano i  risultati e la loro valutazione&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Gli studenti interessati ad effettuare l&amp;rsquo;esonero dovranno mandare una mail al docente indicando, in ordine di priorità, tre scelte dalla lista di cui sotto.
Le assegnazioni verranno comunicate a lezione. La deadline per la consegna degli elaborati è il &lt;strong&gt;10 maggio 2020&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;googlenet&#34;&gt;GoogLeNet&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Scegliere una delle seguenti varianti di GoogLeNet e implementarla, riaddestrando la rete su CIFAR-10 e mostrando i risultati.

&lt;ul&gt;
&lt;li&gt;Utilizzare i layer di batch normalization [&lt;a href=&#34;https://d2l.ai/chapter_references/zreferences.html#ioffe-szegedy-2015&#34; target=&#34;_blank&#34;&gt;Ioffe &amp;amp; Szegedy, 2015]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Effettuare gli aggiustamenti all&amp;rsquo;inception block suggeriti in [&lt;a href=&#34;https://d2l.ai/chapter_references/zreferences.html#szegedy-vanhoucke-ioffe-ea-2016&#34; target=&#34;_blank&#34;&gt;Szegedy et al., 2016]&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;effettuare gli aggiustamenti alle residual connections suggerite in [&lt;a href=&#34;https://d2l.ai/chapter_references/zreferences.html#szegedy-ioffe-vanhoucke-ea-2017&#34; target=&#34;_blank&#34;&gt;Szegedy et al., 2017]&lt;/a&gt;,&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Qual&amp;rsquo;è la dimensione minima richiesta per l&amp;rsquo;immagine di input in GoogLeNet?&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;Ioffe, S., &amp;amp; Szegedy, C. (2015). Batch normalization: accelerating deep network training by reducing internal covariate shift. &lt;em&gt;arXiv preprint arXiv:1502.03167&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., &amp;amp; Wojna, Z. (2016). Rethinking the inception architecture for computer vision. &lt;em&gt;Proceedings of the IEEE conference on computer vision and pattern recognition&lt;/em&gt; (pp. 2818–2826).&lt;/li&gt;
&lt;li&gt;Szegedy, C., Ioffe, S., Vanhoucke, V., &amp;amp; Alemi, A. A. (2017). Inception-v4, inception-resnet and the impact of residual connections on learning. &lt;em&gt;Thirty-First AAAI Conference on Artificial Intelligence&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;resnet&#34;&gt;ResNet&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Nelle versioni successive di ResNet, gli autori hanno cambiato l&amp;rsquo;architettura da “convolution, batch normalization, activation” in “batch normalization, activation, convolution”. Studia gli effetti di questo cambiamento nell&amp;rsquo;implementazione proposta in classe, ristrutturando la rete e riaddestrandola su CIFAR-10. [&lt;a href=&#34;https://arxiv.org/abs/1603.05027&#34; target=&#34;_blank&#34;&gt;He et al., 2016&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Qual&amp;rsquo;è la dimensione minima richiesta per l&amp;rsquo;immagine di input in ResNet ?&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;He, K., Zhang, X., Ren, S., &amp;amp; Sun, J. (2016). Identity mappings in deep residual networks. European conference on computer vision (pp. 630–645).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;saliency-maps-using-flashtorch&#34;&gt;Saliency Maps using Flashtorch&lt;/h2&gt;

&lt;p&gt;Studiare l&amp;rsquo;articolo [&lt;a href=&#34;https://arxiv.org/pdf/1312.6034.pdf&#34; target=&#34;_blank&#34;&gt;Simonyan et al, 2014&lt;/a&gt;] e il package &lt;a href=&#34;https://github.com/MisaOgura/flashtorch&#34; target=&#34;_blank&#34;&gt;flashtorch&lt;/a&gt; e usalo su alcune reti preaddestrate (VGG16, GoogLeNet, ResNet).&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Karen Simonyan, Andrea Vedaldi, Andrew Zisserman. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. ICLR2014&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/MisaOgura/flashtorch/blob/master/examples/visualize_saliency_with_backprop.ipynb&#34; target=&#34;_blank&#34;&gt;Visualize image specific class saliency with backprop&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;alternative-alla-batch-normalization&#34;&gt;Alternative alla Batch Normalization&lt;/h2&gt;

&lt;p&gt;Prendere come riferimento la rete VGG16. Si studino gli effetti della normalizzazione riaddestrando la rete su CIFAR-10:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Aggiungendo Batch Normalization&lt;/li&gt;
&lt;li&gt;Aggiungendo Local Response Normalization&lt;/li&gt;
&lt;li&gt;Implementando la tecnica di Group Normalization descritta in [&lt;a href=&#34;https://arxiv.org/pdf/1803.08494.pdf&#34; target=&#34;_blank&#34;&gt;Wu et al, 2018&lt;/a&gt;]&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;Y. Wu, K. He. Group Normalization.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/an-alternative-to-batch-normalization-2cee9051e8bc&#34; target=&#34;_blank&#34;&gt;An alternative to Batch Normalization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;assegnazioni&#34;&gt;Assegnazioni&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Costa Davide &lt;strong&gt;Alternative alla Batch Normalization&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Azzato Saverio &lt;strong&gt;GoogleLeNet (aggiustamenti all’inception block)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Sergi Alfredo &lt;strong&gt;GoogLeNet (aggiustamenti alle residual connection)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;De Prete Alessandra &lt;strong&gt;Saliency Maps using Flashtorch&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Prospero Papaleo &lt;strong&gt;ResNet&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Lavecchia Umberto Mattia &lt;strong&gt;GoogLeNet (con studio sulla batch normalization)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;risultati&#34;&gt;Risultati&lt;/h1&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Studente&lt;/th&gt;
&lt;th&gt;Voto&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Azzato Saverio&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Costa Davide&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Del Prete Alessandra&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Lavecchia Mattia&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Papaleo Prospero&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Sergi Alfredo&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Progetto 1</title>
      <link>https://gmanco.github.io/courses/computervision/progetti/project1_spec/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/progetti/project1_spec/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;Specifiche&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Action Recognition in ambito sportivo&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Task: individuare 10 tipologie di azioni di basket&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Costruire un modello di classificazione&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Il dataset è composto da 32557 clip (MP4, 16 frame RGB) di azioni di basket. Il dataset è suddiviso in train (22779) e test (9778)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Il test set è utilizzato per la valutazione, le clip (DA NON UTILIZZARE NEL TRAINING) sono definite in un file allegato a queste slide.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tipologia di azioni possibili (10 classi)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Download: &lt;a href=&#34;https://drive.google.com/open?id=1hLpbLmLFK2-GIvsmpJelGlEx94yQM2Ts&#34; target=&#34;_blank&#34;&gt;https://drive.google.com/open?id=1hLpbLmLFK2-GIvsmpJelGlEx94yQM2Ts&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;requisiti&#34;&gt;Requisiti&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Definire e addestrare un modello di classificazione&lt;/li&gt;
&lt;li&gt;Descrivere in una relazione/presentazione le scelte progettuali e tutti i parametri utilizzati nella sperimentazione.&lt;/li&gt;
&lt;li&gt;Consegnare notebook (e/o file sorgente), relazione e dump del modello&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;protocollo-di-valutazione&#34;&gt;Protocollo di valutazione&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Valutazione generale del progetto&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Loading del modello e calcolo delle precisione media sul test set&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Baseline-1 - 8 pt – Valore 0.40 Baseline-2 - 7 pt – Valore 0.60&lt;/p&gt;

&lt;p&gt;Per accedere all’orale sono necessari almeno 8 pt.&lt;/p&gt;

&lt;h3 id=&#34;assegnazione-dei-punteggi&#34;&gt;Assegnazione dei punteggi:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Se il modello supera la Baseline-1: 8 pt&lt;/li&gt;
&lt;li&gt;Se il modello non supera la Baseline-2: (p – Baseline-1)/(Baseline-2 - Baseline-1) * 7 pt&lt;/li&gt;
&lt;li&gt;Se il modello supera la Baseline-2: 7 pt + bonus&lt;/li&gt;
&lt;li&gt;Bonus (p – Baseline-2)/(BESTMODEL - Baseline-2) * 5 pt&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;appelli&#34;&gt;Appelli&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1 – luglio – Progetto attuale&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Consegna entro domenica 28 giugno&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;31 – luglio – Nuovo Progetto (stesse modalità)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rilascio indicativo fine giugno&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Per ulteriori info vedere &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/tree/master/project_1&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adversarial Games for generative modeling of Temporally-Marked Event Sequences</title>
      <link>https://gmanco.github.io/talk/isi-feb20/</link>
      <pubDate>Tue, 18 Feb 2020 11:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/talk/isi-feb20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Some notes on the Semi-Supervised Learning of Variational Autoencoders</title>
      <link>https://gmanco.github.io/post/on-semisupervised-vae/</link>
      <pubDate>Tue, 10 Sep 2019 18:10:09 +0200</pubDate>
      <guid>https://gmanco.github.io/post/on-semisupervised-vae/</guid>
      <description>&lt;p&gt;In what follows I&#39;ll try to explain my basic understanding and interepretation of the semi-supervised framework based on Variational Autoencoders, as described in &lt;a href=&#34;#ref&#34;&gt;[1]&lt;/a&gt;. I shall assume a vector notation where bold symbols $\mathbf{a}$ represent vectors, whose $j$-th component can be represented as $a_j$.&lt;/p&gt;

&lt;p&gt;The starting point of the framework is to consider a dataset &lt;span  class=&#34;math&#34;&gt;\(D = S \cup U\)&lt;/span&gt;, where:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(S = \{(\mathbf{x}_1, \mathbf{y}_1), \ldots, (\mathbf{x}_n, \mathbf{y}_n)\}\)&lt;/span&gt;,&lt;br&gt;&lt;/li&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(U = \{\mathbf{x}_{n+1}, \ldots, \mathbf{x}_{n+m}\}\)&lt;/span&gt;,&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;with &lt;span  class=&#34;math&#34;&gt;\(\mathbf{x}_i \in \mathbb{R}^N\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(\mathbf{y}_i \in \{0,1\}^C\)&lt;/span&gt; represents a one-hot encoding of a class in &lt;span  class=&#34;math&#34;&gt;\(\{1, \ldots, C\}\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;The basic assumption of variational autoencoders is that data is generated according to a density function &lt;span  class=&#34;math&#34;&gt;\(p_\theta(\mathbf{x}| \mathbf{z})\)&lt;/span&gt;, where  &lt;span  class=&#34;math&#34;&gt;\(\mathbf{z}\in \mathbb{R}^K\)&lt;/span&gt; is a latent variables governing the distribution of &lt;span  class=&#34;math&#34;&gt;\(\mathbf{x}\)&lt;/span&gt;. &lt;span  class=&#34;math&#34;&gt;\(\theta\)&lt;/span&gt; represents a model parameter. The above density function can be modeled through a Neural network: thus &lt;span  class=&#34;math&#34;&gt;\(\theta\)&lt;/span&gt; represents all the network weights. An example PyTorch snippet, where the density is be modeled as a softmax over a simple linear layer, is illustrated below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
import torch.nn as nn

class Decoder(nn.Module):
    def __init__(self, latent_size,num_classes,out_size):
        super(Decoder, self).__init__()

        self.linear_layer = nn.Linear(latent_size + num_classes, out_size)
        nn.init.xavier_normal_(self.layer.weight)

        self.activation = nn.Sigmoid(dim=-1)

    def forward(self, z):
      	return self.activation(self.linear_layer(z))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, we are assuming $\mathbf{x}$ binary and consequently the reconstruction exploits a bernoullian distribution for each feature.&lt;/p&gt;

&lt;p&gt;Let&#39;s see how the generative framework can model the likelihood of the data and help us develop a semi-supervised classifier.&lt;/p&gt;

&lt;h2 id=&#34;unsupervised-examples&#34;&gt;Unsupervised examples&lt;/h2&gt;

&lt;p&gt;First, let us consider &lt;span  class=&#34;math&#34;&gt;\(\mathbf{x}\in U\)&lt;/span&gt;. When &lt;span  class=&#34;math&#34;&gt;\(\mathbf{y}\)&lt;/span&gt; is unknown, we can consider it as a latent variable as well. Both &lt;span  class=&#34;math&#34;&gt;\(\mathbf{y}\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(\mathbf{z}\)&lt;/span&gt; assume a prior distribution, given by&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{split}
\mathbf{z} \sim &amp; \mathcal{N}(\mathbf{0},I_K)\\
\mathbf{y} \sim &amp; \mathit{Cat}(\boldsymbol\pi)
\end{split}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Here, &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol\pi\)&lt;/span&gt; is a prior multinomial distribution over &lt;span  class=&#34;math&#34;&gt;\(\{1, \ldots, C\}\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;In such a case, we can extend the generative setting where data samples $\mathbf{x}$ are assumed to be generated by the conditional &lt;span  class=&#34;math&#34;&gt;\(p_\theta(\mathbf{x}| \mathbf{z},\mathbf{y})\)&lt;/span&gt; through the relationship&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{equation}\label{average}\tag{1}
p(\mathbf{x}) = \int p_\theta(\mathbf{x}| \mathbf{z}, \mathbf{y}) p(\mathbf{z})p(\mathbf{y}) \mathrm{d} \mathbf{z} \mathrm{d} \mathbf{y}
\end{equation}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In principle, the &lt;span  class=&#34;math&#34;&gt;\(\theta\)&lt;/span&gt; parameter can be chosen to maximize the evidence on &lt;span  class=&#34;math&#34;&gt;\(D\)&lt;/span&gt;, i.e. by optimizing &lt;span  class=&#34;math&#34;&gt;\(\log p(\mathbf{x})\)&lt;/span&gt;. However, this approach is not feasible because it requires averaging over all possible &lt;span  class=&#34;math&#34;&gt;\(\mathbf{z}\)&lt;/span&gt; pairs. We can approximate the likelihood by sampling a subset of $\mathbf{z}$ latent data points and then averaging over them. Again, this workaround exhibits the drawback that pure random sampling is exposed to high variance. The idea of Variational Autoencoders is to &amp;quot;guide&amp;quot; the sampling by exploiting the evidence: instead of freely choosing $\mathbf{z},\mathbf{y}$, we build a sampler &lt;span  class=&#34;math&#34;&gt;\(q_\phi(\mathbf{z},\mathbf{y}|\mathbf{x})\)&lt;/span&gt; that tunes the probability of &lt;span  class=&#34;math&#34;&gt;\(\mathbf{z},\mathbf{y}\)&lt;/span&gt; according to $\mathbf{x}$. In practice, $q_\phi$ &lt;em&gt;encodes&lt;/em&gt; the information regarding &lt;span  class=&#34;math&#34;&gt;\(\mathbf{x}\)&lt;/span&gt; into the most probable &lt;span  class=&#34;math&#34;&gt;\(\mathbf{z},\mathbf{y}\)&lt;/span&gt; latent variables.&lt;/p&gt;

&lt;p&gt;We can factorize the decoder  as &lt;span  class=&#34;math&#34;&gt;\(q_\phi(\mathbf{z},\mathbf{y}| \mathbf{x}) = q_\varphi(\mathbf{z}|\mathbf{x},\mathbf{y})q_\vartheta(\mathbf{y}|\mathbf{x})\)&lt;/span&gt;, where&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{split}
q_\varphi(\mathbf{z}|\mathbf{x},\mathbf{y}) \equiv &amp; \mathcal{N}(\mathbf{z}| \boldsymbol\mu_\varphi(\mathbf{x},\mathbf{y}), \boldsymbol\sigma_\varphi(\mathbf{x},\mathbf{y})\cdot I_K)\\
q_\vartheta(\mathbf{y}|\mathbf{x}) \equiv &amp; \mathit{Cat}(\mathbf{y}|\boldsymbol\pi_\vartheta(\mathbf{x})), 
\end{split}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Here, the parameters &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol\mu_\varphi(\mathbf{x},\mathbf{y}), \boldsymbol\sigma_\varphi(\mathbf{x},\mathbf{y})\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol\pi_\vartheta(\mathbf{x})\)&lt;/span&gt; represent neural functions parameterized by $\varphi$ and $\vartheta$, respectively.
Again, a PyTorch snippet is given below, where the two encoders are exemplified. Concerning  &lt;span  class=&#34;math&#34;&gt;\(q_\varphi(\mathbf{z}|\mathbf{x}, \mathbf{y})\)&lt;/span&gt;, we have:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Encoder_z(nn.Module):
    def __init__(self, input_size,latent_size):
        super(Decoder, self).__init__()
				
        self.latent_size = latent_size
        
        self.linear_layer = nn.Linear(input_size, 2*latent_size)
        nn.init.xavier_normal_(self.linear_layer.weight)
        
    def _sample_latent(self, mu_q, logvar_q):
        var_q = torch.exp(logvar_q)
        epsilon = torch.randn(var_q.size(),requires_grad=False).to(var_q.device)

        return mu_q + epsilon*var_q
        
    def forward(self, x, y):
      	input = torch.cat([x,y],dim=-1)
        temp_out = self.linear_layer(input)
    
        mu_q = temp_out[:, :self.latent_size]
        logvar_q = temp_out[:, self.latent_size:]

        z = self._sample_latent(mu_q, logvar_q)

        return z, mu_q, logvar_q
​```
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The computation within this class is a variable &lt;span  class=&#34;math&#34;&gt;\(\mathbf{z}\sim q_\varphi(\cdot |\mathbf{x},\mathbf{y})\)&lt;/span&gt;, as well as the parameters of the variational (gaussian) distribution &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol\mu\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol\sigma\)&lt;/span&gt;.
Here, we are exploiting the reparameterization trick: given a variable &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol\epsilon \sim \mathcal{N}(\mathbf{0},I_K)\)&lt;/span&gt;, the transformation &lt;span  class=&#34;math&#34;&gt;\(z = \mu + \epsilon \cdot \sigma\)&lt;/span&gt; guarantees that &lt;span  class=&#34;math&#34;&gt;\(\mathbf{z}\sim \mathcal{N}(\boldsymbol\mu, \boldsymbol\sigma)\)&lt;/span&gt; and at the same time it preserves the backpropagation of the gradient, since &lt;span  class=&#34;math&#34;&gt;\(
\frac{\partial \mathbf{z}}{\partial w} = \frac{\partial \boldsymbol\mu}{\partial w} + \boldsymbol\epsilon \cdot \frac{\partial \boldsymbol\sigma}{\partial w}
\)&lt;/span&gt; and both &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol\mu\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol\sigma\)&lt;/span&gt; are deterministically computed as shown above.&lt;/p&gt;

&lt;p&gt;Similarly, &lt;span  class=&#34;math&#34;&gt;\(q_\vartheta(\mathbf{y}|\mathbf{x})\)&lt;/span&gt; is exemplified by the following snippet:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Classifier(nn.Module):
    def __init__(self, input_size,num_classes):
        super(Decoder, self).__init__()

        self.linear_layer = nn.Linear(input_size, 2*latent_size)
        nn.init.xavier_normal_(self.layer.weight)
      
        self.softmax = nn.Softmax(dim=-1)
        
    def forward(self, x):
        return self.softmax(self.linear_layer(x))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice that, differently from the previous case, within this class we directly output a probability distribution &lt;span  class=&#34;math&#34;&gt;\(q_\vartheta(\mathbf{y}|\mathbf{x})\)&lt;/span&gt;, rather than a sample &lt;span  class=&#34;math&#34;&gt;\(\mathbf{y} \sim q_\vartheta(\cdot|\mathbf{x})\)&lt;/span&gt;. The reason for this choice is that no reparameterization trick is possible for a discrete distribution that preserves backpropagation. However, this does not prevent us from  averaging over all possible samples, as we shall see later.&lt;/p&gt;

&lt;p&gt;What is the relationship between the encoders and the decoder? We can observe that&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(
\begin{split}
\log p(\mathbf{x}) \geq &amp; \mathbb{E}_{q_\phi(\mathbf{z},\mathbf{y}| \mathbf{x})}\left[\log p_\theta(\mathbf{x}| \mathbf{z}) + \log p(\mathbf{z}) + \log p(\mathbf{y}) - \log q_\phi(\mathbf{z},\mathbf{y}| \mathbf{x})\right] \\
= &amp; \sum_\mathbf{y} \mathbb{E}_{q_\varphi(\mathbf{z}| \mathbf{x},\mathbf{y})}\Bigg[  q_\vartheta(\mathbf{y}| \mathbf{x})\bigg(\log p_\theta(\mathbf{x}| \mathbf{z}) + \log p(\mathbf{y}) - \log q_\vartheta(\mathbf{y}| \mathbf{x})\bigg) \\
&amp;  \qquad \qquad + \log p(\mathbf{z}) - \log q_\varphi(\mathbf{z}| \mathbf{x},\mathbf{y})\Bigg] 
\end{split}
\)&lt;/span&gt;
We call the right-hand side of the equation the &lt;em&gt;Evidence Lower Bound&lt;/em&gt; (&lt;em&gt;ELBO&lt;/em&gt;). It turns out that, optimizing this equation with respect to &lt;span  class=&#34;math&#34;&gt;\(\phi, \theta\)&lt;/span&gt; corresponds to optimizing &lt;span  class=&#34;math&#34;&gt;\(\log p(\mathbf{x})\)&lt;/span&gt; as well. Thus, we can specify the loss &lt;span  class=&#34;math&#34;&gt;\(\ell(\mathit{x})\)&lt;/span&gt; as the negative of the &lt;em&gt;ELBO&lt;/em&gt; and exploit a gradient-based optimization strategy. The main difference with respect to directly optimizing eq. (&lt;span  class=&#34;math&#34;&gt;\(\ref{average}\)&lt;/span&gt;) is that the &lt;em&gt;ELBO&lt;/em&gt; is tractable. In fact, we can rewrite it as&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(\begin{split}
\ell(\mathbf{x})= &amp; - \sum_\mathbf{y} \mathbb{E}_{\boldsymbol\epsilon\sim \mathcal{N}(\mathbf{0},I_K)}\Bigg[ q_\vartheta(\mathbf{y}| \mathbf{x})\bigg(\log p_\theta(\mathbf{x}| \mathbf{z}(\boldsymbol\epsilon, \mathbf{x},\mathbf{y})) + \log p(\mathbf{y}) - \log q_\vartheta(\mathbf{y}| \mathbf{x})\bigg) \\
&amp;  \qquad \qquad + \log p(\mathbf{z}(\boldsymbol\epsilon, \mathbf{x},\mathbf{y})) - \log q_\varphi(\mathbf{z}(\boldsymbol\epsilon, \mathbf{x},\mathbf{y})| \mathbf{x},\mathbf{y})\Bigg] 
\end{split}\)&lt;/span&gt;
where &lt;span  class=&#34;math&#34;&gt;\(\mathbf{z}(\boldsymbol\epsilon, \mathbf{x},\mathbf{y}) = \boldsymbol\mu_\varphi(\mathbf{x},\mathbf{y}) + \boldsymbol\epsilon \cdot \sigma_\varphi(\mathbf{x},\mathbf{y})\)&lt;/span&gt; represents the &lt;span  class=&#34;math&#34;&gt;\(\mathbf{z}\)&lt;/span&gt; component in the output of  &lt;code&gt;Decoder_z&lt;/code&gt; and
&lt;span  class=&#34;math&#34;&gt;\(q_{\vartheta}(\mathbf{y}| \mathbf{x})\)&lt;/span&gt; represents the output of &lt;code&gt;Classifier&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;By analysing the above equation we can observe the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Since &lt;span  class=&#34;math&#34;&gt;\(q_\vartheta(\mathbf{y}| \mathbf{x}) = \prod_{j=1}^C \pi_{\vartheta,j}(\mathbf{x})^{y_j}\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(\mathbf{y}\)&lt;/span&gt; ranges over all possible classes, we can simplify the first part of the right-hand side of the equation with &lt;span  class=&#34;math&#34;&gt;\(\sum_{j = 1}^C \pi_{\vartheta,j}(\mathbf{x})\bigg(p_\theta(\mathbf{x}| \mathbf{z}(\epsilon,\mathbf{x},\mathbf{y}), \mathbf{e}_j) + \log \pi_j - \log \pi_{\vartheta,j}(\mathbf{x})\bigg)\)&lt;/span&gt;,  where &lt;span  class=&#34;math&#34;&gt;\(\mathbf{e}_j\)&lt;/span&gt; is the vector of all zeros except for position &lt;span  class=&#34;math&#34;&gt;\(j\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(\pi_{\vartheta,j}(\mathbf{x})\)&lt;/span&gt; is the &lt;span  class=&#34;math&#34;&gt;\(j\)&lt;/span&gt;-th component of &lt;span  class=&#34;math&#34;&gt;\(\pi_{\vartheta}(\mathbf{x})\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Further, by exploiting the definitions, &lt;span  class=&#34;math&#34;&gt;\(\mathbb{E}_{\boldsymbol\epsilon\sim \mathcal{N}(\mathbf{0},I_K)}\left[\log p(\mathbf{z}(\boldsymbol\epsilon, \mathbf{x},\mathbf{y})) - \log q_\varphi(\mathbf{z}(\boldsymbol\epsilon, \mathbf{x},\mathbf{y})| \mathbf{x},\mathbf{y})\right] = \sum_{k} \Bigg(\log \sigma_{\varphi,k}(\mathbf{x},\mathbf{y}) + 1 - \sigma_{\varphi,k}(\mathbf{x},\mathbf{y}) - \boldsymbol\mu_{\varphi,k}(\mathbf{x},\mathbf{y})^2\Bigg)\)&lt;/span&gt; and we see that the only source of nondeterminism is given by the component that computes the log-likelihood.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To summarize, the loss for an element &lt;span  class=&#34;math&#34;&gt;\(\mathbf{x} \in U\)&lt;/span&gt; can be fully specified as follows:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{split}
\ell(\mathbf{x})= &amp; \sum_{j = 1}^C \pi_{\vartheta,j}(\mathbf{x})\left(\log \pi_{\vartheta,j}(\mathbf{x}) - \log \pi_j \right) \\
&amp; - \mathbb{E}_{\boldsymbol\epsilon\sim \mathcal{N(\mathbf{0},I_K)}}\left[\sum_{j = 1}^C \pi_{\vartheta,j}(\mathbf{x})p_\theta(\mathbf{x}| \mathbf{z}(\boldsymbol\epsilon,\mathbf{x},\mathbf{e}_j), \mathbf{e}_j)\right]\\
&amp; - \sum_{j = 1}^C \sum_{k} \Bigg(\log \sigma_{\varphi,k}(\mathbf{x},\mathbf{e}_j) + 1 - \sigma_{\varphi,k}(\mathbf{x},\mathbf{e}_j) - \mu_{\varphi,k}(\mathbf{x},\mathbf{e}_j)^2\Bigg)
\end{split}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The code snippet illustrating &lt;span  class=&#34;math&#34;&gt;\(\ell(\mathbf{x})\)&lt;/span&gt; in PyTorch is the following.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def unsupervised_loss(x,encoder,decoder,classifier,num_classes,y_prior=1):
    y_q = classifier(x)
    kld_cat = torch.mean(torch.sum(y_q*(torch.exp(y_q) - torch.log(y_prior)),-1),-1)  
    
    kld_norm = 0
    e = torch.zeros(y_q.size()).to(x.device)
    
    prob_e = []
    for j in range(num_classes):
        e[:,j] = 1.
        z, mu_q, logvar_q = encoder(x,e)
        kld_norm += torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1)
        prob_e.append(decoder(z))
        e[:,j] = 0.
		
		kld_norm = torch.mean(kld_norm, -1)

    prob_e = torch.floatTensor(log_prob_e)
    prob_x = torch.matmul(llk_e,y_q).squeeze()
    
    loss = nn.BCELoss()
    llk = loss(prob_x,x)
    
    return llk + kld_cat + kld_norm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, since &lt;code&gt;Decoder&lt;/code&gt; provides a probability distribution, the loss is the negative log likelihood. Again, we are assuming $\mathbf{x}$ binary and the underlying probability is bernoullian on each feature.&lt;/p&gt;

&lt;h2 id=&#34;supervised-examples&#34;&gt;Supervised examples&lt;/h2&gt;

&lt;p&gt;The case &lt;span  class=&#34;math&#34;&gt;\((\mathbf{x},\mathbf{y})\in S\)&lt;/span&gt; rensembles the unsupervised case, but with a major difference. For the labelled case,  the joint probability &lt;span  class=&#34;math&#34;&gt;\(p(\mathbf{x},\mathbf{y},\mathbf{z})\)&lt;/span&gt; is decomposed as &lt;span  class=&#34;math&#34;&gt;\(p_\theta(\mathbf{x},\mathbf{y},\mathbf{z}) = q_{\vartheta}(\mathbf{y}|\mathbf{x})p_\theta(\mathbf{x}|\mathbf{z})p(\mathbf{z})\)&lt;/span&gt;. In practice, we consider here a discriminative setting where the &lt;code&gt;Classifier&lt;/code&gt; component as a part of the decoder. This is different from the unsupervised case, where &lt;span  class=&#34;math&#34;&gt;\(\mathbf{y}\)&lt;/span&gt; was considered a latent variable which encoded latent information from $\mathbf{x}$ in a generative setting.&lt;/p&gt;

&lt;p&gt;As a consequence, the joint likelihood can be approximated as&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{split}
\log p(\mathbf{x},\mathbf{y}) \geq &amp; \mathbb{E}_{q_\varphi(\mathbf{z}| \mathbf{x},\mathbf{y})}\Bigg[\log p_\theta(\mathbf{x}| \mathbf{z}) + \log q_\vartheta(\mathbf{y}|\mathbf{x}) + \log p(\mathbf{z}) - \log q_\varphi(\mathbf{z}| \mathbf{x},\mathbf{y})\Bigg] 
\end{split}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;By rearranging the formulas, we obtain the loss term for the supervised case:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{split}\ell(\mathbf{x},\mathbf{y}) = &amp; \mathbb{E}_{\boldsymbol\epsilon\sim \mathcal{N}(0,1)}\Bigg[\log p_\theta(\mathbf{x}| \mathbf{z}(\boldsymbol\epsilon,\mathbf{x}, \mathbf{y}))\Bigg] \\ &amp; + \sum_{j=1}^C y_j \log \pi_{\vartheta,j}(\mathbf{x}) \\&amp; + \sum_{k} \Bigg(\log \sigma_{\varphi,k}(\mathbf{x},\mathbf{y}) + 1 - \sigma_{\varphi,k}(\mathbf{x},\mathbf{y}) - \mu_{\varphi,k}(\mathbf{x},\mathbf{y})^2\Bigg)\end{split}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The corresponding example implementation:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def supervised_loss(x,y,encoder,decoder,classifier):    
    z, mu_q, logvar_q = encoder(x,y)
    kld_norm = torch.mean(torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1),-1)

    prob_x = decoder(x)     
    loss = nn.BCELoss()
    llk = loss(prob_x,x)
    
    y_q = classifier(x)
    loss = CrossEntropyLoss(dim=-1)
    llk_cat = loss(y_q,y)
                          
    return llk + llk_cat + kld_norm    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Other interpretations for the supervised case are possible: see, e.g. the treatment in  &lt;a href=&#34;http://bjlkeng.github.io/posts/semi-supervised-learning-with-variational-autoencoders/&#34;&gt;Brian Keng&#39;s blog&lt;/a&gt;. However, I feel that separating the supervised and the unsupervised case and arranging the derivations accordingly is more intuitive.&lt;/p&gt;

&lt;h2 id=&#34;wrapping-up&#34;&gt;Wrapping up&lt;/h2&gt;

&lt;p&gt;We can finally combine all the above and devise a model for semi-supervised training:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class SSVAE(nn.Module):
    def __init__(self,input_size,num_classes,latent_size, y_prior = 1):
        self.input_size = input_size
        self.num_classes = num_classes
        self.latent_size = latent_size
        
        self.y_prior = y_prior
        
        self.encoder = Encoder(input_size,num_classes,latent_size)
        self.decoder = Decoder(latent_size,input_size)
        self.classifier = Classifier(input_size, num_classes)
        
        llk_loss = nn.BCELoss()
        cat_loss = nn.CrossEntropyLoss()

    def unsupervised_loss(self, x):
        y_q = self.classifier(x)
        kld_cat = torch.mean(torch.sum(y_q*(torch.log(y_q) - torch.log(self.y_prior)),-1),-1)  

        kld_norm = 0
        e = torch.zeros(y_q.size()).to(x.device)
    
        prob_e = []
        for j in range(self.num_classes):
            e[:,j] = 1.
            z, mu_q, logvar_q = self.encoder(x,e)
            kld_norm += torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1)
            prob_e.append(self.decoder(z))
            e[:,j] = 0.
        
        kld_norm = torch.mean(kld_norm, -1))

        prob_e = torch.floatTensor(log_prob_e)
        prob_x = torch.matmul(llk_e,y_q).squeeze()
    
        llk = llk_loss(prob_x,x)
    
        return llk + kld_cat + kld_norm
        
    def supervised_loss(self,x,y):
        z, mu_q, logvar_q = self.encoder(x,y)
        kld_norm = torch.mean(torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1),-1)

        prob_x = self.decoder(x)                          
        llk = loss(prob_x,x)
    
        y_q = self.classifier(x)
        llk_cat = cat_loss(y_q,y)
                          
        return llk + llk_cat + kld_norm    
        
    def forward(self, x, y = None, train = True)
        if not train:
            return self.classifier(x)
        else:
            if y is not None:
                loss = self.supervised_loss(x,y)
            else
                loss = self.unsupervised_loss(x)
            return loss
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can observe that the model can be called in two modes: either in &lt;code&gt;train&lt;/code&gt; mode or not. For the latter, it acts as a classifier and produces the class probabilities. In the training mode, on the other side, it computes either the supervised or the unsupervised, loss based on whether the data is labelled or not. The training procedure is quite straightforward as it just requires a &lt;code&gt;data_loader&lt;/code&gt; capable of ranging over &lt;span  class=&#34;math&#34;&gt;\(D\)&lt;/span&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def train(data_loader,input_size, num_classes, latent_size, y_priors):
    model = SSVAE(input_size,num_classes, latent_size)
    
    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)
    
    for batch_idx,(x,y) in enumerate(data_loader):
        optimizer.zero_grad()
        
        loss = model(x,y)
        loss.backward()
        optimizer.step()
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;

&lt;p&gt;[1]  &lt;a name=&#34;ref&#34;&gt;&lt;/a&gt;Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, Max Welling. &lt;a href=&#34;https://arxiv.org/abs/1406.5298&#34;&gt;Semi-supervised Learning with Deep Generative Models&lt;/a&gt;. Advances in Neural Information Processing Systems 27 (&lt;strong&gt;NIPS 2014&lt;/strong&gt;), Montreal&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://gmanco.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/slides/example/</guid>
      <description>

&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;

&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Code block:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;

&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;

&lt;p&gt;Block math:&lt;/p&gt;

&lt;p&gt;$$
f\left( x \right) = \;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;

&lt;p&gt;Make content appear incrementally&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
   One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three
&lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;

&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;


&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;


&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;

&lt;p&gt;Customize the slide style and background&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;

&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Factorization Approach for Survival Analysis on Diffusion Networks</title>
      <link>https://gmanco.github.io/publication/manco-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/manco-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sequential Variational Autoencoders for Collaborative Filtering</title>
      <link>https://gmanco.github.io/publication/sachdeva-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/sachdeva-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Predicting Temporal Activation Patterns via Recurrent Neural Networks</title>
      <link>https://gmanco.github.io/publication/manco-2018/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/manco-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fault detection and explanation through big data analysis on sensor streams</title>
      <link>https://gmanco.github.io/publication/manco-2017/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/manco-2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Differential Privacy and Neural Networks: A Preliminary Analysis</title>
      <link>https://gmanco.github.io/publication/manco-2017-a/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/manco-2017-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Survival Factorization on Diffusion Networks</title>
      <link>https://gmanco.github.io/publication/barbieri-2017/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Efficient Methods for Influence-Based Network-Oblivious Community Detection</title>
      <link>https://gmanco.github.io/publication/barbieri-2016/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Rialto: A Knowledge Discovery suite for data analysis</title>
      <link>https://gmanco.github.io/publication/manco-2016/</link>
      <pubDate>Sat, 01 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/manco-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How Can SMEs Benefit from Big Data? Challenges and a Path Forward</title>
      <link>https://gmanco.github.io/publication/coleman-2016/</link>
      <pubDate>Sun, 01 May 2016 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/coleman-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Outlying property detection with numerical attributes</title>
      <link>https://gmanco.github.io/publication/angiulli-2016/</link>
      <pubDate>Tue, 01 Mar 2016 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/angiulli-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Probabilistic Approaches to Recommendations</title>
      <link>https://gmanco.github.io/publication/barbieri-2014/</link>
      <pubDate>Thu, 01 May 2014 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2014/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Generative Bayesian Model for Item and User Recommendation in Social Rating Networks with Trust Relationships</title>
      <link>https://gmanco.github.io/publication/costa-2014/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/costa-2014/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Who to follow and why</title>
      <link>https://gmanco.github.io/publication/barbieri-2014-a/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2014-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Influence-Based Network-Oblivious Community Detection</title>
      <link>https://gmanco.github.io/publication/barbieri-2013-a/</link>
      <pubDate>Sun, 01 Dec 2013 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2013-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dealing with trajectory streams by clustering and mathematical transforms</title>
      <link>https://gmanco.github.io/publication/costa-2013-a/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/costa-2013-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Probabilistic topic models for sequence data</title>
      <link>https://gmanco.github.io/publication/barbieri-2013-b/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2013-b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Topic-aware social influence propagation models</title>
      <link>https://gmanco.github.io/publication/barbieri-2013-c/</link>
      <pubDate>Mon, 01 Apr 2013 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2013-c/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hierarchical clustering of XML documents focused on structural components</title>
      <link>https://gmanco.github.io/publication/costa-2013/</link>
      <pubDate>Fri, 01 Mar 2013 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/costa-2013/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cascade-based community detection</title>
      <link>https://gmanco.github.io/publication/barbieri-2013/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2013/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Topic-Aware Social Influence Propagation Models</title>
      <link>https://gmanco.github.io/publication/barbieri-2012-a/</link>
      <pubDate>Sat, 01 Dec 2012 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2012-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Balancing Prediction and Recommendation Accuracy: Hierarchical Latent Factors for Preference Data</title>
      <link>https://gmanco.github.io/publication/barbieri-2012/</link>
      <pubDate>Sun, 01 Apr 2012 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2012/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Probabilistic Sequence Modeling for Recommender Systems</title>
      <link>https://gmanco.github.io/publication/barbieri-bcmr-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-bcmr-12/</guid>
      <description></description>
    </item>
    
    <item>
      <title>From global to local and viceversa: uses of associative rule learning for classification in imprecise environments</title>
      <link>https://gmanco.github.io/publication/costa-2011/</link>
      <pubDate>Thu, 01 Dec 2011 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/costa-2011/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Probabilistic Hierarchical Approach for Pattern Discovery in Collaborative Filtering Data</title>
      <link>https://gmanco.github.io/publication/barbieri-2011-a/</link>
      <pubDate>Fri, 01 Apr 2011 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2011-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Analysis of Probabilistic Methods for Top-N Recommendation in Collaborative Filtering</title>
      <link>https://gmanco.github.io/publication/barbieri-2011/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2011/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modeling item selection and relevance for accurate recommendations</title>
      <link>https://gmanco.github.io/publication/barbieri-2011-b/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2011-b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An incremental clustering scheme for data de-duplication</title>
      <link>https://gmanco.github.io/publication/costa-2009/</link>
      <pubDate>Thu, 01 Oct 2009 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/costa-2009/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Top-Down Parameter-Free Clustering of High-Dimensional Categorical Data</title>
      <link>https://gmanco.github.io/publication/cesario-2007-a/</link>
      <pubDate>Sat, 01 Dec 2007 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/cesario-2007-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mining unconnected patterns in workflows</title>
      <link>https://gmanco.github.io/publication/greco-2007/</link>
      <pubDate>Sun, 01 Jul 2007 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/greco-2007/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Boosting text segmentation via progressive classification</title>
      <link>https://gmanco.github.io/publication/cesario-2007/</link>
      <pubDate>Fri, 01 Jun 2007 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/cesario-2007/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploiting structural similarity for effective Web information extraction</title>
      <link>https://gmanco.github.io/publication/flesca-2007/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/flesca-2007/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mining categories for emails via clustering and pattern discovery</title>
      <link>https://gmanco.github.io/publication/manco-2007/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/manco-2007/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mining and reasoning on workflows</title>
      <link>https://gmanco.github.io/publication/greco-2005/</link>
      <pubDate>Fri, 01 Apr 2005 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/greco-2005/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fast detection of XML structural similarity</title>
      <link>https://gmanco.github.io/publication/flesca-2005/</link>
      <pubDate>Tue, 01 Feb 2005 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/flesca-2005/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://gmanco.github.io/research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/research/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>

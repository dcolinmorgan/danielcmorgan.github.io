<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Panoramica | Giuseppe Manco</title>
    <link>https://gmanco.github.io/courses/computervision/</link>
      <atom:link href="https://gmanco.github.io/courses/computervision/index.xml" rel="self" type="application/rss+xml" />
    <description>Panoramica</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Giuseppe Manco</copyright><lastBuildDate>Wed, 26 Feb 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://gmanco.github.io/img/manco.jpg</url>
      <title>Panoramica</title>
      <link>https://gmanco.github.io/courses/computervision/</link>
    </image>
    
    <item>
      <title>Introduzione al corso</title>
      <link>https://gmanco.github.io/courses/computervision/lecture1/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture1/</guid>
      <description>

&lt;p&gt;Introduzione al corso. Image Processing, Analysis e Computer Vision.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/1.Intro.pdf&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, sect. 1.1-1.3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, Ch. 1,2.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://algorithmia.com/blog/introduction-to-computer-vision&#34; target=&#34;_blank&#34;&gt;Introduction to Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/what-is-computer-vision/&#34; target=&#34;_blank&#34;&gt;A gentle introduction to Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/readers-writers-digest/beginners-guide-to-computer-vision-23606224b720&#34; target=&#34;_blank&#34;&gt;A Beginner&amp;rsquo;s guide to Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/everything-you-ever-wanted-to-know-about-computer-vision-heres-a-look-why-it-s-so-awesome-e8a58dfb641e&#34; target=&#34;_blank&#34;&gt;Everything you wanted to know about Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.thecvf.com/&#34; target=&#34;_blank&#34;&gt;The Computer Vision Foundation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.forbes.com/sites/forbestechcouncil/2020/04/03/three-ways-computer-vision-is-transforming-marketing/#1d6346d0214b&#34; target=&#34;_blank&#34;&gt;Three ways Computer Vision is transforming Marketing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;approfondimenti-contributi-dagli-studenti-alcune-applicazioni-per-la-computer-vision&#34;&gt;Approfondimenti (contributi dagli studenti): Alcune applicazioni per la computer vision&lt;/h2&gt;

&lt;h3 id=&#34;amazon-go&#34;&gt;Amazon GO&lt;/h3&gt;

&lt;p&gt;Il progetto Amazon GO consiste in un negozio in cui non è presente alcuna cassa in quanto il check-out avviene automaticamente direttamente sul proprio account Amazon. I clienti attivano l’app sul telefono e scansionano il proprio qr-code prima di entrare nel negozio, dopodiché un sistema di videocamere e sensori tracceranno gli spostamenti dei clienti, tenendo traccia dei prodotti da loro acquistati. All’uscita dal negozio si scansiona nuovamente il qr-code ed i prodotti acquistati verranno addebitati direttamente sul proprio account amazon.
Riferimenti:
- &lt;a href=&#34;https://youtu.be/uoKsY9HDk6o&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;
- &lt;a href=&#34;https://emerj.com/ai-sector-overviews/computer-vision-applications-shopping-driving-and-more/&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;blood-loss-monitoring&#34;&gt;Blood loss monitoring&lt;/h3&gt;

&lt;p&gt;Questo sistema utilizza le immagini delle spugne chirurgiche e delle taniche di aspirazione per andare a stimare la perdita di sangue tramite il processamento di tali immagini tramite algoritmi di machine learning e di computer vision. Questa applicazione viene utilizzata in alcuni ospedali durante i parti cesarei. Tali misurazioni risultano essere più accurate delle valutazioni fatte ad occhio nudo e risultano essere fondamentali per stabilire se siano necessarie trasfusioni o meno.
Riferimenti:
- &lt;a href=&#34;https://youtu.be/cBzJ43zU4FY&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;
- &lt;a href=&#34;https://emerj.com/ai-sector-overviews/computer-vision-applications-shopping-driving-and-more/&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;real-time-sports-tracking&#34;&gt;Real-time sports tracking&lt;/h3&gt;

&lt;p&gt;La computer vision viene utilizzata per aiutare quella che è l’analisi di gioco e delle strategie ed anche per andare a misurare le performance dei giocatori, questa però non è la sola applicazione utilizzata infatti quest’ultima viene utilizzata anche per studiare quella che è la visibilità dei brand presenti ad esempio sulle divise dei giocatori. Questo è quello che viene effettuato dall’azienda Gumgum.
Riferimenti:
- &lt;a href=&#34;https://gumgum.com/sports/&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;
- &lt;a href=&#34;https://www.forbes.com/sites/bernardmarr/2019/04/08/7-amazing-examples-of-computer-and-machine-vision-in-practice/#2c1921cc1018&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;from-image-to-3d-models&#34;&gt;From image to 3D models&lt;/h3&gt;

&lt;p&gt;La computer vision ha consentito la costruzione di sistemi in grado di ricostruire modelli 3d a partire dalle immagini. I dettagli di tale lavoro sono descritti nel seguente paper. Quest’ultimo mostra anche una serie di campi in cui viene impiegata tale applicazione.
Link utili:
- &lt;a href=&#34;https://www.researchgate.net/profile/Marc_Pollefeys/publication/220423006_From_images_to_3D_models/links/53f5cf0e0cf22be01c3fb089/From-images-to-3D-models.pdf&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;food-quality-control&#34;&gt;Food quality control&lt;/h3&gt;

&lt;p&gt;La computer vision viene utilizzata anche per effettuare la valutazione della qualità del cibo, tale applicazione viene facilmente utilizzata nel campo industriale in quanto veloce e maggiormente oggettiva rispetto all’analisi umana. Il paper riportato riporta una carrellata di tecniche utilizzate e dell’accuratezza da esse riportate per diverse tipologie di cibo che vanno dalla verdura alla frutta a cibi più complessi come ad esempio la pizza ed i dolci.
Link utili:
- &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0168169902001011&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Concetti fondamentali su Image processing e analysis</title>
      <link>https://gmanco.github.io/courses/computervision/lecture2/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture2/</guid>
      <description>

&lt;p&gt;Concetti fondamentali su Image processing e analysis: Image Basics, Manipolazione di immagini. Introduzione alle librerie Python per Image Processing.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/2.Image_fundamentals.pdf&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;La lezione è corredata di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/1.Image_fundamentals.ipynb&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 2.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, sect. 1.4-1.5.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 2-3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 3.1, 3.6.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch.1-6.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://cs231n.github.io/python-numpy-tutorial/&#34; target=&#34;_blank&#34;&gt;Stanford cs231n Python tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cs231n.github.io/ipython-tutorial/&#34; target=&#34;_blank&#34;&gt;Stanford cs231n Jupyter tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.anaconda.com/&#34; target=&#34;_blank&#34;&gt;Anaconda, la piattaforma di riferimento per installare Python e Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://notebooks.azure.com/&#34; target=&#34;_blank&#34;&gt;Piattaforma azure di Microsoft per l&amp;rsquo;esecuzione di notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/notebooks/intro.ipynb&#34; target=&#34;_blank&#34;&gt;Google Colab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://missing.csail.mit.edu/?fbclid=IwAR0N3s-DbRLrSWW3tB1L5iu_thdiEtFuL7cGUwxyOL-yc7skytSDGdT9ZAo&#34; target=&#34;_blank&#34;&gt;The missing semester, un mini-corso del MIT sui tool di base che tutti i data scientist dovrebbero conoscere&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Filtri</title>
      <link>https://gmanco.github.io/courses/computervision/lecture3/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture3/</guid>
      <description>

&lt;p&gt;Manipolazione spaziale. Filtri. Convoluzione.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/3.Filters.pdf&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;La lezione è corredata di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/2.Filters.ipynb&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, Ch. 3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 3.2, 3.3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch. 2, 10-12.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/7075360&#34; target=&#34;_blank&#34;&gt;Adaptive filters for color image processing: A survey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/image-filters-in-python-26ee938e57d2&#34; target=&#34;_blank&#34;&gt;Image filters in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scipy-lectures.org/advanced/image_processing/&#34; target=&#34;_blank&#34;&gt;Image manipulation using Numpy and SciPy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@enzoftware/how-to-build-amazing-images-filters-with-python-median-filter-sobel-filter-%EF%B8%8F-%EF%B8%8F-22aeb8e2f540&#34; target=&#34;_blank&#34;&gt;How to build an image filter in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=otLGDpBglEA&amp;amp;feature=player_embedded&#34; target=&#34;_blank&#34;&gt;Instagram filters in 15 lines of code&lt;/a&gt;. Il codice è disponibile &lt;a href=&#34;https://github.com/lukexyz/CV-Instagram-Filters/blob/master/gotham.py&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Python &amp; image processing</title>
      <link>https://gmanco.github.io/courses/computervision/lecture_lab1/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture_lab1/</guid>
      <description>

&lt;p&gt;Principali argomenti trattati:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Introduzione ai Jupyter notebooks&lt;/li&gt;
&lt;li&gt;Caricamento di immagini con PIL&lt;/li&gt;
&lt;li&gt;Panoramica delle librerie e delle principali funzioni per effettuare trasformazioni geometriche, operazioni aritmetiche, edge detection.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/es01_imageloading.pdf&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;La lezione è corredata di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/labs_lecture/lab01&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 5.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 4.2.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch. 13.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/master/index.html&#34; target=&#34;_blank&#34;&gt;OpenCV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-image.org/docs/stable/api/api.html&#34; target=&#34;_blank&#34;&gt;Scikit Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pillow.readthedocs.io/en/stable/handbook/tutorial.html&#34; target=&#34;_blank&#34;&gt;PIL tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/scikit-image/skimage-tutorials&#34; target=&#34;_blank&#34;&gt;Scikit Image Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Derivate. Edge Detection</title>
      <link>https://gmanco.github.io/courses/computervision/lecture4/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture4/</guid>
      <description>

&lt;p&gt;Derivate di immagini. Gradient-based filtering. trasformata di Fourier e Filtraggio.&lt;/p&gt;

&lt;p&gt;Slides disponibili:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../pdf/4a.Edge Detection.pdf&#34;&gt;edge detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../pdf/4b.Edge_Detection.pdf&#34;&gt;Fourier Transform&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La lezione è corredata di due notebook:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/3.Edge_detection.ipynb&#34; target=&#34;_blank&#34;&gt;Edge detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/4.Fourier_transform.ipynb&#34; target=&#34;_blank&#34;&gt;Fourier Transform&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 5.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 3, 4.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 4.2.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch. 10, 11, 13.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Andrew Ng on Edge Detection: youtube lectures &lt;a href=&#34;https://www.youtube.com/watch?v=XuD4C8vJzEQ&#34; target=&#34;_blank&#34;&gt;(1)&lt;/a&gt; e &lt;a href=&#34;https://www.youtube.com/watch?v=am36dePheDc&#34; target=&#34;_blank&#34;&gt;(2)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://datacarpentry.org/image-processing/08-edge-detection/&#34; target=&#34;_blank&#34;&gt;Introduction to Edge Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tutorialspoint.com/dip/concept_of_edge_detection.htm&#34; target=&#34;_blank&#34;&gt;Concept of Image detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=aDY4aBLFOIg&#34; target=&#34;_blank&#34;&gt;OpenCV Python Tutorial: Image Gradients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@hicraigchen/digital-image-processing-using-fourier-transform-in-python-bcb49424fd82&#34; target=&#34;_blank&#34;&gt;Filtraggio con la trasformata&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Feature descriptors. Classificazione</title>
      <link>https://gmanco.github.io/courses/computervision/lecture5/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture5/</guid>
      <description>

&lt;p&gt;Derivate di immagini. Gradient-based filtering. trasformata di Fourier e Filtraggio.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/5.Classificazione.pdf&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Importante&lt;/strong&gt;
La parte relativa allo studio dei feature descriptors riguarderà anche il primo esonero. I dettagli dell&amp;rsquo;esonero sono pubblicati &lt;a href=&#34;https://gmanco.github.io/courses/computervision/listoffeaturedescriptors/&#34; target=&#34;_blank&#34;&gt;nell&amp;rsquo;apposita pagina&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, Ch. 6, 13.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 2, 8.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, Ch. 4.1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch. 17.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9&#34; target=&#34;_blank&#34;&gt;Introduction to Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/logistic-regression-tutorial-for-machine-learning/&#34; target=&#34;_blank&#34;&gt;Tutorial on Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mathematical-tours.github.io/book-sources/optim-ml/OptimML.pdf&#34; target=&#34;_blank&#34;&gt;Course Notes on Optimization for Machine Learning&lt;/a&gt; and &lt;a href=&#34;https://mathematical-tours.github.io/book-sources/FundationsDataScience.pdf&#34; target=&#34;_blank&#34;&gt;Mathematical Foundations of Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/tutorial/basic/tutorial.html&#34; target=&#34;_blank&#34;&gt;An Introduction to Machine Learning with scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/analytics-vidhya/introduction-to-feature-detection-and-matching-65e27179885d&#34; target=&#34;_blank&#34;&gt;Introduction to Feature Detection and Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aishack.in/tutorials/sift-scale-invariant-feature-transform-features/&#34; target=&#34;_blank&#34;&gt;SIFT: Theory and Practice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_table_of_contents_feature2d/py_table_of_contents_feature2d.html&#34; target=&#34;_blank&#34;&gt;OpenCV Tutorials on Feature Descriptors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Esonero 1</title>
      <link>https://gmanco.github.io/courses/computervision/listoffeaturedescriptors/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/listoffeaturedescriptors/</guid>
      <description>

&lt;h1 id=&#34;feature-descriptors&#34;&gt;Feature descriptors&lt;/h1&gt;

&lt;p&gt;In questo esonero è chiesto agli studenti di sperimentare con un feature descriptr tra quelli elencati in seguito. In particolare si chiede di:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Scegliere un feature descriptor&lt;/li&gt;
&lt;li&gt;Preparare delle slides dettagliate (max 15) che lo descrivono&lt;/li&gt;
&lt;li&gt;Preparare un notebook in cui:

&lt;ul&gt;
&lt;li&gt;si implementa l&amp;rsquo;algoritmo che sta alla slide 62 (istogramma basato sui feature descriptors)&lt;/li&gt;
&lt;li&gt;si applica la regressione logistica (da scikit-learn) sul set di immagini (MNIST)&lt;/li&gt;
&lt;li&gt;si valutano i risultati della classificazione confrontandoli con la regressione logistica applicata alla flattenizzazione raw dell&amp;rsquo;immagine (opportunamente preprocessata).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Gli studenti interessati ad effettuare l&amp;rsquo;esonero dovranno mandare una mail al docente indicando, in ordine di priorità, tre scelte dalla lista di cui sotto.
Le assegnazioni verranno comunicate a lezione. La deadline per la consegna degli elaborati è il &lt;strong&gt;16 aprile 2020&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;opencv&#34;&gt;OpenCV&lt;/h2&gt;

&lt;p&gt;Tutorials disponibili &lt;a href=&#34;https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_table_of_contents_feature2d/py_table_of_contents_feature2d.html&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/4.2.0/d5/d3c/classcv_1_1xfeatures2d_1_1SIFT.html&#34; target=&#34;_blank&#34;&gt;SIFT&lt;/a&gt;, Class for extracting keypoints and computing descriptors using the Scale Invariant Feature Transform (&lt;a href=&#34;https://docs.opencv.org/4.2.0/d5/d3c/classcv_1_1xfeatures2d_1_1SIFT.html&#34; target=&#34;_blank&#34;&gt;SIFT&lt;/a&gt;) algorithm by D. Lowe. Riferimenti: David G Lowe. &lt;a href=&#34;https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&#34; target=&#34;_blank&#34;&gt;Distinctive image features from scale-invariant keypoints&lt;/a&gt;. &lt;em&gt;International journal of computer vision&lt;/em&gt;, 60(2):91–110, 2004.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/4.2.0/d5/df7/classcv_1_1xfeatures2d_1_1SURF.html&#34; target=&#34;_blank&#34;&gt;SURF&lt;/a&gt;, Class for extracting Speeded Up Robust Features from an image. Riferimenti: Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. &lt;a href=&#34;http://scholar.google.it/scholar_url?url=https://lirias.kuleuven.be/retrieve/78517&amp;amp;hl=it&amp;amp;sa=X&amp;amp;scisig=AAGBfm2HNoDAKFbI70IQ22ndV5cBLu7pBw&amp;amp;nossl=1&amp;amp;oi=scholarr&#34; target=&#34;_blank&#34;&gt;Surf: Speeded up robust features&lt;/a&gt;. &lt;em&gt;Computer Vision–ECCV 2006&lt;/em&gt;, pages 404–417, 2006.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/4.2.0/d8/d30/classcv_1_1AKAZE.html&#34; target=&#34;_blank&#34;&gt;AKAZE&lt;/a&gt;, Class implementing the &lt;a href=&#34;https://docs.opencv.org/4.2.0/d8/d30/classcv_1_1AKAZE.html&#34; target=&#34;_blank&#34;&gt;AKAZE&lt;/a&gt; keypoint detector and descriptor extractor. Riferimenti: Pablo F Alcantarilla, Jesús Nuevo, and Adrien Bartoli. &lt;a href=&#34;http://www.bmva.org/bmvc/2013/Papers/paper0013/paper0013.pdf&#34; target=&#34;_blank&#34;&gt;Fast explicit diffusion for accelerated features in nonlinear scale spaces&lt;/a&gt;. &lt;em&gt;Trans. Pattern Anal. Machine Intell&lt;/em&gt;, 34(7):1281–1298, 2011.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/4.2.0/de/dbf/classcv_1_1BRISK.html&#34; target=&#34;_blank&#34;&gt;Brisk&lt;/a&gt;, Class implementing the &lt;a href=&#34;https://docs.opencv.org/4.2.0/de/dbf/classcv_1_1BRISK.html&#34; target=&#34;_blank&#34;&gt;BRISK&lt;/a&gt; keypoint detector and descriptor extractor. [Stefan Leutenegger, Margarita Chli, and Roland Yves Siegwart. &lt;a href=&#34;https://www.researchgate.net/publication/221110715_BRISK_Binary_Robust_invariant_scalable_keypoints&#34; target=&#34;_blank&#34;&gt;Brisk: Binary robust invariant scalable keypoints&lt;/a&gt;. In &lt;em&gt;Computer Vision (ICCV), 2011 IEEE International Conference on&lt;/em&gt;, pages 2548–2555. IEEE, 2011.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/4.2.0/d3/d28/classcv_1_1MSER.html&#34; target=&#34;_blank&#34;&gt;Mser&lt;/a&gt;, Maximally stable extremal region extractor, for grey scale and color image. Riferimenti: David Nistér and Henrik Stewénius. &lt;a href=&#34;https://www.researchgate.net/publication/221304597_Linear_Time_Maximally_Stable_Extremal_Regions&#34; target=&#34;_blank&#34;&gt;Linear time maximally stable extremal regions&lt;/a&gt;. In &lt;em&gt;Computer Vision–ECCV 2008&lt;/em&gt;, pages 183–196. Springer, 2008.; Per-Erik Forssén. Maximally stable colour regions for recognition and matching. In &lt;em&gt;Computer Vision and Pattern Recognition, 2007. CVPR&amp;rsquo;07. IEEE Conference on&lt;/em&gt;, pages 1–8. IEEE, 2007.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/4.2.0/db/d95/classcv_1_1ORB.html&#34; target=&#34;_blank&#34;&gt;ORB&lt;/a&gt;, The algorithm uses FAST in pyramids to detect stable keypoints, selects  the strongest features using FAST or Harris response, finds their  orientation using first-order moments and computes the descriptors using BRIEF (where the coordinates of random point pairs (or k-tuples) are  rotated according to the measured orientation). Riferimenti: Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski. &lt;a href=&#34;http://www.willowgarage.com/sites/default/files/orb_final.pdf&#34; target=&#34;_blank&#34;&gt;Orb: an efficient alternative to sift or surf&lt;/a&gt;. In &lt;em&gt;Computer Vision (ICCV), 2011 IEEE International Conference on&lt;/em&gt;, pages 2564–2571. IEEE, 2011.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;sk-image&#34;&gt;SK-Image&lt;/h2&gt;

&lt;p&gt;Documentation at the home page of the &lt;a href=&#34;https://scikit-image.org/docs/0.16.x/api/skimage.feature.html&#34; target=&#34;_blank&#34;&gt;scikit-image feature description package&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;skimage.feature.daisy&lt;/code&gt;, Extract DAISY feature descriptors densely for the given image. DAISY is a feature descriptor similar to SIFT formulated in a way that allows for fast dense extraction. Typically, this is practical for bag-of-features image representations. Riferimenti: Tola et al. &lt;a href=&#34;https://infoscience.epfl.ch/record/138785/files/tola_daisy_pami_1.pdf&#34; target=&#34;_blank&#34;&gt;Daisy: An efficient dense descriptor applied to wide- baseline stereo&lt;/a&gt;. Pattern Analysis and Machine Intelligence, IEEE Transactions on 32.5 (2010): 815-830.]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;skimage.feature.hog&lt;/code&gt;, Extract Histogram of Oriented Gradients (HOG) for a given image. Riferimenti: Dalal, N and Triggs, B, &lt;a href=&#34;https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf&#34; target=&#34;_blank&#34;&gt;Histograms of Oriented Gradients for Human Detection&lt;/a&gt;, IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2005 San Diego, CA, USA, DOI:10.1109/CVPR.2005.177]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;skimage.feature.local_binary_pattern&lt;/code&gt;, Gray scale and rotation invariant LBP (Local Binary Patterns). Riferimenti: Timo Ojala, Matti Pietikainen, Topi Maenpaa. &lt;a href=&#34;http://www.ee.oulu.fi/research/mvmp/mvg/files/pdf/pdf_94.pdf&#34; target=&#34;_blank&#34;&gt;Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns&lt;/a&gt;. 2002]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;skimage.feature.haar_like_feature&lt;/code&gt;, Compute the Haar-like features for a region of interest (ROI) of an integral image. Riferimenti: Messom, Christopher H. and Andre L. C. Barczak. &lt;a href=&#34;https://www.semanticscholar.org/paper/Stream-processing-for-fast-and-efficient-rotated-Messom-Barczak/b55e215acd9bc0496f1f611d6193fea0b10e4212&#34; target=&#34;_blank&#34;&gt;Stream processing for fast and efficient rotated Haar-like features using rotated integral images&lt;/a&gt;. &lt;em&gt;IJISTA&lt;/em&gt; 7 (2006): 40-57.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;skimage.feature.BRIEF&lt;/code&gt;, BRIEF binary descriptor extractor. BRIEF (Binary Robust Independent Elementary Features) is an efficient feature point descriptor. It is highly discriminative even when using relatively few bits and is computed using simple intensity difference tests. Riferimenti: Calonder, Michael &amp;amp; Lepetit, Vincent &amp;amp; Strecha, Christoph &amp;amp; Fua, Pascal. (2010). &lt;a href=&#34;https://www.cs.ubc.ca/~lowe/525/papers/calonder_eccv10.pdf&#34; target=&#34;_blank&#34;&gt;BRIEF: Binary Robust Independent Elementary Features&lt;/a&gt;. Eur. Conf. Comput. Vis.. 6314. 778-792. 10.&lt;sup&gt;1007&lt;/sup&gt;&amp;frasl;&lt;sub&gt;978&lt;/sub&gt;-3-642-15561-1_56.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;skimage.feature.ORB&lt;/code&gt;, Oriented FAST and rotated BRIEF feature detector and binary descriptor extractor. Riferimenti: Ethan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski. &lt;a href=&#34;http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf&#34; target=&#34;_blank&#34;&gt;ORB: An efficient alternative to SIFT and SURF&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;assegnazioni&#34;&gt;Assegnazioni&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Giulia Katia Galimberti  &lt;strong&gt;SIFT&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Maria Francesca Alati  &lt;strong&gt;skimage.feature.hog&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Lorenzo Defina  &lt;strong&gt;Mser&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Emilio Casella  &lt;strong&gt;Surf&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Simona Nisticò matricola &lt;strong&gt;ORB&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Domenico Montesanto &lt;strong&gt;AKAZE&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Caterina Maugeri &lt;strong&gt;skimage.feature.local_binary_pattern&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Giuseppe Surace  &lt;strong&gt;skimage.feature.haar_like_feature&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Antonello Crea &lt;strong&gt;Daisy&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Anile Anna  &lt;strong&gt;skimage.feature.BRIEF&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Vincenzo Parrilla  &lt;strong&gt;SURF with Harris Corner Detection&lt;/strong&gt;. (OpenCV)&lt;/li&gt;
&lt;li&gt;Davide Medaglia &lt;strong&gt;SIFT with Harris Corner Detection&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Antonio Commisso &lt;strong&gt;Brisk&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Antonio Gagliostro: &lt;strong&gt;skimage.feature.ORB&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>

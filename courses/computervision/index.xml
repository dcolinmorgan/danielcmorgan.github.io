<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Panoramica | Giuseppe Manco</title>
    <link>https://gmanco.github.io/courses/computervision/</link>
      <atom:link href="https://gmanco.github.io/courses/computervision/index.xml" rel="self" type="application/rss+xml" />
    <description>Panoramica</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Giuseppe Manco</copyright><lastBuildDate>Wed, 26 Feb 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://gmanco.github.io/img/manco.jpg</url>
      <title>Panoramica</title>
      <link>https://gmanco.github.io/courses/computervision/</link>
    </image>
    
    <item>
      <title>Introduzione al corso</title>
      <link>https://gmanco.github.io/courses/computervision/lecture1/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture1/</guid>
      <description>

&lt;p&gt;Introduzione al corso. Image Processing, Analysis e Computer Vision.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/1.Intro.pdf&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, sect. 1.1-1.3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, Ch. 1,2.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://algorithmia.com/blog/introduction-to-computer-vision&#34; target=&#34;_blank&#34;&gt;Introduction to Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/what-is-computer-vision/&#34; target=&#34;_blank&#34;&gt;A gentle introduction to Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/readers-writers-digest/beginners-guide-to-computer-vision-23606224b720&#34; target=&#34;_blank&#34;&gt;A Beginner&amp;rsquo;s guide to Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/everything-you-ever-wanted-to-know-about-computer-vision-heres-a-look-why-it-s-so-awesome-e8a58dfb641e&#34; target=&#34;_blank&#34;&gt;Everything you wanted to know about Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.thecvf.com/&#34; target=&#34;_blank&#34;&gt;The Computer Vision Foundation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.forbes.com/sites/forbestechcouncil/2020/04/03/three-ways-computer-vision-is-transforming-marketing/#1d6346d0214b&#34; target=&#34;_blank&#34;&gt;Three ways Computer Vision is transforming Marketing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;approfondimenti-contributi-dagli-studenti-alcune-applicazioni-per-la-computer-vision&#34;&gt;Approfondimenti (contributi dagli studenti): Alcune applicazioni per la computer vision&lt;/h2&gt;

&lt;h3 id=&#34;amazon-go&#34;&gt;Amazon GO&lt;/h3&gt;

&lt;p&gt;Il progetto Amazon GO consiste in un negozio in cui non è presente alcuna cassa in quanto il check-out avviene automaticamente direttamente sul proprio account Amazon. I clienti attivano l’app sul telefono e scansionano il proprio qr-code prima di entrare nel negozio, dopodiché un sistema di videocamere e sensori tracceranno gli spostamenti dei clienti, tenendo traccia dei prodotti da loro acquistati. All’uscita dal negozio si scansiona nuovamente il qr-code ed i prodotti acquistati verranno addebitati direttamente sul proprio account amazon.
Riferimenti:
- &lt;a href=&#34;https://youtu.be/uoKsY9HDk6o&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;
- &lt;a href=&#34;https://emerj.com/ai-sector-overviews/computer-vision-applications-shopping-driving-and-more/&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;blood-loss-monitoring&#34;&gt;Blood loss monitoring&lt;/h3&gt;

&lt;p&gt;Questo sistema utilizza le immagini delle spugne chirurgiche e delle taniche di aspirazione per andare a stimare la perdita di sangue tramite il processamento di tali immagini tramite algoritmi di machine learning e di computer vision. Questa applicazione viene utilizzata in alcuni ospedali durante i parti cesarei. Tali misurazioni risultano essere più accurate delle valutazioni fatte ad occhio nudo e risultano essere fondamentali per stabilire se siano necessarie trasfusioni o meno.
Riferimenti:
- &lt;a href=&#34;https://youtu.be/cBzJ43zU4FY&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;
- &lt;a href=&#34;https://emerj.com/ai-sector-overviews/computer-vision-applications-shopping-driving-and-more/&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;real-time-sports-tracking&#34;&gt;Real-time sports tracking&lt;/h3&gt;

&lt;p&gt;La computer vision viene utilizzata per aiutare quella che è l’analisi di gioco e delle strategie ed anche per andare a misurare le performance dei giocatori, questa però non è la sola applicazione utilizzata infatti quest’ultima viene utilizzata anche per studiare quella che è la visibilità dei brand presenti ad esempio sulle divise dei giocatori. Questo è quello che viene effettuato dall’azienda Gumgum.
Riferimenti:
- &lt;a href=&#34;https://gumgum.com/sports/&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;
- &lt;a href=&#34;https://www.forbes.com/sites/bernardmarr/2019/04/08/7-amazing-examples-of-computer-and-machine-vision-in-practice/#2c1921cc1018&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;from-image-to-3d-models&#34;&gt;From image to 3D models&lt;/h3&gt;

&lt;p&gt;La computer vision ha consentito la costruzione di sistemi in grado di ricostruire modelli 3d a partire dalle immagini. I dettagli di tale lavoro sono descritti nel seguente paper. Quest’ultimo mostra anche una serie di campi in cui viene impiegata tale applicazione.
Link utili:
- &lt;a href=&#34;https://www.researchgate.net/profile/Marc_Pollefeys/publication/220423006_From_images_to_3D_models/links/53f5cf0e0cf22be01c3fb089/From-images-to-3D-models.pdf&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;food-quality-control&#34;&gt;Food quality control&lt;/h3&gt;

&lt;p&gt;La computer vision viene utilizzata anche per effettuare la valutazione della qualità del cibo, tale applicazione viene facilmente utilizzata nel campo industriale in quanto veloce e maggiormente oggettiva rispetto all’analisi umana. Il paper riportato riporta una carrellata di tecniche utilizzate e dell’accuratezza da esse riportate per diverse tipologie di cibo che vanno dalla verdura alla frutta a cibi più complessi come ad esempio la pizza ed i dolci.
Link utili:
- &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0168169902001011&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Concetti fondamentali su Image processing e analysis</title>
      <link>https://gmanco.github.io/courses/computervision/lecture2/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture2/</guid>
      <description>

&lt;p&gt;Concetti fondamentali su Image processing e analysis: Image Basics, Manipolazione di immagini. Introduzione alle librerie Python per Image Processing.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/2.Image_fundamentals.pdf&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;La lezione è corredata di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/1.Image_fundamentals.ipynb&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 2.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, sect. 1.4-1.5.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 2-3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 3.1, 3.6.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch.1-6.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://cs231n.github.io/python-numpy-tutorial/&#34; target=&#34;_blank&#34;&gt;Stanford cs231n Python tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cs231n.github.io/ipython-tutorial/&#34; target=&#34;_blank&#34;&gt;Stanford cs231n Jupyter tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.anaconda.com/&#34; target=&#34;_blank&#34;&gt;Anaconda, la piattaforma di riferimento per installare Python e Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://notebooks.azure.com/&#34; target=&#34;_blank&#34;&gt;Piattaforma azure di Microsoft per l&amp;rsquo;esecuzione di notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/notebooks/intro.ipynb&#34; target=&#34;_blank&#34;&gt;Google Colab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://missing.csail.mit.edu/?fbclid=IwAR0N3s-DbRLrSWW3tB1L5iu_thdiEtFuL7cGUwxyOL-yc7skytSDGdT9ZAo&#34; target=&#34;_blank&#34;&gt;The missing semester, un mini-corso del MIT sui tool di base che tutti i data scientist dovrebbero conoscere&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Filtri</title>
      <link>https://gmanco.github.io/courses/computervision/lecture3/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture3/</guid>
      <description>

&lt;p&gt;Manipolazione spaziale. Filtri. Convoluzione.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/3.Filters.pdf&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;La lezione è corredata di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/2.Filters.ipynb&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, Ch. 3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 3.2, 3.3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch. 2, 10-12.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/7075360&#34; target=&#34;_blank&#34;&gt;Adaptive filters for color image processing: A survey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/image-filters-in-python-26ee938e57d2&#34; target=&#34;_blank&#34;&gt;Image filters in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scipy-lectures.org/advanced/image_processing/&#34; target=&#34;_blank&#34;&gt;Image manipulation using Numpy and SciPy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@enzoftware/how-to-build-amazing-images-filters-with-python-median-filter-sobel-filter-%EF%B8%8F-%EF%B8%8F-22aeb8e2f540&#34; target=&#34;_blank&#34;&gt;How to build an image filter in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=otLGDpBglEA&amp;amp;feature=player_embedded&#34; target=&#34;_blank&#34;&gt;Instagram filters in 15 lines of code&lt;/a&gt;. Il codice è disponibile &lt;a href=&#34;https://github.com/lukexyz/CV-Instagram-Filters/blob/master/gotham.py&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Python &amp; image processing</title>
      <link>https://gmanco.github.io/courses/computervision/lecture_lab1/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture_lab1/</guid>
      <description>

&lt;p&gt;Principali argomenti trattati:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Introduzione ai Jupyter notebooks&lt;/li&gt;
&lt;li&gt;Caricamento di immagini con PIL&lt;/li&gt;
&lt;li&gt;Panoramica delle librerie e delle principali funzioni per effettuare trasformazioni geometriche, operazioni aritmetiche, edge detection.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/es01_imageloading.pdf&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;La lezione è corredata di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/labs_lecture/lab01&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 5.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 4.2.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch. 13.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/master/index.html&#34; target=&#34;_blank&#34;&gt;OpenCV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-image.org/docs/stable/api/api.html&#34; target=&#34;_blank&#34;&gt;Scikit Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pillow.readthedocs.io/en/stable/handbook/tutorial.html&#34; target=&#34;_blank&#34;&gt;PIL tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/scikit-image/skimage-tutorials&#34; target=&#34;_blank&#34;&gt;Scikit Image Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Derivate. Edge Detection</title>
      <link>https://gmanco.github.io/courses/computervision/lecture4/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture4/</guid>
      <description>

&lt;p&gt;Derivate di immagini. Gradient-based filtering. trasformata di Fourier e Filtraggio.&lt;/p&gt;

&lt;p&gt;Slides disponibili:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../pdf/4a.Edge_Detection.pdf&#34;&gt;edge detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../pdf/4b.FFT.pdf&#34;&gt;Fourier Transform&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La lezione è corredata di due notebook:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/3.Edge_detection.ipynb&#34; target=&#34;_blank&#34;&gt;Edge detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/4.Fourier_transform.ipynb&#34; target=&#34;_blank&#34;&gt;Fourier Transform&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 5.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 3, 4.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, sect. 4.2.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch. 10, 11, 13.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Andrew Ng on Edge Detection: youtube lectures &lt;a href=&#34;https://www.youtube.com/watch?v=XuD4C8vJzEQ&#34; target=&#34;_blank&#34;&gt;(1)&lt;/a&gt; e &lt;a href=&#34;https://www.youtube.com/watch?v=am36dePheDc&#34; target=&#34;_blank&#34;&gt;(2)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://datacarpentry.org/image-processing/08-edge-detection/&#34; target=&#34;_blank&#34;&gt;Introduction to Edge Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tutorialspoint.com/dip/concept_of_edge_detection.htm&#34; target=&#34;_blank&#34;&gt;Concept of Image detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=aDY4aBLFOIg&#34; target=&#34;_blank&#34;&gt;OpenCV Python Tutorial: Image Gradients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@hicraigchen/digital-image-processing-using-fourier-transform-in-python-bcb49424fd82&#34; target=&#34;_blank&#34;&gt;Filtraggio con la trasformata&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Feature descriptors. Classificazione</title>
      <link>https://gmanco.github.io/courses/computervision/lecture5/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture5/</guid>
      <description>

&lt;p&gt;Derivate di immagini. Gradient-based filtering. trasformata di Fourier e Filtraggio.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../pdf/5.Classificazione.pdf&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Importante&lt;/strong&gt;
La parte relativa allo studio dei feature descriptors riguarderà anche il primo esonero. I dettagli dell&amp;rsquo;esonero sono pubblicati &lt;a href=&#34;https://gmanco.github.io/courses/computervision/esoneri/listoffeaturedescriptors/&#34; target=&#34;_blank&#34;&gt;nell&amp;rsquo;apposita pagina&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, Ch. 6, 13.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 2, 8.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, Ch. 4.1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Kin19]&lt;/strong&gt;, Ch. 17.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9&#34; target=&#34;_blank&#34;&gt;Introduction to Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/logistic-regression-tutorial-for-machine-learning/&#34; target=&#34;_blank&#34;&gt;Tutorial on Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mathematical-tours.github.io/book-sources/optim-ml/OptimML.pdf&#34; target=&#34;_blank&#34;&gt;Course Notes on Optimization for Machine Learning&lt;/a&gt; and &lt;a href=&#34;https://mathematical-tours.github.io/book-sources/FundationsDataScience.pdf&#34; target=&#34;_blank&#34;&gt;Mathematical Foundations of Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/tutorial/basic/tutorial.html&#34; target=&#34;_blank&#34;&gt;An Introduction to Machine Learning with scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/analytics-vidhya/introduction-to-feature-detection-and-matching-65e27179885d&#34; target=&#34;_blank&#34;&gt;Introduction to Feature Detection and Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aishack.in/tutorials/sift-scale-invariant-feature-transform-features/&#34; target=&#34;_blank&#34;&gt;SIFT: Theory and Practice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_table_of_contents_feature2d/py_table_of_contents_feature2d.html&#34; target=&#34;_blank&#34;&gt;OpenCV Tutorials on Feature Descriptors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Pytorch, SKlearn and Classification</title>
      <link>https://gmanco.github.io/courses/computervision/lecture_lab2/</link>
      <pubDate>Tue, 07 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture_lab2/</guid>
      <description>

&lt;p&gt;Pytorch &amp;amp; SKlearn&lt;/p&gt;

&lt;p&gt;Principali argomenti trattati:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pre-process di un dataset e costruzione dei train/validation/test set&lt;/li&gt;
&lt;li&gt;SKLearn, regressione lineare e logistica&lt;/li&gt;
&lt;li&gt;Introduzione a Pytorch&lt;/li&gt;
&lt;li&gt;Costruzione di un modello di NN&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La lezione è corredata di due notebook disponibili &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/labs_lecture/lab02&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 13.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, ch. 2.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/&#34; target=&#34;_blank&#34;&gt;SKLearn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/index.html&#34; target=&#34;_blank&#34;&gt;Pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.openml.org/d/554/&#34; target=&#34;_blank&#34;&gt;MINIST dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Neural Networks</title>
      <link>https://gmanco.github.io/courses/computervision/lecture6/</link>
      <pubDate>Sat, 18 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture6/</guid>
      <description>

&lt;p&gt;Grafi di computazione e modelli non lineari. Reti Convoluzionali. LeNet-5.&lt;/p&gt;

&lt;p&gt;Materiale didattico:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Modelli non lineari (&lt;a href=&#34;../pdf/6a.Reti neurali_parte1.pdf&#34;&gt;slides&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Reti convoluzionali (&lt;a href=&#34;../pdf/6b.Reti neurali_parte2.pdf&#34;&gt;slides&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/6.Neural_networks.ipynb&#34; target=&#34;_blank&#34;&gt;Notebook&lt;/a&gt; di accompagnamento&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, Ch. 15.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 13.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, Ch. 2,3&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9&#34; target=&#34;_blank&#34;&gt;Introduction to Neural Networks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://d2l.ai/index.html&#34; target=&#34;_blank&#34;&gt;Dive into Deep Learning&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://mathematical-tours.github.io/book-sources/optim-ml/OptimML.pdf&#34; target=&#34;_blank&#34;&gt;Course Notes on Optimization for Machine Learning&lt;/a&gt; and &lt;a href=&#34;https://mathematical-tours.github.io/book-sources/FundationsDataScience.pdf&#34; target=&#34;_blank&#34;&gt;Mathematical Foundations of Data Science&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/&#34; target=&#34;_blank&#34;&gt;Understanding graphs and automatic differentiation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b&#34; target=&#34;_blank&#34;&gt;Yes you should understand backpropagation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf&#34; target=&#34;_blank&#34;&gt;Visualizing and understanding convolutional Networks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Vincent Dumoulin, Francesco Visin - &lt;a href=&#34;https://arxiv.org/abs/1603.07285&#34; target=&#34;_blank&#34;&gt;A guide to convolution arithmetic for deep learning&lt;/a&gt;]&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@_init_/an-illustrated-explanation-of-performing-2d-convolutions-using-matrix-multiplications-1e8de8cd2544&#34; target=&#34;_blank&#34;&gt;An illustrated explanation of performing 2D convolution using matrix multiplication&lt;/a&gt;. Un esempio illustrativo anche su &lt;a href=&#34;https://www.slideshare.net/EdwinEfranJimnezLepe/convolution-as-matrix-multiplication?from_action=save&#34; target=&#34;_blank&#34;&gt;slideshare&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/datadriveninvestor/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4&#34; target=&#34;_blank&#34;&gt;Visualizing neural networks using saliency maps&lt;/a&gt;. Reference paper su &lt;a href=&#34;https://arxiv.org/abs/1312.6034&#34; target=&#34;_blank&#34;&gt;arxiv&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Saliency maps in &lt;a href=&#34;[https://mc.ai/feature-visualisation-in-pytorch%E2%80%8A-%E2%80%8Asaliency-maps/](https://mc.ai/feature-visualisation-in-pytorch - saliency-maps/)&#34; target=&#34;_blank&#34;&gt;Flashtorch&lt;/a&gt;. Medium &lt;a href=&#34;https://towardsdatascience.com/feature-visualisation-in-pytorch-saliency-maps-a3f99d08f78a&#34; target=&#34;_blank&#34;&gt;description&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A collection of &lt;a href=&#34;https://github.com/utkuozbulak/pytorch-cnn-visualizations&#34; target=&#34;_blank&#34;&gt;Deep network visualization techniques&lt;/a&gt; in Pytorch&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Object Detection</title>
      <link>https://gmanco.github.io/courses/computervision/lecture7/</link>
      <pubDate>Tue, 28 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture7/</guid>
      <description>

&lt;p&gt;Object detection.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Region Proposal Networks: R-CNN, Fast R-CNN, Faster R-CNN.&lt;/li&gt;
&lt;li&gt;Single Shot Detectors: SSD, RetinaNet, YOLO.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Materiale didattico:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../pdf/8.Object_detection.pdf&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/8a.Object_Detection.ipynb&#34; target=&#34;_blank&#34;&gt;Notebook&lt;/a&gt; di accompagnamento&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/8b.YOLO_demo.ipynb&#34; target=&#34;_blank&#34;&gt;Demo YOLO&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, Ch. 7&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@fractaldle/brief-overview-on-object-detection-algorithms-ec516929be93&#34; target=&#34;_blank&#34;&gt;An overview of Deep-Learning based Object Detection algorithms&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1311.2524.pdf&#34; target=&#34;_blank&#34;&gt;Rich Feature-Based hierarchies for accurate object detection and semantic segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1504.08083.pdf&#34; target=&#34;_blank&#34;&gt;Fast R-CNN&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1506.01497&#34; target=&#34;_blank&#34;&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Faster R-CNN tutorials: &lt;a href=&#34;https://medium.com/@fractaldle/guide-to-build-faster-rcnn-in-pytorch-95b10c273439&#34; target=&#34;_blank&#34;&gt;A guide to building Faster R-CNN in Pytorch&lt;/a&gt; e &lt;a href=&#34;https://towardsdatascience.com/fasterrcnn-explained-part-1-with-code-599c16568cff&#34; target=&#34;_blank&#34;&gt;Detecting objects in (almost) Real time&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1512.02325&#34; target=&#34;_blank&#34;&gt;SSD: Single Shot MultiBox Detector&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection&#34; target=&#34;_blank&#34;&gt;SSD Tutorial&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/detectron&#34; target=&#34;_blank&#34;&gt;Detectron&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/detectron2&#34; target=&#34;_blank&#34;&gt;Detectron2&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;You Only Look Once: &lt;a href=&#34;https://arxiv.org/pdf/1506.02640.pdf&#34; target=&#34;_blank&#34;&gt;YOLOv1&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1612.08242.pdf&#34; target=&#34;_blank&#34;&gt;YOLOv2&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1804.02767.pdf&#34; target=&#34;_blank&#34;&gt;YOLOv3&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://christopher5106.github.io/object/detectors/2017/08/10/bounding-box-object-detectors-understanding-yolo.html&#34; target=&#34;_blank&#34;&gt;Understanding YOLO&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b&#34; target=&#34;_blank&#34;&gt;What&amp;rsquo;s new in YOLOv3?&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@jonathan_hui/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359&#34; target=&#34;_blank&#34;&gt;Speed and accuracy comparison in object detection&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.10934&#34; target=&#34;_blank&#34;&gt;YOLOv4&lt;/a&gt; paper (with &lt;a href=&#34;https://github.com/AlexeyAB/darknet&#34; target=&#34;_blank&#34;&gt;code&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@riteshkanjee/yolov4-superior-faster-more-accurate-object-detection-7e8194bf1872&#34; target=&#34;_blank&#34;&gt;YOLOv4 -  Superior, faster and more accurate&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@michaelchan_2146/faster-real-time-object-detection-yolov4-in-pytorch-6eef8436ba75&#34; target=&#34;_blank&#34;&gt;YOLOv4 in Pytorch&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1708.02002&#34; target=&#34;_blank&#34;&gt;Focal Loss for Dense Object Detection&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4&#34; target=&#34;_blank&#34;&gt;Review - RetinaNet&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://blog.zenggyu.com/en/post/2018-12-05/retinanet-explained-and-demystified/#fn3&#34; target=&#34;_blank&#34;&gt;RetinaNet explained and demystified&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@lekorotkov/5-tools-to-create-a-custom-object-detection-dataset-27ca37f91e05&#34; target=&#34;_blank&#34;&gt;5 tools to create a custom object detection dataset&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;NEW&lt;/strong&gt; &lt;a href=&#34;https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/&#34; target=&#34;_blank&#34;&gt;DETR: End-to-end object detection with Transformers&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>CNN Architectures</title>
      <link>https://gmanco.github.io/courses/computervision/lecture_lab3/</link>
      <pubDate>Fri, 24 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture_lab3/</guid>
      <description>

&lt;p&gt;Principali argomenti trattati:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AlexNet&lt;/li&gt;
&lt;li&gt;VGG&lt;/li&gt;
&lt;li&gt;Residual Network&lt;/li&gt;
&lt;li&gt;Inception Network&lt;/li&gt;
&lt;li&gt;Data Augmentation&lt;/li&gt;
&lt;li&gt;Transfer Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La lezione è corredata di &lt;a href=&#34;../pdf/7.Reti neurali_parte3.pdf&#34;&gt;slides&lt;/a&gt; e di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/labs_lecture/lab03&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Importante&lt;/strong&gt;
Il secondo esonero riguarderà approfondimenti sulle reti convoluzionali. I dettagli dell&amp;rsquo;esonero sono pubblicati &lt;a href=&#34;https://gmanco.github.io/courses/computervision/esoneri/convolution/&#34; target=&#34;_blank&#34;&gt;nell&amp;rsquo;apposita pagina&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 15.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, ch. 4.6, 5, 6.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Yann LeCun&amp;rsquo;s page on &lt;a href=&#34;http://yann.lecun.com/exdb/lenet/&#34; target=&#34;_blank&#34;&gt;LeNet&lt;/a&gt; (with papers and description).&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&#34; target=&#34;_blank&#34;&gt;ImageNet Classification with Deep Convolutional Neural Networks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/difference-between-local-response-normalization-and-batch-normalization-272308c034ac&#34; target=&#34;_blank&#34;&gt;Local Response Norm vs. Bach Norm&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1409.1556v6.pdf&#34; target=&#34;_blank&#34;&gt;Very Deep Convolutional Networks for Large Scale Image Recognition&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43022.pdf&#34; target=&#34;_blank&#34;&gt;Going Deeper with Convolutions&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202&#34; target=&#34;_blank&#34;&gt;A simple guide to the versions of Inception Networks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1502.03167&#34; target=&#34;_blank&#34;&gt;Batch normalization: accelerating deep network training by reducing internal covariate shift&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1512.03385.pdf&#34; target=&#34;_blank&#34;&gt;Residual Learning for image Recognition&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1411.1792.pdf&#34; target=&#34;_blank&#34;&gt;How transferable are features in deep neural networks?&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/torchvision/models.html&#34; target=&#34;_blank&#34;&gt;Torch pretrained models&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/hub/&#34; target=&#34;_blank&#34;&gt;TorchHUB&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.vision.caltech.edu/Image_Datasets/Caltech101/&#34; target=&#34;_blank&#34;&gt;Caltech101 dataset&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://link.springer.com/article/10.1186/s40537-019-0197-0&#34; target=&#34;_blank&#34;&gt;Survey on data augmentation technique&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Object Detection</title>
      <link>https://gmanco.github.io/courses/computervision/lecture_lab4/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture_lab4/</guid>
      <description>

&lt;p&gt;Principali argomenti trattati:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Anchor box&lt;/li&gt;
&lt;li&gt;SSD&lt;/li&gt;
&lt;li&gt;YOLOv3&lt;/li&gt;
&lt;li&gt;RetinaNet&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La lezione è corredata di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/labs_lecture/lab04&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, ch. 7.&lt;/li&gt;
&lt;li&gt;Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y.,  &amp;amp; Berg, A. C. (2016). Ssd: single shot multibox detector. &lt;em&gt;European conference on computer vision&lt;/em&gt; (pp. 21–37).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection&#34; target=&#34;_blank&#34;&gt;SSD tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/andreaazzini/retinanet.pytorch&#34; target=&#34;_blank&#34;&gt;RetinaNet in pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1708.02002&#34; target=&#34;_blank&#34;&gt;Retinanet Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.10934&#34; target=&#34;_blank&#34;&gt;YOLOv4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/retinanet-how-focal-loss-fixes-single-shot-detection-cb320e3bb0de&#34; target=&#34;_blank&#34;&gt;FocalLoss tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/&#34; target=&#34;_blank&#34;&gt;YOLOv3 example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Segmentation</title>
      <link>https://gmanco.github.io/courses/computervision/lecture8/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture8/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Segmentation.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Approcci classici. Conditional Random Fields. &lt;a href=&#34;../pdf/9a.Segmentation_part1.pdf&#34;&gt;slides&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Approcci Neurali. Semantic Segmentation. Instance Segmentation. &lt;a href=&#34;../pdf/9b.Segmentation_part2.pdf&#34;&gt;slides&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/9.Segmentation.ipynb&#34; target=&#34;_blank&#34;&gt;Notebook&lt;/a&gt; di accompagnamento.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, Ch. 5&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, Ch.11&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch.10&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf&#34; target=&#34;_blank&#34;&gt;An Introduction to Conditional Random Fields&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.nowozin.net/sebastian/papers/nowozin2011structured-tutorial.pdf&#34; target=&#34;_blank&#34;&gt;Structured Learning and Prediction in Computer Vision&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.csd.uwo.ca/~yboykov/Papers/pami04.pdf&#34; target=&#34;_blank&#34;&gt;An experimental Comparison of Min-cut/Max Flow algorithms for Energy Minimization in Vision&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1210.5644.pdf&#34; target=&#34;_blank&#34;&gt;Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials&lt;/a&gt; (with &lt;a href=&#34;http://vladlen.info/papers/densecrf-supplementary.pdf&#34; target=&#34;_blank&#34;&gt;Supplemental Material&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/conditional-random-fields-explained-e5b8256da776&#34; target=&#34;_blank&#34;&gt;Conditional Random Fields explained&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/&#34; target=&#34;_blank&#34;&gt;Introduction to Conditional Random Fields&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.topbots.com/semantic-segmentation-guide/&#34; target=&#34;_blank&#34;&gt;A simple guide to semantic Segmentation&lt;/a&gt; and &lt;a href=&#34;https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc&#34; target=&#34;_blank&#34;&gt;A 2019 guide to semantic segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1605.06211&#34; target=&#34;_blank&#34;&gt;Fully Convolutional Networks for Semantic Segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1&#34; target=&#34;_blank&#34;&gt;Review - FCN&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/activating-robotic-minds/up-sampling-with-transposed-convolution-9ae4f2df52d0&#34; target=&#34;_blank&#34;&gt;Upsampling with Transposed Convolution&lt;/a&gt;; &lt;a href=&#34;https://towardsdatascience.com/transposed-convolution-demystified-84ca81b4baba&#34; target=&#34;_blank&#34;&gt;Transposed Convolution demystified&lt;/a&gt;; &lt;a href=&#34;https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8&#34; target=&#34;_blank&#34;&gt;Transposed convolutions explained with Excel&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://distill.pub/2016/deconv-checkerboard/&#34; target=&#34;_blank&#34;&gt;Deconvolution and checkerboard artifacts&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://cvlab.postech.ac.kr/research/deconvnet/&#34; target=&#34;_blank&#34;&gt;Learning Deconvolution Network for Semantic Segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/1511.00561&#34; target=&#34;_blank&#34;&gt;SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation&lt;/a&gt;]&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1505.04597.pdf&#34; target=&#34;_blank&#34;&gt;U-Net: Convolutional Networks for Biomedical Image Segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.09326&#34; target=&#34;_blank&#34;&gt;The One Hundred Layers Tiramisu&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1612.01105.pdf&#34; target=&#34;_blank&#34;&gt;Pyramid scene parsing Networks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1606.00915.pdf&#34; target=&#34;_blank&#34;&gt;DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1511.07122&#34; target=&#34;_blank&#34;&gt;Multiscale Context aggregation by dilated convolutions&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.05587.pdf&#34; target=&#34;_blank&#34;&gt;Rethinking Atrous Convolution for Semantic Image Segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Review: &lt;a href=&#34;https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d&#34; target=&#34;_blank&#34;&gt;DeepLabv2&lt;/a&gt; e &lt;a href=&#34;https://towardsdatascience.com/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74&#34; target=&#34;_blank&#34;&gt;DeepLabv3&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://openreview.net/pdf?id=S1uHiFyyg&#34; target=&#34;_blank&#34;&gt;Speeding up Semantic segmentation for autonomous driving&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1606.02147.pdf&#34; target=&#34;_blank&#34;&gt;ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;ICNet for Real-Time Semantic Segmentation on High-Resolution Images&#34; target=&#34;_blank&#34;&gt;ICNet for Real-Time Semantic Segmentation on High-Resolution Images&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1902.04502&#34; target=&#34;_blank&#34;&gt;Fast-SCNN: Fast Semantic Segmentation Network&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://openaccess.thecvf.com/content_CVPR_2019/html/Li_DFANet_Deep_Feature_Aggregation_for_Real-Time_Semantic_Segmentation_CVPR_2019_paper.html&#34; target=&#34;_blank&#34;&gt;DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1703.06870&#34; target=&#34;_blank&#34;&gt;Mask R-CNN&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@jonathan_hui/image-segmentation-with-mask-r-cnn-ebe6d793272&#34; target=&#34;_blank&#34;&gt;Image Segmentation with Mask R-CNN&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1611.07709.pdf&#34; target=&#34;_blank&#34;&gt;Fully Convolutional Instance-aware Semantic Segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1904.02689.pdf&#34; target=&#34;_blank&#34;&gt;YOLACT&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1912.06218&#34; target=&#34;_blank&#34;&gt;YOLOACT++&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1801.00868.pdf&#34; target=&#34;_blank&#34;&gt;Pantropic Segmentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Action Recognition</title>
      <link>https://gmanco.github.io/courses/computervision/lecture9/</link>
      <pubDate>Tue, 19 May 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture9/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Action Recognition.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;3D Convolution. Recurrent Networks. Optical Flow. &lt;a href=&#34;../pdf/10.Action_recognition.pdf&#34;&gt;slides&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/10.action_recognition.ipynb&#34; target=&#34;_blank&#34;&gt;Notebook&lt;/a&gt; di accompagnamento.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Sze11]&lt;/strong&gt;, Ch. 8&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, Ch.20&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;•https://www.pyimagesearch.com/2019/11/25/human-activity-recognition-with-opencv-and-deep-learning/&#34; target=&#34;_blank&#34;&gt;Deep Learning for Action Recognition: A Review&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/pytorch-step-by-step-implementation-3d-convolution-neural-network-8bf38c70e8b3&#34; target=&#34;_blank&#34;&gt;3D Convolution in Pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1412.0767.pdf&#34; target=&#34;_blank&#34;&gt;Learning Spatiotemporal Features with 3D Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1705.07750&#34; target=&#34;_blank&#34;&gt;Quo vadis, action recognition? a new model and the kinetics dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dbs.ifi.lmu.de/~yu_k/icml2010_3dcnn.pdf&#34; target=&#34;_blank&#34;&gt;3d convolutional neural networks for human action recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://static.googleusercontent.com/media/research.google.com/it//pubs/archive/42455.pdf&#34; target=&#34;_blank&#34;&gt;Large-Scale Video Classification with Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1711.10305&#34; target=&#34;_blank&#34;&gt;Learning spatio-temporal representation with pseudo-3d residual networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1712.04851.pdf&#34; target=&#34;_blank&#34;&gt;Rethinking Spatiotemporal Feature Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://papers.nips.cc/paper/6433-spatiotemporal-residual-networks-for-video-action-recognition.pdf&#34; target=&#34;_blank&#34;&gt;Spatiotemporal residual networks for video action recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1812.05038&#34; target=&#34;_blank&#34;&gt;Long-Term Feature Banks for Detailed Video Understanding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1705.06950&#34; target=&#34;_blank&#34;&gt;The Kinetics Human Action Recognition Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1411.4389&#34; target=&#34;_blank&#34;&gt;Long-term Recurrent Convolutional Networks for Visual Recognition and Description&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1511.04119.pdf&#34; target=&#34;_blank&#34;&gt;Action Recognition Using Visual Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;Recurrent Tubelet Proposal and Recognition Networks for Action Detection&#34; target=&#34;_blank&#34;&gt;Recurrent Tubelet Proposal and Recognition Networks for Action Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1607.01794.pdf&#34; target=&#34;_blank&#34;&gt;VideoLSTM Convolves, Attends and Flows for Action Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1503.08909.pdf&#34; target=&#34;_blank&#34;&gt;Beyond Short Snippets: Deep Networks for Video Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pdfs.semanticscholar.org/51fe/a461cf3724123c888cb9184474e176c12e61.pdf&#34; target=&#34;_blank&#34;&gt;An Iterative Image Registration Technique with an Application to Stero Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf&#34; target=&#34;_blank&#34;&gt;Two-Frame Motion Estimation with Polynomial Expansion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nanonets.com/blog/optical-flow/&#34; target=&#34;_blank&#34;&gt;Introduction to Motion Estimation with Optical Flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.toronto.edu/~fleet/research/Papers/flowChapter05.pdf&#34; target=&#34;_blank&#34;&gt;Optical Flow Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1504.06852&#34; target=&#34;_blank&#34;&gt;FlowNet: Learning Optical Flow with Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1406.2199.pdf&#34; target=&#34;_blank&#34;&gt;Two-Stream Convolutional Networks for Action Recognition in Videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;•https://medium.com/swlh/what-is-optical-flow-and-why-does-it-matter-in-deep-learning-b3278bb205b5&#34; target=&#34;_blank&#34;&gt;What is optical flow and why does it matter in deep learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=tIwpDuqJqcE&#34; target=&#34;_blank&#34;&gt;Michael Black&amp;rsquo;s lecture on Optical Flow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Modelli Generativi</title>
      <link>https://gmanco.github.io/courses/computervision/lecture10/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture10/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;Modelli Generativi: Variational Autoencoders.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../pdf/11.Generative_models_VAE.pdf&#34;&gt;Slides&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/11.Generative_models.ipynb&#34; target=&#34;_blank&#34;&gt;Notebook&lt;/a&gt; di accompagnamento.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1312.6114&#34; target=&#34;_blank&#34;&gt;Autoencoding Variational Bayes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1401.4082&#34; target=&#34;_blank&#34;&gt;Stochastic Backpropagation and Approximate Inference in Deep Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jaan.io/what-is-variational-autoencoder-vae-tutorial/&#34; target=&#34;_blank&#34;&gt;What is a Variational Autoencoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://papers.nips.cc/paper/6528-variational-autoencoder-for-deep-learning-of-images-labels-and-captions.pdf&#34; target=&#34;_blank&#34;&gt;Variational Autoencoder for Deep Learning of Images, Labels and Captions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1911.07389.pdf&#34; target=&#34;_blank&#34;&gt;Towards Visually Explaining Variational Autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73&#34; target=&#34;_blank&#34;&gt;Understanding VAEs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1606.04934&#34; target=&#34;_blank&#34;&gt;Improved Variational Inference with Inverse Autoregressive Flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gmanco.github.io/post/on-semisupervised-vae/&#34; target=&#34;_blank&#34;&gt;Semi-supervised Variational Autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://deepgenerativemodels.github.io/&#34; target=&#34;_blank&#34;&gt;Deep Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openai.com/blog/generative-models/&#34; target=&#34;_blank&#34;&gt;Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openreview.net/pdf?id=Sy2fzU9gl&#34; target=&#34;_blank&#34;&gt;$\beta$-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Generative Adversarial Networks</title>
      <link>https://gmanco.github.io/courses/computervision/lecture11/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture11/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;Modelli Generativi: Adversarial Learning.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../pdf/12.Generative_models_GAN.pdf&#34;&gt;Slides&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/12.GAN.ipynb&#34; target=&#34;_blank&#34;&gt;Notebook&lt;/a&gt; di accompagnamento.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, Ch. 8, 9.3.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://deepgenerativemodels.github.io/&#34; target=&#34;_blank&#34;&gt;Deep Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1406.2661&#34; target=&#34;_blank&#34;&gt;Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://danieltakeshi.github.io/2017/03/05/understanding-generative-adversarial-networks/&#34; target=&#34;_blank&#34;&gt;Understanding Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1606.03498.pdf&#34; target=&#34;_blank&#34;&gt;Improved Techniques for training GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1511.05101&#34; target=&#34;_blank&#34;&gt;How (not) to train your Generative Model: Scheduled Sampling, Likelihood, Adversary?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1706.08500&#34; target=&#34;_blank&#34;&gt;GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.08224v2.pdf&#34; target=&#34;_blank&#34;&gt;Do GAN actually Learn the Distribution?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.10196.pdf&#34; target=&#34;_blank&#34;&gt;Progressive Growing of GANs for Improved Quality, Stability and Variation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1711.10337&#34; target=&#34;_blank&#34;&gt;Are GANs Created Equal? A Large-Scale Study&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1701.07875.pdf&#34; target=&#34;_blank&#34;&gt;Wasserstein GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wasserstein GANs: Blog posts &lt;a href=&#34;https://medium.com/@jonathan_hui/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://www.depthfirstlearning.com/2019/WassersteinGAN&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;, &lt;a href=&#34;https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html&#34; target=&#34;_blank&#34;&gt;3&lt;/a&gt;, &lt;a href=&#34;http://lernapparat.de/improved-wasserstein-gan/&#34; target=&#34;_blank&#34;&gt;4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans&#34; target=&#34;_blank&#34;&gt;Improved training fo Wasserstein GANs&lt;/a&gt; (with some &lt;a href=&#34;https://github.com/t-vi/pytorch-tvmisc/blob/master/wasserstein-distance/Semi-Improved_Training_of_Wasserstein_GAN.ipynb&#34; target=&#34;_blank&#34;&gt;tricks&lt;/a&gt; on how to implement it in Pytorch)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/znxlwm/pytorch-generative-model-collections&#34; target=&#34;_blank&#34;&gt;A collection of Generative Models in Pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/soumith/ganhacks&#34; target=&#34;_blank&#34;&gt;How to train a GAN?&lt;/a&gt; (&lt;a href=&#34;https://www.youtube.com/watch?v=Qc1F3-Rblbw&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1903.06048.pdf&#34; target=&#34;_blank&#34;&gt;MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1411.1784&#34; target=&#34;_blank&#34;&gt;Conditional Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.07004&#34; target=&#34;_blank&#34;&gt;Image to image translation with Conditional Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/a-gentle-introduction-to-cycle-consistent-adversarial-networks-6731c8424a87&#34; target=&#34;_blank&#34;&gt;A gentle introduction to CycleGAN for Image Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1703.10593&#34; target=&#34;_blank&#34;&gt;Unpaired Image-to-image Translation using Cycle-Consistent Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1807.00374.pdf&#34; target=&#34;_blank&#34;&gt;Augmented Cyclic Adversarial Learning for Low Resource Domain Adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hardikbansal.github.io/CycleGANBlog/&#34; target=&#34;_blank&#34;&gt;Understanding and Implementing CycleGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1610.09585.pdf&#34; target=&#34;_blank&#34;&gt;Conditional Image Synthesis with Auxiliary Classifier GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1803.04469.pdf&#34; target=&#34;_blank&#34;&gt;An Introduction to Image Synthesis with Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1809.11096.pdf&#34; target=&#34;_blank&#34;&gt;Large-Scale GAN Training for High-Fidelity Natural Image Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1809.06222.pdf&#34; target=&#34;_blank&#34;&gt;GANs for Medical Image Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hindupuravinash/the-gan-zoo&#34; target=&#34;_blank&#34;&gt;The GAN Zoo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/c/deepfake-detection-challenge/discussion/121313&#34; target=&#34;_blank&#34;&gt;GAN and Deepfakes resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@jonathan_hui/how-deep-learning-fakes-videos-deepfakes-and-how-to-detect-it-c0b50fbf7cb9&#34; target=&#34;_blank&#34;&gt;How deep learning fakes videos and how to detect it&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/deepfakes-the-ugly-and-the-good-49115643d8dd&#34; target=&#34;_blank&#34;&gt;Deepfakes: The ugly, and the good&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://neurohive.io/en/news/deepfake-videos-gan-sythesizes-a-video-from-a-single-photo/&#34; target=&#34;_blank&#34;&gt;Deepfake Videos: GAN synthesizes a Video from a Single Photo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Neural Style Transfer &lt;a href=&#34;[https://towardsdatascience.com/neural-style-transfer-tutorial-part-1-f5cd3315fa7f&#34; target=&#34;_blank&#34;&gt;Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1508.06576.pdf&#34; target=&#34;_blank&#34;&gt;A Neural algorithm of Artistic Style&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3390462&#34; target=&#34;_blank&#34;&gt;A Deep Journey into Superresolution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>CycleGAN and DeepFake</title>
      <link>https://gmanco.github.io/courses/computervision/lecture_lab5/</link>
      <pubDate>Thu, 23 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture_lab5/</guid>
      <description>

&lt;p&gt;Principali argomenti trattati:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CycleGAN&lt;/li&gt;
&lt;li&gt;DeepFake&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La lezione è corredata di slide disponibili &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/labs_lecture/lab07&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&#34; target=&#34;_blank&#34;&gt;CycleGAN Pytorch implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/image-to-image-translation-using-cyclegan-model-d58cfff04755&#34; target=&#34;_blank&#34;&gt;CycleGAN intro&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jinfagang/faceswap_pytorch&#34; target=&#34;_blank&#34;&gt;DeepFake - face swap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/OValery16/swap-face&#34; target=&#34;_blank&#34;&gt;DeepFake&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Esonero 1</title>
      <link>https://gmanco.github.io/courses/computervision/esoneri/listoffeaturedescriptors/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/esoneri/listoffeaturedescriptors/</guid>
      <description>

&lt;h1 id=&#34;feature-descriptors&#34;&gt;Feature descriptors&lt;/h1&gt;

&lt;p&gt;In questo esonero è chiesto agli studenti di sperimentare con un feature descriptr tra quelli elencati in seguito. In particolare si chiede di:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Scegliere un feature descriptor&lt;/li&gt;
&lt;li&gt;Preparare delle slides dettagliate (max 15) che lo descrivono&lt;/li&gt;
&lt;li&gt;Preparare un notebook in cui:

&lt;ul&gt;
&lt;li&gt;si implementa l&amp;rsquo;algoritmo che sta alla slide 62 (istogramma basato sui feature descriptors)&lt;/li&gt;
&lt;li&gt;si applica la regressione logistica (da scikit-learn) sul set di immagini (MNIST)&lt;/li&gt;
&lt;li&gt;si valutano i risultati della classificazione confrontandoli con la regressione logistica applicata alla flattenizzazione raw dell&amp;rsquo;immagine (opportunamente preprocessata).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Gli studenti interessati ad effettuare l&amp;rsquo;esonero dovranno mandare una mail al docente indicando, in ordine di priorità, tre scelte dalla lista di cui sotto.
Le assegnazioni verranno comunicate a lezione. La deadline per la consegna degli elaborati è il &lt;strong&gt;16 aprile 2020&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;opencv&#34;&gt;OpenCV&lt;/h2&gt;

&lt;p&gt;Tutorials disponibili &lt;a href=&#34;https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_table_of_contents_feature2d/py_table_of_contents_feature2d.html&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/4.2.0/d5/d3c/classcv_1_1xfeatures2d_1_1SIFT.html&#34; target=&#34;_blank&#34;&gt;SIFT&lt;/a&gt;, Class for extracting keypoints and computing descriptors using the Scale Invariant Feature Transform (&lt;a href=&#34;https://docs.opencv.org/4.2.0/d5/d3c/classcv_1_1xfeatures2d_1_1SIFT.html&#34; target=&#34;_blank&#34;&gt;SIFT&lt;/a&gt;) algorithm by D. Lowe. Riferimenti: David G Lowe. &lt;a href=&#34;https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&#34; target=&#34;_blank&#34;&gt;Distinctive image features from scale-invariant keypoints&lt;/a&gt;. &lt;em&gt;International journal of computer vision&lt;/em&gt;, 60(2):91–110, 2004.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/4.2.0/d5/df7/classcv_1_1xfeatures2d_1_1SURF.html&#34; target=&#34;_blank&#34;&gt;SURF&lt;/a&gt;, Class for extracting Speeded Up Robust Features from an image. Riferimenti: Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. &lt;a href=&#34;http://scholar.google.it/scholar_url?url=https://lirias.kuleuven.be/retrieve/78517&amp;amp;hl=it&amp;amp;sa=X&amp;amp;scisig=AAGBfm2HNoDAKFbI70IQ22ndV5cBLu7pBw&amp;amp;nossl=1&amp;amp;oi=scholarr&#34; target=&#34;_blank&#34;&gt;Surf: Speeded up robust features&lt;/a&gt;. &lt;em&gt;Computer Vision–ECCV 2006&lt;/em&gt;, pages 404–417, 2006.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/4.2.0/d8/d30/classcv_1_1AKAZE.html&#34; target=&#34;_blank&#34;&gt;AKAZE&lt;/a&gt;, Class implementing the &lt;a href=&#34;https://docs.opencv.org/4.2.0/d8/d30/classcv_1_1AKAZE.html&#34; target=&#34;_blank&#34;&gt;AKAZE&lt;/a&gt; keypoint detector and descriptor extractor. Riferimenti: Pablo F Alcantarilla, Jesús Nuevo, and Adrien Bartoli. &lt;a href=&#34;http://www.bmva.org/bmvc/2013/Papers/paper0013/paper0013.pdf&#34; target=&#34;_blank&#34;&gt;Fast explicit diffusion for accelerated features in nonlinear scale spaces&lt;/a&gt;. &lt;em&gt;Trans. Pattern Anal. Machine Intell&lt;/em&gt;, 34(7):1281–1298, 2011.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/4.2.0/de/dbf/classcv_1_1BRISK.html&#34; target=&#34;_blank&#34;&gt;Brisk&lt;/a&gt;, Class implementing the &lt;a href=&#34;https://docs.opencv.org/4.2.0/de/dbf/classcv_1_1BRISK.html&#34; target=&#34;_blank&#34;&gt;BRISK&lt;/a&gt; keypoint detector and descriptor extractor. [Stefan Leutenegger, Margarita Chli, and Roland Yves Siegwart. &lt;a href=&#34;https://www.researchgate.net/publication/221110715_BRISK_Binary_Robust_invariant_scalable_keypoints&#34; target=&#34;_blank&#34;&gt;Brisk: Binary robust invariant scalable keypoints&lt;/a&gt;. In &lt;em&gt;Computer Vision (ICCV), 2011 IEEE International Conference on&lt;/em&gt;, pages 2548–2555. IEEE, 2011.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/4.2.0/d3/d28/classcv_1_1MSER.html&#34; target=&#34;_blank&#34;&gt;Mser&lt;/a&gt;, Maximally stable extremal region extractor, for grey scale and color image. Riferimenti: David Nistér and Henrik Stewénius. &lt;a href=&#34;https://www.researchgate.net/publication/221304597_Linear_Time_Maximally_Stable_Extremal_Regions&#34; target=&#34;_blank&#34;&gt;Linear time maximally stable extremal regions&lt;/a&gt;. In &lt;em&gt;Computer Vision–ECCV 2008&lt;/em&gt;, pages 183–196. Springer, 2008.; Per-Erik Forssén. Maximally stable colour regions for recognition and matching. In &lt;em&gt;Computer Vision and Pattern Recognition, 2007. CVPR&amp;rsquo;07. IEEE Conference on&lt;/em&gt;, pages 1–8. IEEE, 2007.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.opencv.org/4.2.0/db/d95/classcv_1_1ORB.html&#34; target=&#34;_blank&#34;&gt;ORB&lt;/a&gt;, The algorithm uses FAST in pyramids to detect stable keypoints, selects  the strongest features using FAST or Harris response, finds their  orientation using first-order moments and computes the descriptors using BRIEF (where the coordinates of random point pairs (or k-tuples) are  rotated according to the measured orientation). Riferimenti: Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski. &lt;a href=&#34;http://www.willowgarage.com/sites/default/files/orb_final.pdf&#34; target=&#34;_blank&#34;&gt;Orb: an efficient alternative to sift or surf&lt;/a&gt;. In &lt;em&gt;Computer Vision (ICCV), 2011 IEEE International Conference on&lt;/em&gt;, pages 2564–2571. IEEE, 2011.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;sk-image&#34;&gt;SK-Image&lt;/h2&gt;

&lt;p&gt;Documentation at the home page of the &lt;a href=&#34;https://scikit-image.org/docs/0.16.x/api/skimage.feature.html&#34; target=&#34;_blank&#34;&gt;scikit-image feature description package&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;skimage.feature.daisy&lt;/code&gt;, Extract DAISY feature descriptors densely for the given image. DAISY is a feature descriptor similar to SIFT formulated in a way that allows for fast dense extraction. Typically, this is practical for bag-of-features image representations. Riferimenti: Tola et al. &lt;a href=&#34;https://infoscience.epfl.ch/record/138785/files/tola_daisy_pami_1.pdf&#34; target=&#34;_blank&#34;&gt;Daisy: An efficient dense descriptor applied to wide- baseline stereo&lt;/a&gt;. Pattern Analysis and Machine Intelligence, IEEE Transactions on 32.5 (2010): 815-830.]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;skimage.feature.hog&lt;/code&gt;, Extract Histogram of Oriented Gradients (HOG) for a given image. Riferimenti: Dalal, N and Triggs, B, &lt;a href=&#34;https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf&#34; target=&#34;_blank&#34;&gt;Histograms of Oriented Gradients for Human Detection&lt;/a&gt;, IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2005 San Diego, CA, USA, DOI:10.1109/CVPR.2005.177]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;skimage.feature.local_binary_pattern&lt;/code&gt;, Gray scale and rotation invariant LBP (Local Binary Patterns). Riferimenti: Timo Ojala, Matti Pietikainen, Topi Maenpaa. &lt;a href=&#34;http://www.ee.oulu.fi/research/mvmp/mvg/files/pdf/pdf_94.pdf&#34; target=&#34;_blank&#34;&gt;Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns&lt;/a&gt;. 2002]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;skimage.feature.haar_like_feature&lt;/code&gt;, Compute the Haar-like features for a region of interest (ROI) of an integral image. Riferimenti: Messom, Christopher H. and Andre L. C. Barczak. &lt;a href=&#34;https://www.semanticscholar.org/paper/Stream-processing-for-fast-and-efficient-rotated-Messom-Barczak/b55e215acd9bc0496f1f611d6193fea0b10e4212&#34; target=&#34;_blank&#34;&gt;Stream processing for fast and efficient rotated Haar-like features using rotated integral images&lt;/a&gt;. &lt;em&gt;IJISTA&lt;/em&gt; 7 (2006): 40-57.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;skimage.feature.BRIEF&lt;/code&gt;, BRIEF binary descriptor extractor. BRIEF (Binary Robust Independent Elementary Features) is an efficient feature point descriptor. It is highly discriminative even when using relatively few bits and is computed using simple intensity difference tests. Riferimenti: Calonder, Michael &amp;amp; Lepetit, Vincent &amp;amp; Strecha, Christoph &amp;amp; Fua, Pascal. (2010). &lt;a href=&#34;https://www.cs.ubc.ca/~lowe/525/papers/calonder_eccv10.pdf&#34; target=&#34;_blank&#34;&gt;BRIEF: Binary Robust Independent Elementary Features&lt;/a&gt;. Eur. Conf. Comput. Vis.. 6314. 778-792. 10.&lt;sup&gt;1007&lt;/sup&gt;&amp;frasl;&lt;sub&gt;978&lt;/sub&gt;-3-642-15561-1_56.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;skimage.feature.ORB&lt;/code&gt;, Oriented FAST and rotated BRIEF feature detector and binary descriptor extractor. Riferimenti: Ethan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski. &lt;a href=&#34;http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf&#34; target=&#34;_blank&#34;&gt;ORB: An efficient alternative to SIFT and SURF&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;assegnazioni&#34;&gt;Assegnazioni&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Giulia Katia Galimberti  &lt;strong&gt;SIFT&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Maria Francesca Alati  &lt;strong&gt;skimage.feature.hog&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Lorenzo Defina  &lt;strong&gt;Mser&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Emilio Casella  &lt;strong&gt;Surf&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Simona Nisticò matricola &lt;strong&gt;ORB&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Domenico Montesanto &lt;strong&gt;AKAZE&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Caterina Maugeri &lt;strong&gt;skimage.feature.local_binary_pattern&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Giuseppe Surace  &lt;strong&gt;skimage.feature.haar_like_feature&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Antonello Crea &lt;strong&gt;Daisy&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Anile Anna  &lt;strong&gt;skimage.feature.BRIEF&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Vincenzo Parrilla  &lt;strong&gt;SURF with Harris Corner Detection&lt;/strong&gt;. (OpenCV)&lt;/li&gt;
&lt;li&gt;Davide Medaglia &lt;strong&gt;SIFT with Harris Corner Detection&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Antonio Commisso &lt;strong&gt;Brisk&lt;/strong&gt; (OpenCV)&lt;/li&gt;
&lt;li&gt;Antonio Gagliostro: &lt;strong&gt;skimage.feature.ORB&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;risultati&#34;&gt;Risultati&lt;/h1&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Studente&lt;/th&gt;
&lt;th&gt;Voto&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Alati Maria  Francesca&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Anile Anna&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Casella Emilio&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Commisso Antonio&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Crea Antonello&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Defina Lorenzo&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Gagliostro Antonio&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Galimberti Giulia&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Maugeri Caterina&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Medaglia Davide&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Montesanto Domenico&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Nisticò Simona&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Parrilla Vincenzo&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Surace Giuseppe&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Esonero 2</title>
      <link>https://gmanco.github.io/courses/computervision/esoneri/convolution/</link>
      <pubDate>Tue, 28 Apr 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/esoneri/convolution/</guid>
      <description>

&lt;h1 id=&#34;convolutional-neural-networks&#34;&gt;Convolutional Neural Networks&lt;/h1&gt;

&lt;p&gt;In questo esonero è chiesto agli studenti di studiare le reti convoluzionali.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Scegliere un topic&lt;/li&gt;
&lt;li&gt;Preparare delle slides dettagliate (max 15) che lo descrivono&lt;/li&gt;
&lt;li&gt;Preparare un notebook in cui:

&lt;ul&gt;
&lt;li&gt;si implementa quello che è richiesto&lt;/li&gt;
&lt;li&gt;si illustrano i  risultati e la loro valutazione&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Gli studenti interessati ad effettuare l&amp;rsquo;esonero dovranno mandare una mail al docente indicando, in ordine di priorità, tre scelte dalla lista di cui sotto.
Le assegnazioni verranno comunicate a lezione. La deadline per la consegna degli elaborati è il &lt;strong&gt;10 maggio 2020&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;googlenet&#34;&gt;GoogLeNet&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Scegliere una delle seguenti varianti di GoogLeNet e implementarla, riaddestrando la rete su CIFAR-10 e mostrando i risultati.

&lt;ul&gt;
&lt;li&gt;Utilizzare i layer di batch normalization [&lt;a href=&#34;https://d2l.ai/chapter_references/zreferences.html#ioffe-szegedy-2015&#34; target=&#34;_blank&#34;&gt;Ioffe &amp;amp; Szegedy, 2015]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Effettuare gli aggiustamenti all&amp;rsquo;inception block suggeriti in [&lt;a href=&#34;https://d2l.ai/chapter_references/zreferences.html#szegedy-vanhoucke-ioffe-ea-2016&#34; target=&#34;_blank&#34;&gt;Szegedy et al., 2016]&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;effettuare gli aggiustamenti alle residual connections suggerite in [&lt;a href=&#34;https://d2l.ai/chapter_references/zreferences.html#szegedy-ioffe-vanhoucke-ea-2017&#34; target=&#34;_blank&#34;&gt;Szegedy et al., 2017]&lt;/a&gt;,&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Qual&amp;rsquo;è la dimensione minima richiesta per l&amp;rsquo;immagine di input in GoogLeNet?&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;Ioffe, S., &amp;amp; Szegedy, C. (2015). Batch normalization: accelerating deep network training by reducing internal covariate shift. &lt;em&gt;arXiv preprint arXiv:1502.03167&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., &amp;amp; Wojna, Z. (2016). Rethinking the inception architecture for computer vision. &lt;em&gt;Proceedings of the IEEE conference on computer vision and pattern recognition&lt;/em&gt; (pp. 2818–2826).&lt;/li&gt;
&lt;li&gt;Szegedy, C., Ioffe, S., Vanhoucke, V., &amp;amp; Alemi, A. A. (2017). Inception-v4, inception-resnet and the impact of residual connections on learning. &lt;em&gt;Thirty-First AAAI Conference on Artificial Intelligence&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;resnet&#34;&gt;ResNet&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Nelle versioni successive di ResNet, gli autori hanno cambiato l&amp;rsquo;architettura da “convolution, batch normalization, activation” in “batch normalization, activation, convolution”. Studia gli effetti di questo cambiamento nell&amp;rsquo;implementazione proposta in classe, ristrutturando la rete e riaddestrandola su CIFAR-10. [&lt;a href=&#34;https://arxiv.org/abs/1603.05027&#34; target=&#34;_blank&#34;&gt;He et al., 2016&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Qual&amp;rsquo;è la dimensione minima richiesta per l&amp;rsquo;immagine di input in ResNet ?&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;He, K., Zhang, X., Ren, S., &amp;amp; Sun, J. (2016). Identity mappings in deep residual networks. European conference on computer vision (pp. 630–645).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;saliency-maps-using-flashtorch&#34;&gt;Saliency Maps using Flashtorch&lt;/h2&gt;

&lt;p&gt;Studiare l&amp;rsquo;articolo [&lt;a href=&#34;https://arxiv.org/pdf/1312.6034.pdf&#34; target=&#34;_blank&#34;&gt;Simonyan et al, 2014&lt;/a&gt;] e il package &lt;a href=&#34;https://github.com/MisaOgura/flashtorch&#34; target=&#34;_blank&#34;&gt;flashtorch&lt;/a&gt; e usalo su alcune reti preaddestrate (VGG16, GoogLeNet, ResNet).&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Karen Simonyan, Andrea Vedaldi, Andrew Zisserman. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. ICLR2014&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/MisaOgura/flashtorch/blob/master/examples/visualize_saliency_with_backprop.ipynb&#34; target=&#34;_blank&#34;&gt;Visualize image specific class saliency with backprop&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;alternative-alla-batch-normalization&#34;&gt;Alternative alla Batch Normalization&lt;/h2&gt;

&lt;p&gt;Prendere come riferimento la rete VGG16. Si studino gli effetti della normalizzazione riaddestrando la rete su CIFAR-10:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Aggiungendo Batch Normalization&lt;/li&gt;
&lt;li&gt;Aggiungendo Local Response Normalization&lt;/li&gt;
&lt;li&gt;Implementando la tecnica di Group Normalization descritta in [&lt;a href=&#34;https://arxiv.org/pdf/1803.08494.pdf&#34; target=&#34;_blank&#34;&gt;Wu et al, 2018&lt;/a&gt;]&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;Y. Wu, K. He. Group Normalization.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/an-alternative-to-batch-normalization-2cee9051e8bc&#34; target=&#34;_blank&#34;&gt;An alternative to Batch Normalization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;assegnazioni&#34;&gt;Assegnazioni&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Costa Davide &lt;strong&gt;Alternative alla Batch Normalization&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Azzato Saverio &lt;strong&gt;GoogleLeNet (aggiustamenti all’inception block)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Sergi Alfredo &lt;strong&gt;GoogLeNet (aggiustamenti alle residual connection)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;De Prete Alessandra &lt;strong&gt;Saliency Maps using Flashtorch&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Prospero Papaleo &lt;strong&gt;ResNet&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Lavecchia Umberto Mattia &lt;strong&gt;GoogLeNet (con studio sulla batch normalization)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;risultati&#34;&gt;Risultati&lt;/h1&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Studente&lt;/th&gt;
&lt;th&gt;Voto&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Azzato Saverio&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Costa Davide&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Del Prete Alessandra&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Lavecchia Mattia&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Papaleo Prospero&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Sergi Alfredo&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Progetto 1</title>
      <link>https://gmanco.github.io/courses/computervision/progetti/project1_spec/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/progetti/project1_spec/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;Specifiche&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Action Recognition in ambito sportivo&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Task: individuare 10 tipologie di azioni di basket&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Costruire un modello di classificazione&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Il dataset è composto da 32557 clip (MP4, 16 frame RGB) di azioni di basket. Il dataset è suddiviso in train (22779) e test (9778)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Il test set è utilizzato per la valutazione, le clip (DA NON UTILIZZARE NEL TRAINING) sono definite in un file allegato a queste slide.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tipologia di azioni possibili (10 classi)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Download: &lt;a href=&#34;https://drive.google.com/open?id=1hLpbLmLFK2-GIvsmpJelGlEx94yQM2Ts&#34; target=&#34;_blank&#34;&gt;https://drive.google.com/open?id=1hLpbLmLFK2-GIvsmpJelGlEx94yQM2Ts&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;requisiti&#34;&gt;Requisiti&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Definire e addestrare un modello di classificazione&lt;/li&gt;
&lt;li&gt;Descrivere in una relazione/presentazione le scelte progettuali e tutti i parametri utilizzati nella sperimentazione.&lt;/li&gt;
&lt;li&gt;Consegnare notebook (e/o file sorgente), relazione e dump del modello&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;protocollo-di-valutazione&#34;&gt;Protocollo di valutazione&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Valutazione generale del progetto&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Loading del modello e calcolo delle precisione media sul test set&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Baseline-1 - 8 pt – Valore 0.40 Baseline-2 - 7 pt – Valore 0.60&lt;/p&gt;

&lt;p&gt;Per accedere all’orale sono necessari almeno 8 pt.&lt;/p&gt;

&lt;h3 id=&#34;assegnazione-dei-punteggi&#34;&gt;Assegnazione dei punteggi:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Se il modello supera la Baseline-1: 8 pt&lt;/li&gt;
&lt;li&gt;Se il modello non supera la Baseline-2: (p – Baseline-1)/(Baseline-2 - Baseline-1) * 7 pt&lt;/li&gt;
&lt;li&gt;Se il modello supera la Baseline-2: 7 pt + bonus&lt;/li&gt;
&lt;li&gt;Bonus (p – Baseline-2)/(BESTMODEL - Baseline-2) * 5 pt&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;appelli&#34;&gt;Appelli&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1 – luglio – Progetto attuale&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Consegna entro domenica 28 giugno&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;31 – luglio – Nuovo Progetto (stesse modalità)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rilascio indicativo fine giugno&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Per ulteriori info vedere &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/tree/master/project_1&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Progetto 2</title>
      <link>https://gmanco.github.io/courses/computervision/progetti/project2_spec/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/progetti/project2_spec/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;Specifiche&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Action Recognition in ambito sportivo&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Task: individuare 10 tipologie di azioni di basket&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Costruire un modello di classificazione&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;Si utilizza lo stesso dataset dell&amp;rsquo;appello precedente (&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/project1&#34; target=&#34;_blank&#34;&gt;appello precedente&lt;/a&gt;) con lo stesso protocollo di valutazione e le baseline riportate di seguito.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Principali differenze rispetto all&amp;rsquo;appello precedente:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nuovo test set&lt;/li&gt;
&lt;li&gt;nuovi valori baseline&lt;/li&gt;
&lt;li&gt;oltre al materiale già indicato, è necessario consegnare anche una presentazione da mostrare durante l&amp;rsquo;esame&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;requisiti&#34;&gt;Requisiti&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Definire e addestrare un modello di classificazione&lt;/li&gt;
&lt;li&gt;Descrivere in una presentazione le scelte progettuali e tutti i parametri utilizzati nella sperimentazione.&lt;/li&gt;
&lt;li&gt;Consegnare notebook (e/o file sorgente), &lt;strong&gt;PRESENTAZIONE&lt;/strong&gt; e dump del modello&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;protocollo-di-valutazione&#34;&gt;Protocollo di valutazione&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Valutazione generale del progetto&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Loading del modello e calcolo delle precisione media sul test set&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Baseline-1 - 8 pt – Valore 0.65
Baseline-2 - 7 pt – Valore 0.70

Per accedere all’orale sono necessari almeno 8 pt.

Assegnazione dei punteggi:
Se il modello supera la Baseline-1: 8 pt
Se il modello non supera la Baseline-2: (p – Baseline-1)/(Baseline-2 - Baseline-1) * 7 pt
Se il modello supera la Baseline-2: 7 pt + bonus
Bonus (p – Baseline-2)/(BESTMODEL - Baseline-2) * 5 pt
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Nuovo TestSet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Per costruire il test set utilizzare le chiavi fornite nel file &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/project_2/testset_keys_1lug2020.txt&#34; target=&#34;_blank&#34;&gt;testset_keys_1lug2020.txt&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MD5 (testset_keys_1lug2020.txt) = d66ed18631567fdf3069cbd7fc9e5de1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Appello&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Data Rilascio 01/07/2020&lt;/li&gt;
&lt;li&gt;Data Consegna 29/07/2020 ore 12.00&lt;/li&gt;
&lt;li&gt;Data Appello 31/07/2020&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Per ulteriori info vedere &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/tree/master/project_2&#34; target=&#34;_blank&#34;&gt;https://github.com/gmanco/cv_notebooks/tree/master/project_2&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Progetto 3</title>
      <link>https://gmanco.github.io/courses/computervision/progetti/project3_spec/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/progetti/project3_spec/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;Specifiche&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Action Recognition in ambito sportivo&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Task: individuare 10 tipologie di azioni di basket&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Costruire un modello di classificazione&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;Si utilizza lo stesso dataset dell&amp;rsquo;appello precedente (&lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/project1&#34; target=&#34;_blank&#34;&gt;appello precedente&lt;/a&gt;) con lo stesso protocollo di valutazione e le baseline riportate di seguito.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Principali differenze rispetto all&amp;rsquo;appello precedente:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nuovo test set&lt;/li&gt;
&lt;li&gt;nuovi valori baseline&lt;/li&gt;
&lt;li&gt;oltre al materiale già indicato, è necessario consegnare anche una presentazione da mostrare durante l&amp;rsquo;esame&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;requisiti&#34;&gt;Requisiti&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Definire e addestrare un modello di classificazione&lt;/li&gt;
&lt;li&gt;Descrivere in una presentazione le scelte progettuali e tutti i parametri utilizzati nella sperimentazione.&lt;/li&gt;
&lt;li&gt;Consegnare notebook (e/o file sorgente), &lt;strong&gt;PRESENTAZIONE&lt;/strong&gt; e dump del modello&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;protocollo-di-valutazione&#34;&gt;Protocollo di valutazione&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Valutazione generale del progetto&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Loading del modello e calcolo delle precisione media sul test set&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Baseline-1 - 8 pt – Valore 0.75
Baseline-2 - 7 pt – Valore 0.80

Per accedere all’orale sono necessari almeno 8 pt.

Assegnazione dei punteggi:
Se il modello supera la Baseline-1: 8 pt
Se il modello non supera la Baseline-2: (p – Baseline-1)/(Baseline-2 - Baseline-1) * 7 pt
Se il modello supera la Baseline-2: 7 pt + bonus
Bonus (p – Baseline-2)/(BESTMODEL - Baseline-2) * 5 pt
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Nuovo TestSet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Per costruire il test set utilizzare le chiavi fornite nel file &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/project_2/testset_keys_1lug2020.txt&#34; target=&#34;_blank&#34;&gt;testset_keys_1lug2020.txt&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MD5 (testset_keys_1lug2020.txt) = d66ed18631567fdf3069cbd7fc9e5de1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Appello&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Data Rilascio 31/07/2020&lt;/li&gt;
&lt;li&gt;Data Consegna 01/09/2020 ore 12.00&lt;/li&gt;
&lt;li&gt;Data Appello 03/09/2020 ore 9.00&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Per ulteriori info vedere &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/tree/master/project_3&#34; target=&#34;_blank&#34;&gt;https://github.com/gmanco/cv_notebooks/tree/master/project_3&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

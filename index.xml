<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Giuseppe Manco</title>
    <link>https://gmanco.github.io/</link>
      <atom:link href="https://gmanco.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Giuseppe Manco</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Giuseppe Manco</copyright><lastBuildDate>Thu, 27 Feb 2020 00:00:00 +0100</lastBuildDate>
    <image>
      <url>https://gmanco.github.io/img/manco.jpg</url>
      <title>Giuseppe Manco</title>
      <link>https://gmanco.github.io/</link>
    </image>
    
    <item>
      <title>Introduzione al corso</title>
      <link>https://gmanco.github.io/courses/computervision/lecture1/</link>
      <pubDate>Thu, 27 Feb 2020 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture1/</guid>
      <description>

&lt;p&gt;Introduzione al corso. Image Processing, Analysis e Computer Vision.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../1.Intro.pdf&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 1.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, sect. 1.1-1.3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://algorithmia.com/blog/introduction-to-computer-vision&#34; target=&#34;_blank&#34;&gt;Introduction to Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/what-is-computer-vision/&#34; target=&#34;_blank&#34;&gt;A gentle introduction to Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/readers-writers-digest/beginners-guide-to-computer-vision-23606224b720&#34; target=&#34;_blank&#34;&gt;A Beginner&amp;rsquo;s guide to Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/everything-you-ever-wanted-to-know-about-computer-vision-heres-a-look-why-it-s-so-awesome-e8a58dfb641e&#34; target=&#34;_blank&#34;&gt;Everything you wanted to know about Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.thecvf.com/&#34; target=&#34;_blank&#34;&gt;The Computer Vision Foundation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Concetti fondamentali su Image processing e analysis</title>
      <link>https://gmanco.github.io/courses/computervision/lecture2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture2/</guid>
      <description>

&lt;p&gt;Concetti fondamentali su Image processing e analysis: Image Basics, Manipolazione di immagini. Introduzione alle librerie Python per Image Processing.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../2.Image_fundamentals.pdf&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;La lezione è corredata di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/1.Image_fundamentals.ipynb&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 2.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Elg20]&lt;/strong&gt;, sect. 1.4-1.5.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 2-3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://cs231n.github.io/python-numpy-tutorial/&#34; target=&#34;_blank&#34;&gt;Stanford cs231n Python tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cs231n.github.io/ipython-tutorial/&#34; target=&#34;_blank&#34;&gt;Stanford cs231n Jupyter tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.anaconda.com/&#34; target=&#34;_blank&#34;&gt;Anaconda, la piattaforma di riferimento per installare Python e Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://notebooks.azure.com/&#34; target=&#34;_blank&#34;&gt;Piattaforma azure di Microsoft per l&amp;rsquo;esecuzione di notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/notebooks/intro.ipynb&#34; target=&#34;_blank&#34;&gt;Google Colab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://missing.csail.mit.edu/?fbclid=IwAR0N3s-DbRLrSWW3tB1L5iu_thdiEtFuL7cGUwxyOL-yc7skytSDGdT9ZAo&#34; target=&#34;_blank&#34;&gt;The missing semester, un mini-corso del MIT sui tool di base che tutti i data scientist dovrebbero conoscere&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Filtri</title>
      <link>https://gmanco.github.io/courses/computervision/lecture3/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://gmanco.github.io/courses/computervision/lecture3/</guid>
      <description>

&lt;p&gt;Manipolazione spaziale. Filtri. Convoluzione.&lt;/p&gt;

&lt;p&gt;Slides disponibili &lt;a href=&#34;../3.Filters.pdf&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;La lezione è corredata di un notebook disponibile &lt;a href=&#34;https://github.com/gmanco/cv_notebooks/blob/master/2.Filters.ipynb&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;riferimenti-bibliografici&#34;&gt;Riferimenti bibliografici&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Davies18]&lt;/strong&gt;, ch. 3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Gon18]&lt;/strong&gt;, Ch. 3.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[Solem12]&lt;/strong&gt;, Ch. 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;link-utili&#34;&gt;Link utili&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/7075360&#34; target=&#34;_blank&#34;&gt;Adaptive filters for color image processing: A survey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/image-filters-in-python-26ee938e57d2&#34; target=&#34;_blank&#34;&gt;Image filters in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scipy-lectures.org/advanced/image_processing/&#34; target=&#34;_blank&#34;&gt;Image manipulation using Numpy and SciPy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@enzoftware/how-to-build-amazing-images-filters-with-python-median-filter-sobel-filter-%EF%B8%8F-%EF%B8%8F-22aeb8e2f540&#34; target=&#34;_blank&#34;&gt;How to build an image filter in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=otLGDpBglEA&amp;amp;feature=player_embedded&#34; target=&#34;_blank&#34;&gt;Instagram filters in 15 lines of code&lt;/a&gt;. Il codice è disponibile &lt;a href=&#34;https://github.com/lukexyz/CV-Instagram-Filters/blob/master/gotham.py&#34; target=&#34;_blank&#34;&gt;qui&lt;/a&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Adversarial Games for generative modeling of Temporally-Marked Event Sequences</title>
      <link>https://gmanco.github.io/talk/isi-feb20/</link>
      <pubDate>Tue, 18 Feb 2020 11:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/talk/isi-feb20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Some notes on the Semi-Supervised Learning of Variational Autoencoders</title>
      <link>https://gmanco.github.io/post/on-semisupervised-vae/</link>
      <pubDate>Tue, 10 Sep 2019 18:10:09 +0200</pubDate>
      <guid>https://gmanco.github.io/post/on-semisupervised-vae/</guid>
      <description>&lt;p&gt;In what follows I&#39;ll try to explain my basic understanding and interepretation of the semi-supervised framework based on Variational Autoencoders, as described in &lt;a href=&#34;#ref&#34;&gt;[1]&lt;/a&gt;. I shall assume a vector notation where bold symbols $\mathbf{a}$ represent vectors, whose $j$-th component can be represented as $a_j$.&lt;/p&gt;

&lt;p&gt;The starting point of the framework is to consider a dataset &lt;span  class=&#34;math&#34;&gt;\(D = S \cup U\)&lt;/span&gt;, where:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(S = \{(\mathbf{x}_1, \mathbf{y}_1), \ldots, (\mathbf{x}_n, \mathbf{y}_n)\}\)&lt;/span&gt;,&lt;br&gt;&lt;/li&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(U = \{\mathbf{x}_{n+1}, \ldots, \mathbf{x}_{n+m}\}\)&lt;/span&gt;,&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;with &lt;span  class=&#34;math&#34;&gt;\(\mathbf{x}_i \in \mathbb{R}^N\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(\mathbf{y}_i \in \{0,1\}^C\)&lt;/span&gt; represents a one-hot encoding of a class in &lt;span  class=&#34;math&#34;&gt;\(\{1, \ldots, C\}\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;The basic assumption of variational autoencoders is that data is generated according to a density function &lt;span  class=&#34;math&#34;&gt;\(p_\theta(\mathbf{x}| \mathbf{z})\)&lt;/span&gt;, where  &lt;span  class=&#34;math&#34;&gt;\(\mathbf{z}\in \mathbb{R}^K\)&lt;/span&gt; is a latent variables governing the distribution of &lt;span  class=&#34;math&#34;&gt;\(\mathbf{x}\)&lt;/span&gt;. &lt;span  class=&#34;math&#34;&gt;\(\theta\)&lt;/span&gt; represents a model parameter. The above density function can be modeled through a Neural network: thus &lt;span  class=&#34;math&#34;&gt;\(\theta\)&lt;/span&gt; represents all the network weights. An example PyTorch snippet, where the density is be modeled as a softmax over a simple linear layer, is illustrated below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
import torch.nn as nn

class Decoder(nn.Module):
    def __init__(self, latent_size,num_classes,out_size):
        super(Decoder, self).__init__()

        self.linear_layer = nn.Linear(latent_size + num_classes, out_size)
        nn.init.xavier_normal_(self.layer.weight)

        self.activation = nn.Sigmoid(dim=-1)

    def forward(self, z):
      	return self.activation(self.linear_layer(z))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, we are assuming $\mathbf{x}$ binary and consequently the reconstruction exploits a bernoullian distribution for each feature.&lt;/p&gt;

&lt;p&gt;Let&#39;s see how the generative framework can model the likelihood of the data and help us develop a semi-supervised classifier.&lt;/p&gt;

&lt;h2 id=&#34;unsupervised-examples&#34;&gt;Unsupervised examples&lt;/h2&gt;

&lt;p&gt;First, let us consider &lt;span  class=&#34;math&#34;&gt;\(\mathbf{x}\in U\)&lt;/span&gt;. When &lt;span  class=&#34;math&#34;&gt;\(\mathbf{y}\)&lt;/span&gt; is unknown, we can consider it as a latent variable as well. Both &lt;span  class=&#34;math&#34;&gt;\(\mathbf{y}\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(\mathbf{z}\)&lt;/span&gt; assume a prior distribution, given by&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{split}
\mathbf{z} \sim &amp; \mathcal{N}(\mathbf{0},I_K)\\
\mathbf{y} \sim &amp; \mathit{Cat}(\boldsymbol\pi)
\end{split}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Here, &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol\pi\)&lt;/span&gt; is a prior multinomial distribution over &lt;span  class=&#34;math&#34;&gt;\(\{1, \ldots, C\}\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;In such a case, we can extend the generative setting where data samples $\mathbf{x}$ are assumed to be generated by the conditional &lt;span  class=&#34;math&#34;&gt;\(p_\theta(\mathbf{x}| \mathbf{z},\mathbf{y})\)&lt;/span&gt; through the relationship&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{equation}\label{average}\tag{1}
p(\mathbf{x}) = \int p_\theta(\mathbf{x}| \mathbf{z}, \mathbf{y}) p(\mathbf{z})p(\mathbf{y}) \mathrm{d} \mathbf{z} \mathrm{d} \mathbf{y}
\end{equation}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In principle, the &lt;span  class=&#34;math&#34;&gt;\(\theta\)&lt;/span&gt; parameter can be chosen to maximize the evidence on &lt;span  class=&#34;math&#34;&gt;\(D\)&lt;/span&gt;, i.e. by optimizing &lt;span  class=&#34;math&#34;&gt;\(\log p(\mathbf{x})\)&lt;/span&gt;. However, this approach is not feasible because it requires averaging over all possible &lt;span  class=&#34;math&#34;&gt;\(\mathbf{z}\)&lt;/span&gt; pairs. We can approximate the likelihood by sampling a subset of $\mathbf{z}$ latent data points and then averaging over them. Again, this workaround exhibits the drawback that pure random sampling is exposed to high variance. The idea of Variational Autoencoders is to &amp;quot;guide&amp;quot; the sampling by exploiting the evidence: instead of freely choosing $\mathbf{z},\mathbf{y}$, we build a sampler &lt;span  class=&#34;math&#34;&gt;\(q_\phi(\mathbf{z},\mathbf{y}|\mathbf{x})\)&lt;/span&gt; that tunes the probability of &lt;span  class=&#34;math&#34;&gt;\(\mathbf{z},\mathbf{y}\)&lt;/span&gt; according to $\mathbf{x}$. In practice, $q_\phi$ &lt;em&gt;encodes&lt;/em&gt; the information regarding &lt;span  class=&#34;math&#34;&gt;\(\mathbf{x}\)&lt;/span&gt; into the most probable &lt;span  class=&#34;math&#34;&gt;\(\mathbf{z},\mathbf{y}\)&lt;/span&gt; latent variables.&lt;/p&gt;

&lt;p&gt;We can factorize the decoder  as &lt;span  class=&#34;math&#34;&gt;\(q_\phi(\mathbf{z},\mathbf{y}| \mathbf{x}) = q_\varphi(\mathbf{z}|\mathbf{x},\mathbf{y})q_\vartheta(\mathbf{y}|\mathbf{x})\)&lt;/span&gt;, where&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{split}
q_\varphi(\mathbf{z}|\mathbf{x},\mathbf{y}) \equiv &amp; \mathcal{N}(\mathbf{z}| \boldsymbol\mu_\varphi(\mathbf{x},\mathbf{y}), \boldsymbol\sigma_\varphi(\mathbf{x},\mathbf{y})\cdot I_K)\\
q_\vartheta(\mathbf{y}|\mathbf{x}) \equiv &amp; \mathit{Cat}(\mathbf{y}|\boldsymbol\pi_\vartheta(\mathbf{x})), 
\end{split}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Here, the parameters &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol\mu_\varphi(\mathbf{x},\mathbf{y}), \boldsymbol\sigma_\varphi(\mathbf{x},\mathbf{y})\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol\pi_\vartheta(\mathbf{x})\)&lt;/span&gt; represent neural functions parameterized by $\varphi$ and $\vartheta$, respectively.
Again, a PyTorch snippet is given below, where the two encoders are exemplified. Concerning  &lt;span  class=&#34;math&#34;&gt;\(q_\varphi(\mathbf{z}|\mathbf{x}, \mathbf{y})\)&lt;/span&gt;, we have:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Encoder_z(nn.Module):
    def __init__(self, input_size,latent_size):
        super(Decoder, self).__init__()
				
        self.latent_size = latent_size
        
        self.linear_layer = nn.Linear(input_size, 2*latent_size)
        nn.init.xavier_normal_(self.linear_layer.weight)
        
    def _sample_latent(self, mu_q, logvar_q):
        var_q = torch.exp(logvar_q)
        epsilon = torch.randn(var_q.size(),requires_grad=False).to(var_q.device)

        return mu_q + epsilon*var_q
        
    def forward(self, x, y):
      	input = torch.cat([x,y],dim=-1)
        temp_out = self.linear_layer(input)
    
        mu_q = temp_out[:, :self.latent_size]
        logvar_q = temp_out[:, self.latent_size:]

        z = self._sample_latent(mu_q, logvar_q)

        return z, mu_q, logvar_q
​```
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The computation within this class is a variable &lt;span  class=&#34;math&#34;&gt;\(\mathbf{z}\sim q_\varphi(\cdot |\mathbf{x},\mathbf{y})\)&lt;/span&gt;, as well as the parameters of the variational (gaussian) distribution &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol\mu\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol\sigma\)&lt;/span&gt;.
Here, we are exploiting the reparameterization trick: given a variable &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol\epsilon \sim \mathcal{N}(\mathbf{0},I_K)\)&lt;/span&gt;, the transformation &lt;span  class=&#34;math&#34;&gt;\(z = \mu + \epsilon \cdot \sigma\)&lt;/span&gt; guarantees that &lt;span  class=&#34;math&#34;&gt;\(\mathbf{z}\sim \mathcal{N}(\boldsymbol\mu, \boldsymbol\sigma)\)&lt;/span&gt; and at the same time it preserves the backpropagation of the gradient, since &lt;span  class=&#34;math&#34;&gt;\(
\frac{\partial \mathbf{z}}{\partial w} = \frac{\partial \boldsymbol\mu}{\partial w} + \boldsymbol\epsilon \cdot \frac{\partial \boldsymbol\sigma}{\partial w}
\)&lt;/span&gt; and both &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol\mu\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol\sigma\)&lt;/span&gt; are deterministically computed as shown above.&lt;/p&gt;

&lt;p&gt;Similarly, &lt;span  class=&#34;math&#34;&gt;\(q_\vartheta(\mathbf{y}|\mathbf{x})\)&lt;/span&gt; is exemplified by the following snippet:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Classifier(nn.Module):
    def __init__(self, input_size,num_classes):
        super(Decoder, self).__init__()

        self.linear_layer = nn.Linear(input_size, 2*latent_size)
        nn.init.xavier_normal_(self.layer.weight)
      
        self.softmax = nn.Softmax(dim=-1)
        
    def forward(self, x):
        return self.softmax(self.linear_layer(x))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice that, differently from the previous case, within this class we directly output a probability distribution &lt;span  class=&#34;math&#34;&gt;\(q_\vartheta(\mathbf{y}|\mathbf{x})\)&lt;/span&gt;, rather than a sample &lt;span  class=&#34;math&#34;&gt;\(\mathbf{y} \sim q_\vartheta(\cdot|\mathbf{x})\)&lt;/span&gt;. The reason for this choice is that no reparameterization trick is possible for a discrete distribution that preserves backpropagation. However, this does not prevent us from  averaging over all possible samples, as we shall see later.&lt;/p&gt;

&lt;p&gt;What is the relationship between the encoders and the decoder? We can observe that&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(
\begin{split}
\log p(\mathbf{x}) \geq &amp; \mathbb{E}_{q_\phi(\mathbf{z},\mathbf{y}| \mathbf{x})}\left[\log p_\theta(\mathbf{x}| \mathbf{z}) + \log p(\mathbf{z}) + \log p(\mathbf{y}) - \log q_\phi(\mathbf{z},\mathbf{y}| \mathbf{x})\right] \\
= &amp; \sum_\mathbf{y} \mathbb{E}_{q_\varphi(\mathbf{z}| \mathbf{x},\mathbf{y})}\Bigg[  q_\vartheta(\mathbf{y}| \mathbf{x})\bigg(\log p_\theta(\mathbf{x}| \mathbf{z}) + \log p(\mathbf{y}) - \log q_\vartheta(\mathbf{y}| \mathbf{x})\bigg) \\
&amp;  \qquad \qquad + \log p(\mathbf{z}) - \log q_\varphi(\mathbf{z}| \mathbf{x},\mathbf{y})\Bigg] 
\end{split}
\)&lt;/span&gt;
We call the right-hand side of the equation the &lt;em&gt;Evidence Lower Bound&lt;/em&gt; (&lt;em&gt;ELBO&lt;/em&gt;). It turns out that, optimizing this equation with respect to &lt;span  class=&#34;math&#34;&gt;\(\phi, \theta\)&lt;/span&gt; corresponds to optimizing &lt;span  class=&#34;math&#34;&gt;\(\log p(\mathbf{x})\)&lt;/span&gt; as well. Thus, we can specify the loss &lt;span  class=&#34;math&#34;&gt;\(\ell(\mathit{x})\)&lt;/span&gt; as the negative of the &lt;em&gt;ELBO&lt;/em&gt; and exploit a gradient-based optimization strategy. The main difference with respect to directly optimizing eq. (&lt;span  class=&#34;math&#34;&gt;\(\ref{average}\)&lt;/span&gt;) is that the &lt;em&gt;ELBO&lt;/em&gt; is tractable. In fact, we can rewrite it as&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(\begin{split}
\ell(\mathbf{x})= &amp; - \sum_\mathbf{y} \mathbb{E}_{\boldsymbol\epsilon\sim \mathcal{N}(\mathbf{0},I_K)}\Bigg[ q_\vartheta(\mathbf{y}| \mathbf{x})\bigg(\log p_\theta(\mathbf{x}| \mathbf{z}(\boldsymbol\epsilon, \mathbf{x},\mathbf{y})) + \log p(\mathbf{y}) - \log q_\vartheta(\mathbf{y}| \mathbf{x})\bigg) \\
&amp;  \qquad \qquad + \log p(\mathbf{z}(\boldsymbol\epsilon, \mathbf{x},\mathbf{y})) - \log q_\varphi(\mathbf{z}(\boldsymbol\epsilon, \mathbf{x},\mathbf{y})| \mathbf{x},\mathbf{y})\Bigg] 
\end{split}\)&lt;/span&gt;
where &lt;span  class=&#34;math&#34;&gt;\(\mathbf{z}(\boldsymbol\epsilon, \mathbf{x},\mathbf{y}) = \boldsymbol\mu_\varphi(\mathbf{x},\mathbf{y}) + \boldsymbol\epsilon \cdot \sigma_\varphi(\mathbf{x},\mathbf{y})\)&lt;/span&gt; represents the &lt;span  class=&#34;math&#34;&gt;\(\mathbf{z}\)&lt;/span&gt; component in the output of  &lt;code&gt;Decoder_z&lt;/code&gt; and
&lt;span  class=&#34;math&#34;&gt;\(q_{\vartheta}(\mathbf{y}| \mathbf{x})\)&lt;/span&gt; represents the output of &lt;code&gt;Classifier&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;By analysing the above equation we can observe the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Since &lt;span  class=&#34;math&#34;&gt;\(q_\vartheta(\mathbf{y}| \mathbf{x}) = \prod_{j=1}^C \pi_{\vartheta,j}(\mathbf{x})^{y_j}\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(\mathbf{y}\)&lt;/span&gt; ranges over all possible classes, we can simplify the first part of the right-hand side of the equation with &lt;span  class=&#34;math&#34;&gt;\(\sum_{j = 1}^C \pi_{\vartheta,j}(\mathbf{x})\bigg(p_\theta(\mathbf{x}| \mathbf{z}(\epsilon,\mathbf{x},\mathbf{y}), \mathbf{e}_j) + \log \pi_j - \log \pi_{\vartheta,j}(\mathbf{x})\bigg)\)&lt;/span&gt;,  where &lt;span  class=&#34;math&#34;&gt;\(\mathbf{e}_j\)&lt;/span&gt; is the vector of all zeros except for position &lt;span  class=&#34;math&#34;&gt;\(j\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(\pi_{\vartheta,j}(\mathbf{x})\)&lt;/span&gt; is the &lt;span  class=&#34;math&#34;&gt;\(j\)&lt;/span&gt;-th component of &lt;span  class=&#34;math&#34;&gt;\(\pi_{\vartheta}(\mathbf{x})\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Further, by exploiting the definitions, &lt;span  class=&#34;math&#34;&gt;\(\mathbb{E}_{\boldsymbol\epsilon\sim \mathcal{N}(\mathbf{0},I_K)}\left[\log p(\mathbf{z}(\boldsymbol\epsilon, \mathbf{x},\mathbf{y})) - \log q_\varphi(\mathbf{z}(\boldsymbol\epsilon, \mathbf{x},\mathbf{y})| \mathbf{x},\mathbf{y})\right] = \sum_{k} \Bigg(\log \sigma_{\varphi,k}(\mathbf{x},\mathbf{y}) + 1 - \sigma_{\varphi,k}(\mathbf{x},\mathbf{y}) - \boldsymbol\mu_{\varphi,k}(\mathbf{x},\mathbf{y})^2\Bigg)\)&lt;/span&gt; and we see that the only source of nondeterminism is given by the component that computes the log-likelihood.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To summarize, the loss for an element &lt;span  class=&#34;math&#34;&gt;\(\mathbf{x} \in U\)&lt;/span&gt; can be fully specified as follows:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{split}
\ell(\mathbf{x})= &amp; \sum_{j = 1}^C \pi_{\vartheta,j}(\mathbf{x})\left(\log \pi_{\vartheta,j}(\mathbf{x}) - \log \pi_j \right) \\
&amp; - \mathbb{E}_{\boldsymbol\epsilon\sim \mathcal{N(\mathbf{0},I_K)}}\left[\sum_{j = 1}^C \pi_{\vartheta,j}(\mathbf{x})p_\theta(\mathbf{x}| \mathbf{z}(\boldsymbol\epsilon,\mathbf{x},\mathbf{e}_j), \mathbf{e}_j)\right]\\
&amp; - \sum_{j = 1}^C \sum_{k} \Bigg(\log \sigma_{\varphi,k}(\mathbf{x},\mathbf{e}_j) + 1 - \sigma_{\varphi,k}(\mathbf{x},\mathbf{e}_j) - \mu_{\varphi,k}(\mathbf{x},\mathbf{e}_j)^2\Bigg)
\end{split}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The code snippet illustrating &lt;span  class=&#34;math&#34;&gt;\(\ell(\mathbf{x})\)&lt;/span&gt; in PyTorch is the following.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def unsupervised_loss(x,encoder,decoder,classifier,num_classes,y_prior=1):
    y_q = classifier(x)
    kld_cat = torch.mean(torch.sum(y_q*(torch.exp(y_q) - torch.log(y_prior)),-1),-1)  
    
    kld_norm = 0
    e = torch.zeros(y_q.size()).to(x.device)
    
    prob_e = []
    for j in range(num_classes):
        e[:,j] = 1.
        z, mu_q, logvar_q = encoder(x,e)
        kld_norm += torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1)
        prob_e.append(decoder(z))
        e[:,j] = 0.
		
		kld_norm = torch.mean(kld_norm, -1)

    prob_e = torch.floatTensor(log_prob_e)
    prob_x = torch.matmul(llk_e,y_q).squeeze()
    
    loss = nn.BCELoss()
    llk = loss(prob_x,x)
    
    return llk + kld_cat + kld_norm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, since &lt;code&gt;Decoder&lt;/code&gt; provides a probability distribution, the loss is the negative log likelihood. Again, we are assuming $\mathbf{x}$ binary and the underlying probability is bernoullian on each feature.&lt;/p&gt;

&lt;h2 id=&#34;supervised-examples&#34;&gt;Supervised examples&lt;/h2&gt;

&lt;p&gt;The case &lt;span  class=&#34;math&#34;&gt;\((\mathbf{x},\mathbf{y})\in S\)&lt;/span&gt; rensembles the unsupervised case, but with a major difference. For the labelled case,  the joint probability &lt;span  class=&#34;math&#34;&gt;\(p(\mathbf{x},\mathbf{y},\mathbf{z})\)&lt;/span&gt; is decomposed as &lt;span  class=&#34;math&#34;&gt;\(p_\theta(\mathbf{x},\mathbf{y},\mathbf{z}) = q_{\vartheta}(\mathbf{y}|\mathbf{x})p_\theta(\mathbf{x}|\mathbf{z})p(\mathbf{z})\)&lt;/span&gt;. In practice, we consider here a discriminative setting where the &lt;code&gt;Classifier&lt;/code&gt; component as a part of the decoder. This is different from the unsupervised case, where &lt;span  class=&#34;math&#34;&gt;\(\mathbf{y}\)&lt;/span&gt; was considered a latent variable which encoded latent information from $\mathbf{x}$ in a generative setting.&lt;/p&gt;

&lt;p&gt;As a consequence, the joint likelihood can be approximated as&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{split}
\log p(\mathbf{x},\mathbf{y}) \geq &amp; \mathbb{E}_{q_\varphi(\mathbf{z}| \mathbf{x},\mathbf{y})}\Bigg[\log p_\theta(\mathbf{x}| \mathbf{z}) + \log q_\vartheta(\mathbf{y}|\mathbf{x}) + \log p(\mathbf{z}) - \log q_\varphi(\mathbf{z}| \mathbf{x},\mathbf{y})\Bigg] 
\end{split}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;By rearranging the formulas, we obtain the loss term for the supervised case:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{split}\ell(\mathbf{x},\mathbf{y}) = &amp; \mathbb{E}_{\boldsymbol\epsilon\sim \mathcal{N}(0,1)}\Bigg[\log p_\theta(\mathbf{x}| \mathbf{z}(\boldsymbol\epsilon,\mathbf{x}, \mathbf{y}))\Bigg] \\ &amp; + \sum_{j=1}^C y_j \log \pi_{\vartheta,j}(\mathbf{x}) \\&amp; + \sum_{k} \Bigg(\log \sigma_{\varphi,k}(\mathbf{x},\mathbf{y}) + 1 - \sigma_{\varphi,k}(\mathbf{x},\mathbf{y}) - \mu_{\varphi,k}(\mathbf{x},\mathbf{y})^2\Bigg)\end{split}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The corresponding example implementation:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def supervised_loss(x,y,encoder,decoder,classifier):    
    z, mu_q, logvar_q = encoder(x,y)
    kld_norm = torch.mean(torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1),-1)

    prob_x = decoder(x)     
    loss = nn.BCELoss()
    llk = loss(prob_x,x)
    
    y_q = classifier(x)
    loss = CrossEntropyLoss(dim=-1)
    llk_cat = loss(y_q,y)
                          
    return llk + llk_cat + kld_norm    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Other interpretations for the supervised case are possible: see, e.g. the treatment in  &lt;a href=&#34;http://bjlkeng.github.io/posts/semi-supervised-learning-with-variational-autoencoders/&#34;&gt;Brian Keng&#39;s blog&lt;/a&gt;. However, I feel that separating the supervised and the unsupervised case and arranging the derivations accordingly is more intuitive.&lt;/p&gt;

&lt;h2 id=&#34;wrapping-up&#34;&gt;Wrapping up&lt;/h2&gt;

&lt;p&gt;We can finally combine all the above and devise a model for semi-supervised training:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class SSVAE(nn.Module):
    def __init__(self,input_size,num_classes,latent_size, y_prior = 1):
        self.input_size = input_size
        self.num_classes = num_classes
        self.latent_size = latent_size
        
        self.y_prior = y_prior
        
        self.encoder = Encoder(input_size,num_classes,latent_size)
        self.decoder = Decoder(latent_size,input_size)
        self.classifier = Classifier(input_size, num_classes)
        
        llk_loss = nn.BCELoss()
        cat_loss = nn.CrossEntropyLoss()

    def unsupervised_loss(self, x):
        y_q = self.classifier(x)
        kld_cat = torch.mean(torch.sum(y_q*(torch.log(y_q) - torch.log(self.y_prior)),-1),-1)  

        kld_norm = 0
        e = torch.zeros(y_q.size()).to(x.device)
    
        prob_e = []
        for j in range(self.num_classes):
            e[:,j] = 1.
            z, mu_q, logvar_q = self.encoder(x,e)
            kld_norm += torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1)
            prob_e.append(self.decoder(z))
            e[:,j] = 0.
        
        kld_norm = torch.mean(kld_norm, -1))

        prob_e = torch.floatTensor(log_prob_e)
        prob_x = torch.matmul(llk_e,y_q).squeeze()
    
        llk = llk_loss(prob_x,x)
    
        return llk + kld_cat + kld_norm
        
    def supervised_loss(self,x,y):
        z, mu_q, logvar_q = self.encoder(x,y)
        kld_norm = torch.mean(torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1),-1)

        prob_x = self.decoder(x)                          
        llk = loss(prob_x,x)
    
        y_q = self.classifier(x)
        llk_cat = cat_loss(y_q,y)
                          
        return llk + llk_cat + kld_norm    
        
    def forward(self, x, y = None, train = True)
        if not train:
            return self.classifier(x)
        else:
            if y is not None:
                loss = self.supervised_loss(x,y)
            else
                loss = self.unsupervised_loss(x)
            return loss
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can observe that the model can be called in two modes: either in &lt;code&gt;train&lt;/code&gt; mode or not. For the latter, it acts as a classifier and produces the class probabilities. In the training mode, on the other side, it computes either the supervised or the unsupervised, loss based on whether the data is labelled or not. The training procedure is quite straightforward as it just requires a &lt;code&gt;data_loader&lt;/code&gt; capable of ranging over &lt;span  class=&#34;math&#34;&gt;\(D\)&lt;/span&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def train(data_loader,input_size, num_classes, latent_size, y_priors):
    model = SSVAE(input_size,num_classes, latent_size)
    
    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)
    
    for batch_idx,(x,y) in enumerate(data_loader):
        optimizer.zero_grad()
        
        loss = model(x,y)
        loss.backward()
        optimizer.step()
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;

&lt;p&gt;[1]  &lt;a name=&#34;ref&#34;&gt;&lt;/a&gt;Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, Max Welling. &lt;a href=&#34;https://arxiv.org/abs/1406.5298&#34;&gt;Semi-supervised Learning with Deep Generative Models&lt;/a&gt;. Advances in Neural Information Processing Systems 27 (&lt;strong&gt;NIPS 2014&lt;/strong&gt;), Montreal&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://gmanco.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/slides/example/</guid>
      <description>

&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;

&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Code block:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;

&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;

&lt;p&gt;Block math:&lt;/p&gt;

&lt;p&gt;$$
f\left( x \right) = \;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;

&lt;p&gt;Make content appear incrementally&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
   One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three
&lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;

&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;


&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;


&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;

&lt;p&gt;Customize the slide style and background&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;

&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Factorization Approach for Survival Analysis on Diffusion Networks</title>
      <link>https://gmanco.github.io/publication/manco-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/manco-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sequential Variational Autoencoders for Collaborative Filtering</title>
      <link>https://gmanco.github.io/publication/sachdeva-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/sachdeva-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Predicting Temporal Activation Patterns via Recurrent Neural Networks</title>
      <link>https://gmanco.github.io/publication/manco-2018/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/manco-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fault detection and explanation through big data analysis on sensor streams</title>
      <link>https://gmanco.github.io/publication/manco-2017/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/manco-2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Differential Privacy and Neural Networks: A Preliminary Analysis</title>
      <link>https://gmanco.github.io/publication/manco-2017-a/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/manco-2017-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Survival Factorization on Diffusion Networks</title>
      <link>https://gmanco.github.io/publication/barbieri-2017/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Efficient Methods for Influence-Based Network-Oblivious Community Detection</title>
      <link>https://gmanco.github.io/publication/barbieri-2016/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Rialto: A Knowledge Discovery suite for data analysis</title>
      <link>https://gmanco.github.io/publication/manco-2016/</link>
      <pubDate>Sat, 01 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/manco-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How Can SMEs Benefit from Big Data? Challenges and a Path Forward</title>
      <link>https://gmanco.github.io/publication/coleman-2016/</link>
      <pubDate>Sun, 01 May 2016 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/coleman-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Outlying property detection with numerical attributes</title>
      <link>https://gmanco.github.io/publication/angiulli-2016/</link>
      <pubDate>Tue, 01 Mar 2016 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/angiulli-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Probabilistic Approaches to Recommendations</title>
      <link>https://gmanco.github.io/publication/barbieri-2014/</link>
      <pubDate>Thu, 01 May 2014 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2014/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Generative Bayesian Model for Item and User Recommendation in Social Rating Networks with Trust Relationships</title>
      <link>https://gmanco.github.io/publication/costa-2014/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/costa-2014/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Who to follow and why</title>
      <link>https://gmanco.github.io/publication/barbieri-2014-a/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2014-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Influence-Based Network-Oblivious Community Detection</title>
      <link>https://gmanco.github.io/publication/barbieri-2013-a/</link>
      <pubDate>Sun, 01 Dec 2013 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2013-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dealing with trajectory streams by clustering and mathematical transforms</title>
      <link>https://gmanco.github.io/publication/costa-2013-a/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/costa-2013-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Probabilistic topic models for sequence data</title>
      <link>https://gmanco.github.io/publication/barbieri-2013-b/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2013-b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Topic-aware social influence propagation models</title>
      <link>https://gmanco.github.io/publication/barbieri-2013-c/</link>
      <pubDate>Mon, 01 Apr 2013 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2013-c/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hierarchical clustering of XML documents focused on structural components</title>
      <link>https://gmanco.github.io/publication/costa-2013/</link>
      <pubDate>Fri, 01 Mar 2013 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/costa-2013/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cascade-based community detection</title>
      <link>https://gmanco.github.io/publication/barbieri-2013/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2013/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Topic-Aware Social Influence Propagation Models</title>
      <link>https://gmanco.github.io/publication/barbieri-2012-a/</link>
      <pubDate>Sat, 01 Dec 2012 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2012-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Balancing Prediction and Recommendation Accuracy: Hierarchical Latent Factors for Preference Data</title>
      <link>https://gmanco.github.io/publication/barbieri-2012/</link>
      <pubDate>Sun, 01 Apr 2012 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2012/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Probabilistic Sequence Modeling for Recommender Systems</title>
      <link>https://gmanco.github.io/publication/barbieri-bcmr-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-bcmr-12/</guid>
      <description></description>
    </item>
    
    <item>
      <title>From global to local and viceversa: uses of associative rule learning for classification in imprecise environments</title>
      <link>https://gmanco.github.io/publication/costa-2011/</link>
      <pubDate>Thu, 01 Dec 2011 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/costa-2011/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Probabilistic Hierarchical Approach for Pattern Discovery in Collaborative Filtering Data</title>
      <link>https://gmanco.github.io/publication/barbieri-2011-a/</link>
      <pubDate>Fri, 01 Apr 2011 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2011-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Analysis of Probabilistic Methods for Top-N Recommendation in Collaborative Filtering</title>
      <link>https://gmanco.github.io/publication/barbieri-2011/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2011/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modeling item selection and relevance for accurate recommendations</title>
      <link>https://gmanco.github.io/publication/barbieri-2011-b/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/barbieri-2011-b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An incremental clustering scheme for data de-duplication</title>
      <link>https://gmanco.github.io/publication/costa-2009/</link>
      <pubDate>Thu, 01 Oct 2009 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/costa-2009/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Top-Down Parameter-Free Clustering of High-Dimensional Categorical Data</title>
      <link>https://gmanco.github.io/publication/cesario-2007-a/</link>
      <pubDate>Sat, 01 Dec 2007 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/cesario-2007-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mining unconnected patterns in workflows</title>
      <link>https://gmanco.github.io/publication/greco-2007/</link>
      <pubDate>Sun, 01 Jul 2007 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/greco-2007/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Boosting text segmentation via progressive classification</title>
      <link>https://gmanco.github.io/publication/cesario-2007/</link>
      <pubDate>Fri, 01 Jun 2007 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/cesario-2007/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploiting structural similarity for effective Web information extraction</title>
      <link>https://gmanco.github.io/publication/flesca-2007/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/flesca-2007/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mining categories for emails via clustering and pattern discovery</title>
      <link>https://gmanco.github.io/publication/manco-2007/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/manco-2007/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mining and reasoning on workflows</title>
      <link>https://gmanco.github.io/publication/greco-2005/</link>
      <pubDate>Fri, 01 Apr 2005 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/greco-2005/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fast detection of XML structural similarity</title>
      <link>https://gmanco.github.io/publication/flesca-2005/</link>
      <pubDate>Tue, 01 Feb 2005 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/publication/flesca-2005/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://gmanco.github.io/research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://gmanco.github.io/research/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
